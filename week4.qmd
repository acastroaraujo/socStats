---
title: "Week 4"
callout-appearance: simple
callout-icon: false
---

The purpose of this week's homework is to keep developing simulation skills and more intuition about (1) *the* *law of large numbers* and (2) *the central limit theorem.*

Don't forget to load the following packages:

```{r}
#| message: false
library(tidyverse)
theme_set(theme_light(base_family = "Avenir Next Condensed"))
```

*Note that exercise questions are surrounded by depicted with blue and hints are depicted with green.*

::: callout-note
Questions...
:::

::: callout-tip
Hints...
:::

## Exercise 

So far, we have generated data using the `sample()` and `rbinom()` functions. R has a built-in function for generating data from normal distributions, it's called `rnorm()`.

Recall that all normal distributions exhibit the following behavior:

-   The probability that $x$ is within one standard deviation $\sigma$ away from the mean is roughly 68%.

-   The probability that $x$ is within two standard deviations $2\sigma$ away from the mean is roughly 95%.

-   The probability that $x$ is above the mean ( $x \geq \mu$ ) is 50%.

-   The probability that $x$ is below the mean ( $x \leq \mu$ ) is also 50%.

The following chunk generates 100,000 observations from a normal distribution with mean = 0 and standard deviation = 1.

```{r}
x <- rnorm(100000, mean = 0, sd = 1)
```

::: callout-note
Use the `mean()` function to verify that `x` exhibits those four behaviors.
:::

::: callout-tip
Hint: Remember that you can coerce numeric variables into `TRUE` and `FALSE` values using logical operators (`==`, `>`, `<`, etc.)
:::

## Exercise

::: callout-note
Use the `quantile()` function on the `x` created in the previous exercise. Explain the results.
:::

## Exercise

::: callout-note
Now modify the `probs` argument in the `quantile()` to find the 0.5% and 0.995% percentiles of `x`.
:::

::: callout-tip
Hint: If you Google "99 percent confidence interval" you should see a table that foreshadows the answer---i.e., you should get some number roughly equal to -2.576 and 2.576 respectively.
:::

## Exercise

::: callout-note
Use the `mean()` function to verify that the probability of $x$ being between $-2.576$ *and* $2.576$ is roughly 99%.
:::

::: callout-tip
Hint: The logical operator for "AND" is `&`.
:::

## Exercise

**Central Limit Theorem**

::: callout-note
Let $x = x_1 + \dots + x_{20}$, the sum of 20 independent uniform(0, 1) random variables. In R, create 1000 simulations of $x$ and plot the histogram of their means.
:::

::: callout-tip
Hint: You can simulate the sum of 20 independent uniform random variables using the following code:

```{r}
sum(runif(n = 20, min = 0, max = 1))
```

The simulation you create will have 1000 instances of a value like this.
:::

The **sampling distribution** is the set of possible sample "statistics" (e.g., means) that could have been observed, *if the data collection process had been repeated many many times*. There's an important theorem in statistics called the Central Limit Theorem (CLT) that *proves* that this sampling distribution converges to a normal distribution when the sample size is big enough, *regardless of how our data looks like.*

The **standard error** is how we estimate of the *the standard deviation of the sampling distribution*.

::: callout-note
Use the `sd()` function on the sampling distribution simulated earlier, then compare this value to the standard error given by $\text{se}(x) = \sigma_x / \sqrt{N}$ for one sample---i.e., `sd(runif(20)) / sqrt(20)`. Do they match? Are they close?
:::

## Exercise

**Simulation study of CLT**

Many introductory statistics textbooks say that "sample sizes equal to or greater than 30 are often considered sufficient for the CLT to hold."

::: callout-note
Write down intuitively what you think this means.
:::

Now lets revisit last class' simulation exercise.

<https://github.com/vaiseys/soc-stats-1/blob/main/demos/day-08.R>

You'll notice that the sample size is controlled by the following object:

```{r}
svy_size <- 2247  # number of people in each "poll"
```

And the "true" population proportion is given by:

```{r}
est_prop <- 1/3
```

In class, we also calculated the standard error "analytically" using the following mathematical equation:

$$
\text{se}_x = \sqrt{\frac{p (1-p)}{n}}
$$

Or in code:

```{r}
std_error <- sqrt((1/3) * (2/3) / svy_size)
std_error
```

This means that the 95% confidence interval should be roughly 2 standard errors away from 0.33

```{r}
0.33 - 1.96*std_error ## "1.96" is roughly "2"!
0.33 + 1.96*std_error
```

In Steve's simulation, this is given by `ci95`.

::: callout-note
Repeat Steve's simulation with different values of `svy_size`. Is the 95% confidence interval still roughly 2 standard errors away from 0.33?

Do these results change your initial interpretation of the idea that "sample sizes equal to or greater than 30 are often considered sufficient for the CLT to hold" ?
:::
