[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Statistics I (Exercises)",
    "section": "",
    "text": "Preface\nSyllabus\nHi everyone, I will be uploading the homework questions to this website.\nFeel free to reach out to me with any questions."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Social Statistics I (Exercises)",
    "section": "Resources",
    "text": "Resources\nThese are my personal recommendations for resources to start getting interested in statistics. It’s somewhat incomplete—e.g., there are no dedicated textbooks to causal inference, which is what we’ll cover next semester.\nClass Resources:\n\nR for Data Science (Wickham et al. 2023)\nI learned a lot using the first edition of this book. Feel free to skip some chapters on a first pass and come back to them if you think you might need them (e.g., strings, regular expressions, webscraping).\nAlso, I suggest you start with chapters 29 and 30.\nThe tidyverse style guide\nIt will help you write pretty code.\nData Visualization (Healy 2018)\nIt will help you make good graphs.\n\nGood books to play around with:\n\nData Analysis for Social Science: A Friendly and Practical Introduction (Llaudet and Imai 2022)\nVery introductory but useful.\nStatistical Inference via Data Science (Ismay and Kim 2019)\nIt’s good!\nQuantitative Social Science: An Introduction in Tidyverse (Imai and Williams 2022)\nI read this one a while ago, before it was re-written in tidyverse dialect. The chapters on probability and uncertainty are a great self-contained introduction to probability and statistical inference.\nRegression and Other Stories (Gelman et al. 2020)\nThis one seems a little too advanced for a first pass, but not advanced enough for a second pass? I like it a lot though.\n\nAdvanced Resources:\n\nAdvanced R (Wickham 2019).\nThis one is good for those of you that have a background in computer science and are looking for reasons to like R. It’s also good for those of you who finished (most of) R4DS and want to learn more general programming ideas.\nIntroduction to probability (Blitzstein and Hwang 2019)\nCovers more probability theory than what you’ll probably need, but it’s a fascinating topic and it’s very accessible.\nStatistical Rethinking (McElreath 2020)\nIt’s what the cool kids are into these days.\n\n\n\n\n\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to Probability. CRC Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nHealy, Kieran. 2018. Data Visualization: A Practical Introduction.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton University Press.\n\n\nIsmay, Chester, and Albert Y. Kim. 2019. Statistical Inference via Data Science: A ModernDive into r and the Tidyverse. CRC Press.\n\n\nLlaudet, Elena, and Kosuke Imai. 2022. Data Analysis for Social Science: A Friendly and Practical Introduction. Princeton University Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. CRC press.\n\n\nWickham, Hadley. 2019. Advanced R. CRC Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "sec1.html",
    "href": "sec1.html",
    "title": "Computing preliminaries",
    "section": "",
    "text": "That time we learned how to program in R, write simple reports using .qmd documents, and review some high school math."
  },
  {
    "objectID": "week1.html#data-structures",
    "href": "week1.html#data-structures",
    "title": "1  Week 1",
    "section": "1.1 Data Structures",
    "text": "1.1 Data Structures\n\n1.1.1 Vectors\nR has two kinds of vectors:\n\nAtomic vectors. All elements of the vector must have the same type (e.g., logical, integer, double, character).1 They are homogeneous.\nSee Figure 1.1.\nNote that integer and double are collectively known as numeric.\nLists. The elements of the vector can have different types. They can be heterogeneous.\n\nAll data structures—e.g., data frames, matrices, factors, dates, and more complex model objects—are built on top of these. All of these will have an additional class attribute. For example, a “data frame” (e.g., mtcars) is basically a list of atomic vectors that have the same length().\n\n\n\nFigure 1.1: Atomic vectors\nhttps://adv-r.hadley.nz/vectors-chap.html#atomic-vectors\n\n\n\n\n\n\n\n\nExercise 1.1\nTry typing typeof(mtcars) and class(mtcars) in the console to see what happens.\nNow type the following chunks of code into your console and understand what they do:\n\nnrow(mtcars)\nncol(mtcars)\nlength(mtcars)\ndim(mtcars)\nrownames(mtcars)\ncolnames(mtcars)\n\nBriefly describe what each of these do.\n\n\n\nNote. The absence of a vector is usually represented with NULL (as opposed to NA which is used to represent the absence of a value in a vector). NULL typically behaves like a vector of length 0.\nCreating vectors of length 1 (scalars).\nEach of the four primary types depicted in Figure 1.1 can be created using a special syntax:\n\nLogicals can be written in full (TRUE or FALSE), or abbreviated (T or F).\nDoubles are the default for numbers (123). They can also be specified in decimal (0.1234) and scientific (1.23e4).\nThere are three special values unique to doubles: Inf, -Inf, and NaN (not a number). Don’t worry about these for now!\nIntegers are written similarly to doubles but must be followed by L (1234L)\nStrings are surrounded by \" (\"hi\") or ' ('bye').\n\n\n\n\n\n\n\nExercise 1.2\nI suggest you always use long-form when creating logical vectors. Try assigning a different value to TRUE and to T.\n\n\nCode\nT &lt;- 123\nTRUE &lt;- 123\n\n\nWhat just happened?\nExercise 1.3\nImplicit coercion\nYou can create atomic vectors of any length with c() for “concatenate”.\nFor example:\n\n\nCode\nlgl &lt;- c(TRUE, FALSE, NA)\nint &lt;- c(1L, 6L, NA, 10L)\ndbl &lt;- c(1, NA, 2.5, 4.5)\nchr &lt;- c(NA, \"these are\", \"some strings\")\n\n\nRecall that atomic vectors are homogeneous. If you try to concatenate vectors of different types you will end up discovering implicit coercion. Basically, different types will be coerced in the following order: logical → integer → double → character.\nFor example, a logical and a character combine into a character:\n\n\nCode\nstr(c(TRUE, \"chr\")) ## str() is (almost) identical to dplyr::glimpse()\n\n\n chr [1:2] \"TRUE\" \"chr\"\n\n\nTest your knowledge of the vector coercion rules by predicting the output of the following uses of c():\n\n\nCode\nc(1, FALSE)\nc(\"a\", 1)\nc(TRUE, 1L)\n\n\nExercise 1.4\nExplicit coercion\nExplicit coercion happens when you call a function like as.logical(), as.integer(), as.double(), or as.character(). Use as.integer() on FALSE and TRUE, what values do they get coerced to?\nExercise 1.5\nThe most common form of implicit coercion\nThe following chunk of code creates a logical vector of size 75.\n\n\nCode\nx &lt;- sample(c(TRUE, FALSE), size = 75, replace = TRUE)\nstr(x)\n\n\n logi [1:75] FALSE FALSE FALSE TRUE FALSE TRUE ...\n\n\nUse sum(x) to get the number of TRUE values. Use mean(x) to get the proportion of TRUE values. Verify that mean(x) and sum(x) / length(x) give the same value.\n\n\n\nWe will usually use logical operators to transform a variable and then do the kinds of calculations in Exercise 1.5.\nFor example:\n\n\nCode\n## the proportion of cars in the dataset with more than 3 carburators\nmean(mtcars$carb &gt; 3)\n\n\n[1] 0.375\n\n\nSequences\nWe will sometimes create sequences of integers for various purposes (e.g., subsetting). For example, we can use the seq() to create a sequence of even numbers this way:\n\n\nCode\nseq(from = 2, to = 26, by = 2)\n\n\n [1]  2  4  6  8 10 12 14 16 18 20 22 24 26\n\n\nYou can create a simple sequence of numbers from x1 to x2 by using the : operator this way:\n\n\nCode\n1:10\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseq(1, 10, by = 1)\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\n1.1.2 Subsetting\nVectors\nAs a reminder, you can subset named lists (and therefore data frames) with the $ operator.\nFor example:\n\n\nCode\nmtcars$mpg\n\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n\nCode\nx &lt;- list(chr, lgl, letters)\nstr(x)\n\n\nList of 3\n $ : chr [1:3] NA \"these are\" \"some strings\"\n $ : logi [1:3] TRUE FALSE NA\n $ : chr [1:26] \"a\" \"b\" \"c\" \"d\" ...\n\n\nCode\nnames(x) &lt;- c(\"chr\", \"lgl\", \"alphabet\")\nstr(x)\n\n\nList of 3\n $ chr     : chr [1:3] NA \"these are\" \"some strings\"\n $ lgl     : logi [1:3] TRUE FALSE NA\n $ alphabet: chr [1:26] \"a\" \"b\" \"c\" \"d\" ...\n\n\nCode\nx$alphabet\n\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n\nYou can also do this using the [ and [[ operators, but this time you have to put the name in quotation marks.\nThus:\n\n\nCode\nmtcars[[mpg]]\n\n\nError in (function(x, i, exact) if (is.matrix(i)) as.matrix(x)[[i]] else .subset2(x, : object 'mpg' not found\n\n\nCode\nmtcars[[\"mpg\"]]\n\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n\n\n\n\n\n\n\nExercise 1.6\nWhat is the difference between mtcars[\"mpg\"] and mtcars[[\"mpg\"]]? More generally, what is the difference between the [ and [[ operators?\nWhich of the following two is TRUE?\n\n\nCode\nidentical(mtcars[\"mpg\"], mtcars$mpg)\nidentical(mtcars[[\"mpg\"]], mtcars$mpg)\n\n\n\n\n\nYou will be subsetting different kinds of things in R—mostly data frames—using integers and logicals.\n\n\n\n\n\n\nExercise 1.7\nletters is a built-in object in R that contains the 26 letters of English alphabet.\nUsing the [ operator, do the following:\n\nExtract the 17th value of letters\nCreate a sequence of even numbers from 2 to 26 and use that to subset letters\nUse 8:12 to subset letters.\n\nThis is known as integer subsetting.\nWhat happens if instead of [ you use [[?\n\n\n\nSubsetting + Assignment\nYou can use the assignment operator &lt;- in combination with subsetting to replace the values of a vector.\nFor example:\n\n\nCode\ndbl\n\n\n[1] 1.0  NA 2.5 4.5\n\n\nCode\ndbl[1] &lt;- 10\ndbl\n\n\n[1] 10.0   NA  2.5  4.5\n\n\nCode\ndbl[is.na(dbl)] &lt;- 0\ndbl\n\n\n[1] 10.0  0.0  2.5  4.5\n\n\nMake sure you understand what is.na() is doing here. Did we just do “integer” or “logical” subsetting?\n\n\n\n\n\n\nExercise 1.8\nNow that you know all this\nReplace the 18th value of letters with a missing value (NA).\n\n\n\n\n\n1.1.3 The most common error you’ll see\n\n\nCode\nmean[1:5]\n\n\nError in mean[1:5]: object of type 'closure' is not subsettable\n\n\nThis just means that you have tried to subset a function, and functions are most definitely not vectors.\nThis will happen for example if you think you created a dataset called df and try to extract a column:\n\n\nCode\ndf$col\n\n\nError in df$col: object of type 'closure' is not subsettable\n\n\n\n\n1.1.4 Data Frames\nThe most obvious use case $, [, or [[ is in the context of working with data frames.\nHere we will use the [ operator behaves differently when used on some objects—e.g., data frames and matrices.\n\nWhen subsetting with a single index, data frames behave like lists and index the columns, so mtcars[1:2] selects the first two columns.\nWhen subsetting with two indices, mtcars[1:3, ] selects the first three rows (and all the columns); mtcars[5, ] selects the fifth row and all columns; mtcars[1:5, \"cyl\"] selects the cyl column and the first five rows. Matrices behave in the same way.\n\nLogical subsetting\nThe most common way of using logicals to subset data frames is to learn some Boolean operators—e.g., &lt;, &lt;=, &gt;, &gt;=, !=, and ==.\nFor example, we can create a logical vector that tests for mtcars$wt values greater than 4.\n\n\nCode\nmtcars$wt &gt; 4\n\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[13] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nAnd then we can subset with [:\n\n\nCode\nmtcars[mtcars$wt &gt; 4, ]\n\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n\n\n\n\n\n\n\n\nExercise 1.9\nSubset mtcars so that we only see the observations for which cyl == 4.\nSubset mtcars so that we only see the observations for which mpg is greater than 23.\n\n\n\nSometimes it will be easier to use the %in% operator to test for many conditions at the same time.\nFor example:\n\n\nCode\nmtcars[mtcars$carb %in% c(3, 6, 8), ]\n\n\n               mpg cyl  disp  hp drat   wt qsec vs am gear carb\nMerc 450SE    16.4   8 275.8 180 3.07 4.07 17.4  0  0    3    3\nMerc 450SL    17.3   8 275.8 180 3.07 3.73 17.6  0  0    3    3\nMerc 450SLC   15.2   8 275.8 180 3.07 3.78 18.0  0  0    3    3\nFerrari Dino  19.7   6 145.0 175 3.62 2.77 15.5  0  1    5    6\nMaserati Bora 15.0   8 301.0 335 3.54 3.57 14.6  0  1    5    8\n\n\nAlternatively we could have done this:\n\n\nCode\nmtcars[mtcars$carb == 3 | mtcars$carb == 6 | mtcars$carb == 8, ]\n\n\n               mpg cyl  disp  hp drat   wt qsec vs am gear carb\nMerc 450SE    16.4   8 275.8 180 3.07 4.07 17.4  0  0    3    3\nMerc 450SL    17.3   8 275.8 180 3.07 3.73 17.6  0  0    3    3\nMerc 450SLC   15.2   8 275.8 180 3.07 3.78 18.0  0  0    3    3\nFerrari Dino  19.7   6 145.0 175 3.62 2.77 15.5  0  1    5    6\nMaserati Bora 15.0   8 301.0 335 3.54 3.57 14.6  0  1    5    8\n\n\nWhich do you prefer?"
  },
  {
    "objectID": "week1.html#search",
    "href": "week1.html#search",
    "title": "1  Week 1",
    "section": "1.2 Search",
    "text": "1.2 Search\nIn this section I will introduce two functions that are pretty much useless except for the fact that they will help use understand how R finds “objects” in your R session: search() and find().\nType search() into your console. I you are like me—and haven’t loaded any package yet—you should see the exact same output as this:\n\n\nCode\nsearch()\n\n\n[1] \".GlobalEnv\"        \"package:stats\"     \"package:graphics\" \n[4] \"package:grDevices\" \"package:utils\"     \"package:datasets\" \n[7] \"package:methods\"   \"Autoloads\"         \"package:base\"     \n\n\n.GlobalEnv is the what’s known as the “global environment.” Any variable that shows up in your Environment pane is stored there.\nType names(.GlobalEnv) into the console and see what shows up. You’ll notice that there’s an object called .Random.seed in .GlobalEnv that doesn’t show up in your Environment pane. That’s because RStudio “hides” any variable that has a . prefix.\n\n\nCode\n.secret_var &lt;- 1:10\n.secret_var\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nYou should not be able to see .secret_var in the Environment pane and yet it’s there!\nAny time you type something in R, it will proceed to search for it sequentially: first, in .GlobalEnv, then in the built-in stats package, then in graphics, and so on until it reaches the base package.2\nSo, if you type asdfasdfasdf into the console, R will search all these environments and produce an error once it comes out empty handed.\n\n\nCode\nasdfasdfasdf\n\n\nError in eval(expr, envir, enclos): object 'asdfasdfasdf' not found\n\n\nBut if you type mtcars, R will search all these environments until it finds mtcars living in the built-in datasets package.\nYou can verify that this is the case using the find() function.\n\n\nCode\nfind(\"mtcars\")\n\n\n[1] \"package:datasets\"\n\n\nNow, suppose you decide to create an object called mtcars, which then gets saved to the global environment.\n\n\nCode\nmtcars &lt;- \"this is not the mtcars dataset\"\nmtcars\n\n\n[1] \"this is not the mtcars dataset\"\n\n\nIf you now type find(\"mtcars\") into the console you’ll see the names of two environments in the order that R searches for mtcars.\n\n\nCode\nfind(\"mtcars\")\n\n\n[1] \".GlobalEnv\"       \"package:datasets\"\n\n\nYou can still access the original mtcars dataset using the :: operator like this:\n\n\nCode\nstr(datasets::mtcars)\n\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\nIn fact, you can use pkgname::obj to access any object in any package (even if you haven’t loaded it yet).\nFor example:\n\n\nCode\ndplyr::glimpse(datasets::mtcars)\n\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\n1.2.1 Errors!\nSuppose you want to analyze the penguins dataset contained in the palmerpenguins package.\nYou will have to type the following into your console once if you haven’t already:\n\n\nCode\ninstall.packages(\"palmerpenguins\")\n\n\nYou want to use the table() function to count up the number of times a specific year shows up in the dataset.\n\n\nCode\n# library(tidyverse)\n# library(palmerpenguins)\ntable(year)\n\n\nError in table(year): object 'year' not found\n\n\nOh no, an error!\nAfter realizing that you “commented out” the library(pkg) lines, you remove the # symbols and do this:\n\n\nCode\nlibrary(tidyverse)\nlibrary(palmerpenguins)\ntable(year)\n\n\nError in unique.default(x, nmax = nmax): unique() applies only to vectors\n\n\nOh no, a different error!\nYou realize that table() has no way of knowing that you wanted to access the year variable in the penguins dataset. You forgot to subset!\nThis should work:\n\n\nCode\ntable(penguins$year)\n\n\n\n2007 2008 2009 \n 110  114  120 \n\n\n\n\n\n\n\n\nExercise 1.10\nUsing what I told you earlier about the search() function, explain why you get two different errors. What is going on? What is R doing when you type table(year)? (You might want to type search() into the console again). In what package does R find the year object?"
  },
  {
    "objectID": "week1.html#dplyr-subsetting",
    "href": "week1.html#dplyr-subsetting",
    "title": "1  Week 1",
    "section": "1.3 dplyr subsetting",
    "text": "1.3 dplyr subsetting\nYou will almost never subset data frames in the way we did for the previous exercises. In fact, we won’t use “base R” much. Part of the reason we use tidyverse instead of base R is because the documentation for the tidyverse is excellent. Also, the error messages are easier to understand.\nWe will subset data frames using two functions contained in the dplyr package:\n\nslice() for integer subsetting.\nfilter() for logical subsetting.\n\n\nI recommend you follow these two links.\n\nDon’t forget to library(tidyverse) if you haven’t done so already.\n\n\nCode\nlibrary(tidyverse)\n\n\n\n\n\n\n\n\nExercise 1.11\nUse slice() to extract the even-numbered rows in the penguins dataset.\nIt will look something like this:\n\n\nCode\npenguins |&gt; \n  slice(\"SOME NUMERIC VECTOR GOES HERE\")\n\n\nNow use slice() to extract every third row—i.e., row 3, 6, 9, and so on.\nExercise 1.12\nUse filter() to extract the observations in the penguins dataset for which species == \"Gentoo\", island == \"Biscoe\", and body_mass_g is between 5,000 and 5,500."
  },
  {
    "objectID": "week1.html#footnotes",
    "href": "week1.html#footnotes",
    "title": "1  Week 1",
    "section": "",
    "text": "Technically, there are two other types of atomic vectors: complex and raw. I don’t think you’ll see much of these.↩︎\nIgnore “Autoloads” and “tools:rstudio” (this last one shows up when you type search() in the console if you are using RStudio). This is not important!↩︎"
  },
  {
    "objectID": "week2.html#communication",
    "href": "week2.html#communication",
    "title": "2  Week 2",
    "section": "2.1 Communication",
    "text": "2.1 Communication\nSkim Chapter 29 in R for Data Science (Wickham et al. 2023).1\nYou will then need to do the following:\n\nInstall Zotero in your computer if you haven’t already.\nRead Citations from Zotero and stop reading when you get to “Group Libraries.” This is 3 paragraphs.\nGo to this repository and download any CSL file you want—e.g., I am using the this one. Save the file in your Project folder.\nInstall the following packages:\n\n\nCode\ninstall.packages(\"modelsummary\")\ninstall.packages(\"flextable\")\ninstall.packages(\"tinytex\")\n\n\nYou should also type this into the console after installing tinytex:\n\n\nCode\ntinytex::install_tinytex()\n\n\n\n\n\n\n\n\n\nExercise\nDownload this .qmd file and put it into your project folder.\n\nChange Figure 1 (the cat picture) to an image of your liking. Adjust the caption accordingly.\nChange Equation 1 to a different equation.\nHint: If your not familiar with writing these sorts of equations, you can ask ChatGPT to generate the latex code for a different equation—e.g., “the normal distribution.”\nChange the “citations paragraph” at the end to so that it corresponds to yourself—i.e., different name, different citations.\nEdit the YAML file so that it includes your name (name), the appropriate date (date), a different font (mainfont), and whatever csl file you decided to go with (csl).\nToggle between the Source and Visual editors and try to understand what is going on.\nCreate a pdf and an html file.\nHint: This is how my .pdf and .html files look like.\nUpload the .qmd, .pdf, and .html files to your github repository."
  },
  {
    "objectID": "week2.html#data-wrangling",
    "href": "week2.html#data-wrangling",
    "title": "2  Week 2",
    "section": "2.2 Data Wrangling",
    "text": "2.2 Data Wrangling\n\n\n\n\n\n\nMultiple Exercises\nRead Chapter 4 of R4DS (Wickham et al. 2023) and complete the following exercises:\n\n4.2.5: all six exercises\n4.3.5: all seven exercises\n4.5.7: all six exercises\n\nAnswer these exercises in a quarto document and upload both .qmd and .html files to your github repository, just as you did for last week’s homework.\n\n\n\nYes, this means that for this week you will upload five different files to your github repository!\n\n\n\n\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "week2.html#footnotes",
    "href": "week2.html#footnotes",
    "title": "2  Week 2",
    "section": "",
    "text": "Some additional resources to skim:\n\nhttps://quarto.org/docs/get-started/authoring/rstudio.html\nhttps://quarto.org/docs/visual-editor/technical.html\n\n↩︎"
  },
  {
    "objectID": "sec2.html",
    "href": "sec2.html",
    "title": "Data, sampling, and probability",
    "section": "",
    "text": "That time we learned a little bit about probability and simulation. This stuff is important, we can’t really understand statistics without probability, as hinted at in Figure 1.\n\n\n\nFigure 1: Source: Wasserman (2004)\n\n\n\nTo make this a little bit more concrete, here’s a story about three coin flips:\n\n\n\n\n\n\nFlip 3 fair coins and record the number of heads.\n\n\n\nA data generating process (DGP) is just a simplified story about how data are produced and recorded, which we usually denote with some parameters. In this case we have the data \\(X\\) (number of heads) and two parameters that control its probability distribution: \\(n = 3\\) and \\(p = 0.5\\).\nThis is how we write this same DGP in mathematical form:\n\\[\nX \\sim \\text{Binomial}(n = 3, \\ p = 0.5)\n\\tag{1}\\]\n\nThis notation should make sense eventually!\n\nIn R, we can simulate this process using the following function:\n\nrbinom(n, size, prob): draw n samples from the binomial distribution with some \\(n\\) (size) and some \\(p\\) (prob). Yes, this transition from \\(n\\) to size is confusing!\nWhen \\(n = 1\\) (or size = 1), we usually call this a “Bernoulli distribution.”\n\n\nCode\n## 10 random draws from the DGP:\nrbinom(10, size = 3, prob = 1/2)\n\n\n [1] 1 2 1 1 1 2 2 2 1 0\n\n\n\nThe sample space \\(\\Omega\\) for this three coin flip DGP—the set of all possible outcomes—is as follows:\n\\[\n\\Omega =\n\\begin{Bmatrix}\nHHH \\\\ HHT \\\\ HTT \\\\ TTT \\\\ TTH \\\\ THH \\\\ THT \\\\ HTH\n\\end{Bmatrix}\n\\]\n\nThis sample space is a set has 8 outcomes.\n\nIn this example we are recording the number of “heads.” This is our random variable. A random variable \\(X\\) assigns some number to the underlying sample space.\nAnd so we have the following:\n\\[\n\\Omega =\n\\begin{Bmatrix}\nHHH \\\\ HHT \\\\ HTH \\\\ THH \\\\ TTH \\\\ THT \\\\ HTT \\\\ TTT\n\\end{Bmatrix}  \\to\n\\begin{Bmatrix}\n3 \\\\ 2 \\\\ 2 \\\\ 2 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 0\n\\end{Bmatrix}\n\\tag{2}\\]\nAssigning a probability to these outcomes is relatively straightforward because the coin is fair ( \\(p = 0.5\\) ), and so each of these outcomes is equally likely. Thus, the probability of each of the outcomes in \\(\\Omega\\) is \\(1/8\\). Assigning probabilities get trickier when the coin is not fair, \\(p \\neq 0.5\\). We will ignore this issue for now!\nGlaring intensely at Equation 2 should convince you of the following statements:\n\\[\n\\begin{align}\n\\Pr(X = 3) &= 1/8, \\\\\n\\Pr(X = 2) &= 3/8, \\\\\n\\Pr(X = 1) &= 3/8, \\\\\n\\Pr(X = 0) &= 1/8\n\\end{align}\n\\]\nThis is exactly what the dbinom() does for us.\n\n\nCode\nX &lt;- c(3, 2, 1, 0) ## number of heads\ndbinom(X, size = 3, prob = 0.5)\n\n\n[1] 0.125 0.375 0.375 0.125\n\n\nNote. Doing statistical inference simply means that we start with the recorded data and then ask questions about the potential DGPs, including the potential values of corresponding parameters!\n\n\n\n\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in Statistical Inference. Springer."
  },
  {
    "objectID": "week3.html#ggplot2",
    "href": "week3.html#ggplot2",
    "title": "3  Week 3",
    "section": "3.1 ggplot2",
    "text": "3.1 ggplot2\n\n3.1.1 Introduction\nWe will look at the basic ggplot2 use using the faithful dataset, giving information on the eruption pattern of the Old Faithful geyser in Yellowstone National Park.\n\n\nCode\nlibrary(tidyverse)  ## ggplot2 is part of the tidyverse\ndata(\"faithful\")    ## this creates a copy of `faithful` in your global environment.\n\n# Basic scatterplot\nggplot(\n  data = faithful, \n  mapping = aes(x = eruptions, y = waiting)\n  ) + \n  geom_point()\n\n\n\n\n\nCode\n# Data and mapping can be given both as global (in ggplot()) or per layer\nggplot() + \n  geom_point(mapping = aes(x = eruptions, y = waiting), data = faithful)\n\n\n\n\n\nIf an aesthetic is linked to data it is put into aes()\n\n\nCode\nfaithful |&gt; \n  ggplot() + \n  geom_point(aes(x = eruptions, y = waiting, color = eruptions &lt; 3))\n\n\n\n\n\nIf you simple want to set it to a value, put it outside of aes()\n\n\nCode\nggplot(faithful) + \n  geom_point(aes(x = eruptions, y = waiting),\n             color = 'steelblue')\n\n\n\n\n\nSome geoms only need a single mapping and will calculate the rest for you—e.g., histograms and boxplots.\n\n\nCode\nggplot(faithful) + \n  geom_histogram(aes(x = eruptions))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFinally, geoms are drawn in the order they are added. The point layer is thus drawn on top of the density contours in the example below:\n\n\nCode\nggplot(faithful, aes(x = eruptions, y = waiting)) + \n  geom_density_2d() + \n  geom_point()\n\n\n\n\n\n\n3.1.1.1 Exercise\n\n\n\n\n\n\nModify the code below to make the points larger squares and slightly transparent. See ?geom_point for more information on the point layer.\n\n\n\n\n\nCode\nggplot(faithful) + \n  geom_point(aes(x = eruptions, y = waiting))\n\n\n\n\n\n\n\n\nHint 1: transparency is controlled with alpha, and shape with shape\nHint 2: remember the difference between mapping and setting aesthetics\nHint 3: the shape argument can also be controlled via the following numeric values\n\n\n\n\n\nNote that shapes 21 to 25 can be assigned color (for the stroke) and fill values (shown above as pink).\n\n\n\n\n\n3.1.1.2 Exercise\n\n\n\n\n\n\nColor the two visible clusters in the histogram with different colors.\n\n\n\n\n\nCode\nggplot(faithful) + \n  geom_histogram(aes(x = eruptions))\n\n\n\n\n\n\n\n\nHint 1: For polygons you can map two different color-like aesthetics: color (the color of the stroke) and fill (the fill color)\n\n\n\n\n\n3.1.1.3 Exercise\n\n\n\n\n\n\nAdd a line that separates the two point distributions. See ?geom_abline for how to draw straight lines from a slope and intercept.\n\n\n\n\n\nCode\nggplot(faithful) + \n  geom_point(aes(x = eruptions, y = waiting))\n\n\n\n\n\n3.1.2 The “Statistics” Layer\nWe will use the mpg dataset giving information about fuel economy on different car models.\nEvery geom has a stat. This is why new data (count) can appear when using geom_bar().\n\n\nCode\ndata(\"mpg\")  ## this dataset lives in the ggplot2 package\n\nmpg |&gt; \n  ggplot(aes(x = class)) + \n  geom_bar()\n\n\n\n\n\nThe stat can be overwritten. If we have pre-computed count we don’t want any additional computations to perform and we use the identity stat to leave the data alone.\n\n\nCode\nmpg_counted &lt;- mpg |&gt; \n  group_by(class) |&gt; \n  summarize(count = n())\n  \nmpg_counted |&gt; \n  ggplot() + \n  geom_bar(aes(x = class, y = count), stat = 'identity')\n\n\n\n\n\nMost obvious “geom” + “stat” combinations have a dedicated geom constructor. For example, the one above is available directly as geom_col().\n\n\nCode\nggplot(mpg_counted) + ## `mpg_counted` is a summarized version of `mpg`\n  geom_col(aes(x = class, y = count))\n\n\n\n\n\n\n\n\n\n\n\nTypical geoms that rely heavily on “statistics layers” are geom_boxplot(), geom_density(), geom_smooth(), and geom_jitter().\n\n\n\nThis is the most confusing aspect about ggplot2. Thomas Pederson says we should think about it as a “data transformation” pipeline that sits in between the input data and the geom we want to use. Don’t worry about it for now! We will skip the exercises here!\n\n\n3.1.3 Scales\nScales define how the mapping you specify inside aes() should happen. All mappings have an associated scale even if not specified explicitly.\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class))\n\n\n\n\n\nWe can take control by adding one explicitly. All scales follow the same naming conventions—i.e., scale_&lt;aes&gt;_&lt;type&gt;.\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class)) + \n  scale_color_brewer(type = 'qual')\n\n\n\n\n\nPositional mappings (x and y) also have associated scales.\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  scale_x_continuous(breaks = c(3, 5, 6)) + \n  scale_y_log10()\n\n\n\n\n\n\n3.1.3.1 Exercise\n\n\n\n\n\n\nUse RColorBrewer::display.brewer.all() to see all the different palettes from Color Brewer and pick your favorite. Modify the code below to use it.\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class)) + \n  scale_color_brewer(type = 'qual')\n\n\n\n\n3.1.3.2 Exercise\n\n\n\n\n\n\nModify the code below to create a bubble chart (scatterplot with size mapped to a continuous variable) showing cyl with size. Make sure that only the present amount of cylinders (4, 5, 6, and 8) are present in the legend.\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class)) + \n  scale_color_brewer(type = 'qual')\n\n\n\n\n\n\n\n\nHint: The breaks argument in the scale is used to control which values are present in the legend.\n\n\n\n\n\n3.1.3.3 Exercise\n\n\n\n\n\n\nModify the code below so that color is no longer mapped to the discrete class variable, but to the continuous cty variable. What happens to the guide?\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class, size = cty))\n\n\n\n\n\n\n\n\nThe type of guide can be controlled with the guide argument in the scale, or with the guides() function. Continuous colors have a gradient color bar by default, but setting it to legend will turn it back to the standard look. What happens when multiple aesthetics are mapped to the same variable and uses the guide type?\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = cty, size = cty))\n\n\n\n\n\n3.1.4 Facets\nThe facet defines how data is split among panels. The default facet (facet_null()) puts all the data in a single panel, while facet_wrap() and facet_grid() allows you to specify different types of small multiples.\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_wrap(~ class)\n\n\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_grid(year ~ drv)\n\n\n\n\n\n\n3.1.4.1 Exercise\n\n\n\n\n\n\nOne of the great things about facets is that they share the axes between the different panels. Sometimes this is undesirable though, and the behavior can be changed with the scales argument. Experiment with the different possible settings in the plot below:\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_wrap(~ drv)\n\n\n\n\n\n3.1.5 Theme\nTheming defines the feel and look of your final visualization and is something you will normally defer to the final polishing of the plot. It is very easy to change looks with a pre-built theme\n\n\nCode\nggplot(mpg) + \n  geom_bar(aes(y = class)) + \n  facet_wrap(~year) + \n  theme_minimal()\n\n\n\n\n\nFurther adjustments can be done in the end to get exactly the look you want\n\n\nCode\nggplot(mpg) + \n  geom_bar(aes(y = class)) + \n  facet_wrap(~year) + \n  labs(title = \"Number of car models per class\",\n       caption = \"source: http://fueleconomy.gov\",\n       x = NULL,\n       y = NULL) +\n  scale_x_continuous(expand = c(0, NA)) + \n  theme_minimal() + \n  theme(\n    text = element_text('Avenir Next Condensed'),\n    strip.text = element_text(face = 'bold', hjust = 0),\n    plot.caption = element_text(face = 'italic'),\n    panel.grid.major = element_line('white', linewidth = 0.5),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.ontop = TRUE\n  )\n\n\n\n\n\n\n3.1.5.1 Exercise\n\n\n\n\n\n\nThemes can be overwhelming, especially as you often try to optimize for beauty while you learn. To remove the last part of the equation, the exercise is to take the plot given below and make it as hideous as possible using the theme function. Go absolutely crazy, but take note of the effect as you change different settings.\n\n\n\n\n\nCode\nmpg |&gt; \n  ggplot(aes(y = class, fill = drv)) + \n  geom_bar() + \n  facet_wrap(~year) + \n  labs(\n    title = \"Number of car models per class\",\n    caption = \"source: http://fueleconomy.gov\",\n    x = 'Number of cars',\n    y = NULL\n  )"
  },
  {
    "objectID": "week3.html#simulation",
    "href": "week3.html#simulation",
    "title": "3  Week 3",
    "section": "3.2 Simulation",
    "text": "3.2 Simulation\n\n3.2.1 Voting Poll Example\nThere are many correct ways of simulating data.\nThis exercise takes off right were Steve left off (here). I have modified this code somewhat so that it’s easier to use for visualization—i.e., I created a function called simulation_votes(). We may eventually learn more of this, but for the moment let me explain how this function works.\nFirst, save the function to your global environment with the source() function.\n\n\nCode\nurl &lt;- \"https://raw.githubusercontent.com/acastroaraujo/socStats/main/simulation_function_week3.R\"\nsource(url)\n\n\nSecond, choose three parameters for the simulation:\n\ndem_prob_pop: the probability that the person will vote for “Democrats” in the population\nsample_size: the number of people in each “poll” (or sample)\nnum_sims: number of simulations\n\nThird, inspect the simulation data set.\n\n\nCode\nsims &lt;- simulation_votes(dem_prob_pop = 0.75, sample_size = 90, num_sims = 1e3)\nsims\n\n\n# A tibble: 90,000 × 5\n      id vote  dem_prob_pop sample_size num_sims\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;   \n 1     1 Rep   0.75         90          1000    \n 2     1 Dem   0.75         90          1000    \n 3     1 Dem   0.75         90          1000    \n 4     1 Dem   0.75         90          1000    \n 5     1 Dem   0.75         90          1000    \n 6     1 Dem   0.75         90          1000    \n 7     1 Dem   0.75         90          1000    \n 8     1 Rep   0.75         90          1000    \n 9     1 Dem   0.75         90          1000    \n10     1 Dem   0.75         90          1000    \n# ℹ 89,990 more rows\n\n\nThe following chunk of code replicates what we did in class:\n\n\nCode\n## First I'll set up the ggplot2 theme I personally like best.\n## You might not have this font if you are on a Windows computer\ntheme_set(theme_light(base_family = \"Avenir Next Condensed\")) \n\nsims &lt;- simulation_votes(dem_prob_pop = 0.52, sample_size = 300, num_sims = 500)\n\nresults &lt;- sims |&gt; \n  group_by(id) |&gt; \n  summarize(dem_prop = mean(vote == \"Dem\"))\n\nresults\n\n\n# A tibble: 500 × 2\n      id dem_prop\n   &lt;int&gt;    &lt;dbl&gt;\n 1     1    0.507\n 2     2    0.533\n 3     3    0.49 \n 4     4    0.557\n 5     5    0.54 \n 6     6    0.49 \n 7     7    0.487\n 8     8    0.527\n 9     9    0.523\n10    10    0.547\n# ℹ 490 more rows\n\n\nCode\n# plot the results\n\nresults |&gt; \n  ggplot(aes(x = dem_prop)) +\n  geom_histogram(color = \"white\", boundary = .5, binwidth = .01) +\n  labs(title = \"Simulation\", subtitle = \"dem_prob = 0.52, sample_size = 300, num_sim = 500\")\n\n\n\n\n\nCode\n# how often does the poll predict the winner?\nmean(results$dem_prop &gt; 0.5)\n\n\n[1] 0.764\n\n\nCode\n# shade the same plot\nresults &lt;- results |&gt; \n  mutate(winner = if_else(dem_prop &gt; 0.5, \"Dem\", \"Rep\"))\n\nresults |&gt; \n  ggplot(aes(x = dem_prop, fill = winner)) +\n  geom_histogram(color = \"white\", boundary = .5, binwidth = .01) +\n  scale_fill_brewer(palette = \"Set1\", direction = -1)\n\n\n\n\n\nCode\n# strip plot\nresults |&gt; \n  ggplot(aes(dem_prop, \"\")) +\n  geom_boxplot(outlier.shape = NA) + \n  geom_jitter(height = 1/5, alpha = 0.2)\n\n\n\n\n\nCode\n# density plot\nresults |&gt; \n  ggplot(aes(dem_prop)) + \n  geom_density(fill = \"grey90\") + \n  geom_vline(xintercept = 0.5, linetype = \"dashed\")\n\n\n\n\n\n\n3.2.1.1 Exercise\n\n\n\n\n\n\nIn the simulation above, what is the average dem_prop? What is the standard deviation of dem_prop? How does this change for different values of sample_size?\n\n\n\n\n\n3.2.1.2 Exercise\n\n\n\n\n\n\nCreate five different simulations with different values of sample_size (e.g., 50, 200, 500, 1000, 2000). Put them together into a single dataset and then visualize the results using boxplots. What is going on?\n\n\n\n\n\n\n\n\n\nHint: You can stack together different datasets using the bind_rows() function in the dplyr package.\nHint: You will have to group_by(id, sample_size) to calculate dem_prop.\n\n\n\n\n\n3.2.1.3 Exercise\n\n\n\n\n\n\nCreate five different simulations with different values of dem_prob_pop (e.g., 0.49, 0.52, 0.55, 0.58). Put them together into a single dataset and then visualize the results using boxplots. What is going on?\n\n\n\n\n\n\n\n\n\nHealy, Kieran. 2018. Data Visualization: A Practical Introduction.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "week4.html#exercise",
    "href": "week4.html#exercise",
    "title": "4  Week 4",
    "section": "4.1 Exercise",
    "text": "4.1 Exercise\nSo far, we have generated data using the sample() and rbinom() functions. R has a built-in function for generating data from normal distributions, it’s called rnorm().\nRecall that all normal distributions exhibit the following behavior:\n\nThe probability that \\(x\\) is within one standard deviation \\(\\sigma\\) away from the mean is roughly 68%.\nThe probability that \\(x\\) is within two standard deviations \\(2\\sigma\\) away from the mean is roughly 95%.\nThe probability that \\(x\\) is above the mean ( \\(x \\geq \\mu\\) ) is 50%.\nThe probability that \\(x\\) is below the mean ( \\(x \\leq \\mu\\) ) is also 50%.\n\nThe following chunk generates 100,000 observations from a normal distribution with mean = 0 and standard deviation = 1.\n\n\nCode\nx &lt;- rnorm(100000, mean = 0, sd = 1)\n\n\n\n\n\n\n\n\nUse the mean() function to verify that x exhibits those four behaviors.\n\n\n\n\n\n\n\n\n\nHint: Remember that you can coerce numeric variables into TRUE and FALSE values using logical operators (==, &gt;, &lt;, etc.)"
  },
  {
    "objectID": "week4.html#exercise-1",
    "href": "week4.html#exercise-1",
    "title": "4  Week 4",
    "section": "4.2 Exercise",
    "text": "4.2 Exercise\n\n\n\n\n\n\nUse the quantile() function on the x created in the previous exercise. Explain the results."
  },
  {
    "objectID": "week4.html#exercise-2",
    "href": "week4.html#exercise-2",
    "title": "4  Week 4",
    "section": "4.3 Exercise",
    "text": "4.3 Exercise\n\n\n\n\n\n\nNow modify the probs argument in the quantile() to find the 0.5% and 99.5% percentiles of x.\n\n\n\n\n\n\n\n\n\nHint: If you Google “99 percent confidence interval” you should see a table that foreshadows the answer—i.e., you should get some number roughly equal to -2.576 and 2.576 respectively."
  },
  {
    "objectID": "week4.html#exercise-3",
    "href": "week4.html#exercise-3",
    "title": "4  Week 4",
    "section": "4.4 Exercise",
    "text": "4.4 Exercise\n\n\n\n\n\n\nUse the mean() function to verify that the probability of \\(x\\) being between \\(-2.576\\) and \\(2.576\\) is roughly 99%.\n\n\n\n\n\n\n\n\n\nHint: The logical operator for “AND” is &."
  },
  {
    "objectID": "week4.html#exercise-4",
    "href": "week4.html#exercise-4",
    "title": "4  Week 4",
    "section": "4.5 Exercise",
    "text": "4.5 Exercise\nCentral Limit Theorem\n\n\n\n\n\n\nLet \\(x = x_1 + \\dots + x_{20}\\), the sum of 20 independent uniform(0, 1) random variables. In R, create 1000 simulations of \\(x\\) and plot their histogram.\n\n\n\n\n\n\n\n\n\nHint: You can simulate the sum of 20 independent uniform random variables using the following code:\n\n\nCode\nsum(runif(n = 20, min = 0, max = 1))\n\n\n[1] 8.229349\n\n\nThe simulation you create will have 1000 instances of a value like this.\n\n\n\nThe sampling distribution is the set of possible sample “statistics” or “data summaries” (e.g., means) that could have been observed, if the data collection process had been repeated many many times. There’s an important theorem in statistics called the Central Limit Theorem (CLT) that proves that this sampling distribution converges to a normal distribution when the sample size is big enough, regardless of how our data looks like.\nThe standard error is how we estimate of the the standard deviation of the sampling distribution.\n\n\n\n\n\n\nBonus:\nUse the sd() function on the sampling distribution simulated earlier, then compare this value to the standard error given by the formula for the standard error of a sum of 20 uniform(0, 1) random variables:\n\\[\n\\text{se}(x) = \\sqrt{\\frac{20}{12}}\n\\]\nSee that? We can calculate the standard errors of different “data summaries” without having to think too much about the math.1"
  },
  {
    "objectID": "week4.html#exercise-5",
    "href": "week4.html#exercise-5",
    "title": "4  Week 4",
    "section": "4.6 Exercise",
    "text": "4.6 Exercise\nSimulation study of CLT\nMany introductory statistics textbooks say that “sample sizes equal to or greater than 30 are often considered sufficient for the CLT to hold.”\n\n\n\n\n\n\nWrite down intuitively what you think this means.\n\n\n\nNow lets revisit last class’ simulation exercise (click here).\nYou’ll notice that the sample size is controlled by the following object:\n\n\nCode\nsvy_size &lt;- 2247  # number of people in each \"poll\"\n\n\nAnd the “true” population proportion is given by:\n\n\nCode\nest_prop &lt;- 1/3\n\n\nIn class, we also calculated the standard error “analytically” using the following mathematical equation:\n\\[\n\\text{se}_x = \\sqrt{\\frac{p (1-p)}{n}}\n\\]\nOr in code:\n\n\nCode\nstd_error &lt;- sqrt((1/3) * (2/3) / svy_size)\nstd_error\n\n\n[1] 0.009944712\n\n\nThis means that the 95% confidence interval should be roughly 2 standard errors away from 0.33\n\n\nCode\n0.33 - 1.96*std_error ## \"1.96\" is roughly \"2\"!\n\n\n[1] 0.3105084\n\n\nCode\n0.33 + 1.96*std_error\n\n\n[1] 0.3494916\n\n\nIn Steve’s simulation, this is given by ci95.\n\n\n\n\n\n\nRepeat Steve’s simulation with different values of svy_size. Is the 95% confidence interval still roughly 2 standard errors away from 0.33?\nDo these results change your initial interpretation of the idea that “sample sizes equal to or greater than 30 are often considered sufficient for the CLT to hold” ?"
  },
  {
    "objectID": "week4.html#footnotes",
    "href": "week4.html#footnotes",
    "title": "4  Week 4",
    "section": "",
    "text": "We can estimate the standard error for one sample using sd(runif(20)) * sqrt(20). The answer should be close to the “true” value (1.29), but it shouldn’t match.↩︎"
  },
  {
    "objectID": "week5.html#probability-primer",
    "href": "week5.html#probability-primer",
    "title": "5  Week 5",
    "section": "5.1 Probability Primer",
    "text": "5.1 Probability Primer\nKeep track of the bold words.\nA random variable assigns some number to each outcome of a random event.\nSuppose we flip a coin exactly one time. Then the set of all outcomes—or sample space—is given by the following set:\n\\[\n\\Omega = \\{H, T\\}\n\\]\n\nAn event is a well-defined subset of \\(\\Omega\\).\n\nIf we assume that the coin is fair—i.e., that both events are equally likely—then we get the following probabilities.\n\\[\n\\begin{align}\n\\Pr(H) = \\frac{1}{2}, && \\Pr(T) = \\frac{1}{2}\n\\end{align}\n\\]\nA probability measure assigns a number between 0 and 1 to every event in the sample space ( \\(\\Omega\\) ). Moreover, the sum of all probabilities must equal one. Those are the rules!\nGoing back to the notion of a random variable, if we are recording the number of “heads,” then \\(H\\) gets assigned 1 and \\(T\\) gets assigned to 0.\n\\[\n\\begin{align}\nX\\Big(\\{H\\}\\Big) = 1, && X\\Big(\\{T\\}\\Big) = 0\n\\end{align}\n\\]"
  },
  {
    "objectID": "week5.html#bernoulli-distribution",
    "href": "week5.html#bernoulli-distribution",
    "title": "5  Week 5",
    "section": "5.2 Bernoulli distribution",
    "text": "5.2 Bernoulli distribution\nThese are the building blocks of what’s known as a Bernoulli probability distribution, which takes the following form:\n\\[\n\\Pr(X = x) = \\begin{cases}\n    p &\\text{if} \\ x = 1 \\\\\\\\\n    1 - p &\\text{if} \\ x = 0 \\\\\\\\\n    0 &\\text{if} \\ x = \\text{anything else}\n\\end{cases}\n\\tag{5.1}\\]\nNote. If we flip a fair coin, we are basically saying that \\(X\\) will follow a Bernouilli distribution with \\(p = 0.5\\).\n\n\n\n\n\n\nYou should be able to define in your own words the following terms: random variable, sample space, and event.\n\n\n\nThe cumulative distribution function returns the probability that an outcome is less than or equal to \\(x\\). It looks exactly like this:\n\\[\n\\Pr(X \\leq x) = \\begin{cases}\n    0 &\\text{if} \\ x &lt; 0 \\\\\\\\\n    1 - p &\\text{if} \\ x = 0 \\\\\\\\\n    1 &\\text{if} \\ x = 1 \\\\\\\\\n    1 &\\text{if} \\ x &gt; 1\n\\end{cases}\n\\tag{5.2}\\]\n\n5.2.1 Exercise\nThe Bernoulli distribution is a special case of the binomial distribution. If we set the argument size = 1, we basically have a Bernouilli distribution.\nSuppose that \\(p = 2/3\\).\n\n\n\n\n\n\nUse dbinom() to verify the results in Equation 5.1.\nUse pbinom() to verify the results in Equation 5.2.\nUse rbinom() to generate 100 samples from the Bernouilli distribution with prob = 2/3. Plot the results in ggplot using geom_bar."
  },
  {
    "objectID": "week5.html#binomial-distribution",
    "href": "week5.html#binomial-distribution",
    "title": "5  Week 5",
    "section": "5.3 Binomial distribution",
    "text": "5.3 Binomial distribution\nThe binomial distribution comes from adding \\(n\\) Bernouilli distributions. Thus, it has two parameters: \\(n\\) and \\(p\\). This “\\(n\\)” corresponds to the size argument in the R functions we’ve been working with.\n\n5.3.1 Exercise\nSuppose we flip a fair coin exactly four times.\n\n\n\n\n\n\nDraw the sample space \\(\\Omega\\) for this “experiment.”\nNote. You can pencil and paper and insert a photo of the image if you want to. But I encourage you to play around with the \\(\\LaTeX\\) formulas.\n\n\n\n\n\n\n\n\n\nHint: There’s this little thing in combinatorics called the multiplication rule. The number of ways in which we can flip four coins is \\(2 \\times 2 \\times 2 \\times 2 = 16\\). Thus, your sample space should consist of 16 events.\n\n\n\n\n\n5.3.2 Exercise\nTurn the previous sample space into a random variable that counts the number of heads ( \\(H\\) ) in each event.\n\n\n\n\n\n\nHow many possible ways are there to get \\(X = 0\\), \\(X = 1\\), \\(X = 2\\), \\(X = 3\\), and \\(X = 4\\)?\nWhat is the probability that \\(X = 2\\)?\nVerify this by using the correct R function to calculate the probability that \\(X = 2\\)?\nUse the correct R function to calculate the probability that \\(X \\leq 1\\)?\n\n\n\n\n\n5.3.3 Exercise\n\n\n\n\n\n\nUse rbinom() to generate 1000 samples from the Binomial distribution with size = 5 and prob = 2/3. Plot the results in ggplot using geom_bar.\nEstimate the probability that \\(X\\) is an even number using the mean() function on those 1000 values.\n\n\n\n\n\n\n\n\n\nYou can calculate (almost) the same number by adding up the following values:\n\n\nCode\ndbinom(c(2, 4), size = 5, prob = 2/3)\n\n\n[1] 0.1646091 0.3292181\n\n\n\n\n\n\n\n5.3.4 Unfair Coins and Independence\nSo far, you’ve seen that it’s easy to calculate probabilities by hand when we have fair coins—i.e., when \\(p=1/2\\). It’s just addition and division.\nBut what happens when \\(p \\neq 0.5\\)?\nIf we assume that these coin flips are independent—i.e., that getting \\(H\\) during the first flip doesn’t affect the chances of getting \\(H\\) during the second flip—then we have a very convenient of calculating joint probabilities.\nWe say that two events \\(A\\) and \\(B\\) are independent if and only if \\(\\Pr(A \\cap B) = \\Pr(A) \\times \\Pr(B)\\).\nIn terms of fair coin flipes, we would have that the probability of getting a sequence of \\(HTHH\\) is the same as:\n\\[\n\\Pr(HTHH) = \\Pr(H)\\Pr(T)\\Pr(H)\\Pr(H) = \\frac{1}{16}\n\\]\nThis same procedure holds if the coin is unfair.\nFor example:\nIf \\(\\Pr(H) = \\frac{1}{3}\\), then\n\\[\n\\Pr(HTHH) = \\frac{1}{3} \\cdot \\frac{2}{3} \\cdot \\frac{1}{3} \\cdot \\frac{1}{3} = \\frac{2}{81}\n\\]\n\n\n5.3.5 Exercise\n\n\n\n\n\n\nRevisit what you did Section 5.3.1 and Section 5.3.2.\nWhat is the probability that \\(X = 2\\) when \\(p = 1/3\\) and \\(n = 4\\)?\nVerify this by using the correct R function to calculate the probability that \\(X = 2\\)?\n\n\n\n\n\n5.3.6 Binomial distribution, the formula\nLooking back at what you’ve done, you should be able to understand that the formula for a probability distribution with \\(n=4\\) has a sample space with 16 events, and that there are five possible outcomes \\(X = \\{0, 1, 2, 3, 4 \\}\\).\nThe Binomial probability distribution, with parameters \\(p\\) and \\(n = 4\\) takes the following form:\n\\[\n\\Pr(X = x) = \\begin{cases}\n    1 \\ (1 - p)^4 &\\text{if} \\ x = 0 \\\\\n    4 \\ p(1 - p)^3 &\\text{if} \\ x = 1 \\\\\n    6 \\ p^2(1-p)^2 &\\text{if} \\ x = 2 \\\\\n    4 \\ p^1(1-p)^3 &\\text{if} \\ x = 3 \\\\\n    1 \\ p^4 &\\text{if} \\ x = 4 \\\\\n    0 &\\text{if} \\ x = \\text{anything else}\n\\end{cases}\n\\]\nThe 1s, 4s, and and 6s come from counting the number of ways in which \\(X\\) can equal one of these numbers. You should have gotten these results in Section 5.3.2. If you didn’t, go back because you did something wrong!\nHowever, this is not how you’ll see binomial distributions written out in the wild. We need new notation in order to write any binomial distribution, which we get by using the so-called binomial coefficient:\n\\[\n{n \\choose x} = \\frac{n!}{x! (n - x)!}\n\\]\n\nYou might remember some of this from the GRE.\n\nSo the probability (mass) function of any binomial distribution is then this:\n\\[\n\\Pr(X = x) = {n \\choose x} p^x (1-p)^{n-x}\n\\tag{5.3}\\]\nAnd the cumulative distribution function is as follows:\n\\[\n\\Pr(X \\leq x) = \\sum_{i = 0}^x {n \\choose x} p^x (1-p)^{n-x}\n\\tag{5.4}\\]\n\n\n5.3.7 Exercise\nSuppose that \\(p = 2/3\\) and \\(n = 15\\)\n\n\n\n\n\n\nUse dbinom() to verify the results in Equation 5.3.\nUse pbinom() to verify the results in Equation 5.4.\n\n\n\n\n\n\n\n\n\nHint: You should strive to verify these results using R. There’s a function called choose() which calculates the binomial coefficient.\n\n\n\n\n\n\n\n\n\nHint: Note that because pbinom is just adding different pieces of dbinom together, the following two lines of code are equivalent:\n\n\nCode\npbinom(q = seq(-1, 6), size = 5, prob = 0.75)\n\n\n[1] 0.0000000000 0.0009765625 0.0156250000 0.1035156250 0.3671875000\n[6] 0.7626953125 1.0000000000 1.0000000000\n\n\nCode\ncumsum(dbinom(x = seq(-1, 6), size = 5, prob = 0.75))\n\n\n[1] 0.0000000000 0.0009765625 0.0156250000 0.1035156250 0.3671875000\n[6] 0.7626953125 1.0000000000 1.0000000000"
  },
  {
    "objectID": "week5.html#likelihoods",
    "href": "week5.html#likelihoods",
    "title": "5  Week 5",
    "section": "5.4 Likelihoods",
    "text": "5.4 Likelihoods\nLikelihood are similar to a probabilities, except that we think of them as a function of parameters ( \\(X\\) is “fixed”); in the previous exercises, we assumed fixed parameters and tried to answer questions about \\(X\\). Now we do the opposite.\nThis introduces one weird implication:\nSuppose we have a binomial distribution with \\(p = 0.5\\) and \\(n = 55\\). Then, summing over all possible values of \\(X\\) should add up to \\(1\\).\n\n\nCode\nx &lt;- 0:55\nsum(dbinom(x, size = 55, prob = 1/2))\n\n\n[1] 1\n\n\nThis does not happen if we take the same formula as a function of the parameter \\(p\\) with \\(X = 22\\).\n\n\nCode\np &lt;- seq(0, 1, by = 0.001)\nsum(dbinom(x = 22, size = 55, prob = p))\n\n\n[1] 17.85714\n\n\nFurthermore, we didn’t even sum over all the possible values of \\(p\\). The vector p (in this example) is of length 1001, but \\(p\\) can actually take an infinite range of values! This is why Steve did the the whole “normalizing” thing in last week’s class in order to turn the likelihoods into a probability distribution for \\(p\\).\n\n—What? Did you just give probability \\(p\\) it’s own probability distribution?\n—Yes.\n\n\n5.4.1 Exercise\nLet’s revisit Steve’s grid approximation code.\nI’ll give you a start.\n\n\nCode\nlibrary(tidyverse)\ngrid &lt;- tibble(prob = seq(0, 1, by = 0.001))\ngrid$like &lt;- dbinom(21, 47, grid$prob) ## MAKE SURE YOU USE THESE VALUES\n\n\n\n\n\n\n\n\nFollowing Steve’s code, calculate clike_raw and clike_normalized.\nLooking at this data frame, what are the chances that prob is equal to or greater than 0.588?\n\n\n\n\nModified on 2023-10-01.\nThis question used to read “what probability would you assign to prob having a value equal to or greater than 0.588?”. Hopefully the wording is easier to understand.\n\n\n\n\n\n\n\nHint: You might want to look at something like this:\n\n\nCode\ngrid |&gt; filter(prob == 0.588)"
  },
  {
    "objectID": "week5.html#footnotes",
    "href": "week5.html#footnotes",
    "title": "5  Week 5",
    "section": "",
    "text": "We will get to the meaning of this “mass” word here in a future exercise when we work with continuous random variables. A continuous random can take an infinite number of possible values. Since the number of outcomes is uncountable, we need to take a different approach to how we compute its probability distribution. For now, we can safely ignore this technicality.↩︎"
  },
  {
    "objectID": "week6.html#the-truth-is-known",
    "href": "week6.html#the-truth-is-known",
    "title": "6  Week 6",
    "section": "6.1 The Truth is Known",
    "text": "6.1 The Truth is Known\n\n6.1.1 Sampling Distributions\nWe have already seen sampling distributions in class.\nFor example, suppose we know that the true proportion of people voting Democrats is \\(0.53\\). If we decide to poll \\(1000\\) individuals, what can we say beforehand about the unobserved results?\nThe following chunk of code simulates this sampling distribution.\n\n\nCode\nS &lt;- 1e4 ## number of simulated draws\npoll_size &lt;- 1000 ## sample size\n\ndraws &lt;- rbinom(S, size = poll_size, prob = 0.53)\nproportions &lt;- draws / poll_size\n\ntibble(proportions) |&gt; \n  ggplot(aes(proportions)) + \n  geom_histogram(color = \"white\", boundary = 0.5, binwidth = 0.005) +\n  labs(title = \"Sampling Distribution of p = 0.53 and n = 1000\")\n\n\n\n\n\n\nThe sampling distribution is the set of possible sample “statistics” or “data summaries” (e.g., means, proportions) that could have been observed, if the data collection process had been repeated many many times.\n\nThis sampling distribution is centered around the true value of \\(0.53\\).\n\n\nCode\nmean(proportions)\n\n\n[1] 0.5301697\n\n\nThe standard error is the standard deviation of the sample distribution.\n\n\nCode\nse &lt;- sd(proportions)\nse\n\n\n[1] 0.01595023\n\n\nFurthermore, this sampling distribution looks normal, which is something we come to expect because of the Central Limit Theorem.\nIn a normal distribution:\n\nWe expect 68% of the values to be within 1 standard deviations around the center.\n\n\nCode\nlower &lt;- 0.53 - 1*se\nupper &lt;- 0.53 + 1*se\nmean(proportions &gt;= lower & proportions &lt;= upper)\n\n\n[1] 0.6642\n\n\nWe expect 95% of the values to be roughly within 2 standard deviations around the mean.\n\n\nCode\nlower &lt;- 0.53 - 2*se\nupper &lt;- 0.53 + 2*se\nmean(between(proportions, lower, upper)) ## using dplyr's between function\n\n\n[1] 0.9532\n\n\n\nNow that we know this we can ask questions such as what is the probability that a poll of 1000 people tells us that the proportion of people voting Democrats is less than or equal to \\(0.5\\)?\n\n\nCode\nmean(proportions &lt;= 0.5)\n\n\n[1] 0.0317\n\n\nSo, the probability is very low. It seems that a poll of 1000 is very likely to declare the correct winner. Researchers do this kind of calculation all the time to make sure their sample size is sufficiently large for their purposes.\nSampling distributions without simulations (aka using Math)\nBecause of the CLT, we can construct some sampling distributions simply by calculating the center and the spread.\nWe know the center is \\(0.53\\), and there’s a convenient formula for calculating the standard error of a proportion:\n\\[\n\\text{se}_p = \\sqrt{\\frac{p(1-p)}{n}} = \\sqrt{\\frac{0.53 \\times 0.47}{1000}} \\approx 0.01578\n\\]\n\nLearning this formula is worth your time!\n\nWe can overlay this “analytical” sampling distribution on top of our simulated sampling distribution like this:\n\n\nCode\ntibble(proportions) |&gt; \n  ggplot(aes(proportions)) + \n  geom_histogram(\n    color = \"white\", boundary = 0.5, binwidth = 0.005,\n    ## this is a little hack to ensure the y-axis are the same\n    mapping = aes(y = after_stat(density))\n  ) +\n  labs(title = \"Sampling Distribution of p = 0.53 and n = 1000\") +\n  ## adding the normal distribution on top\n  geom_function(fun = \\(x) dnorm(x, mean = 0.53, sd = se)) \n\n\n\n\n\n\nKeep in mind that the output of dnorm is labelled “density” whereas we had been calling output of dbinom a “probability.” That’s because binomial distributions are discrete, whereas normal distributions as continuous. We can’t really assign probabilities to continuous numbers because there are infinitely many of them and the math gets weird; thus, we think about them a little differently.\n\nWe can ask the same question as before: What is the probability that a poll of 1000 people tells us that the proportion of people voting Democrats is less than or equal to \\(0.5\\)? This probability is a tail-area probability. We call them like that because they correspond to that little shaded area under the curve.\n\nP-values are also tail area probabilities!\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npnorm(0.5, mean = 0.53, sd = sqrt(0.53*0.47/1000))\n\n\n[1] 0.02866469\n\n\n\n\n6.1.2 Exercise\n\n\n\n\n\n\nChoosing sample size.\nYou are designing a survey to estimate the proportion of individuals who will vote Democrat. Assuming that respondents are a simple random sample of the voting population, how many people do you need to poll so that the standard error is less than 5 percentage points?\n\n\n\n\n\n6.1.3 Confidence Intervals\nConfidence intervals are constructed from the data we get to observe. In the example above, we established that the true population parameter was equal to \\(0.53\\) and that the sample size was equal to \\(1000\\).\nThis is how one dataset could look like:\n\n\nCode\nset.seed(321)\none_dataset &lt;- rbinom(poll_size, size = 1, prob = 0.53)\nglimpse(one_dataset) ## one thousand observations of 1s and 0s\n\n\n int [1:1000] 0 0 1 1 1 1 1 1 1 0 ...\n\n\nThe estimated proportion from this one dataset is calculated as follows:\n\n\nCode\nprop_hat &lt;- mean(one_dataset)\nprop_hat\n\n\n[1] 0.513\n\n\nIt’s increasingly common practice to estimate standard errors using a computational approximation technique called the bootstrap.\nBootstrap resampling is done with replacement; that is, the same data point can appear multiple times in a resampled dataset. This is necessary, as otherwise it would not be possible to get new datasets of the same size as the original.\nThe following chunk of code estimates 10,000 resampled proportions.\n\n\nCode\nboot_stats &lt;- replicate(1e4, {\n  resample &lt;- sample(one_dataset, replace = TRUE)\n  mean(resample)\n})\n\nglimpse(boot_stats)\n\n\n num [1:10000] 0.547 0.525 0.521 0.499 0.484 0.52 0.507 0.517 0.494 0.516 ...\n\n\nWe can then construct a confidence interval using the quantile() function on the bootstrap statistics.\n\n\nCode\nci95 &lt;- quantile(boot_stats, c(0.025, 0.975))\nci95\n\n\n 2.5% 97.5% \n0.482 0.544 \n\n\nThis is how the bootstrap proportions—and the 95% confidence interval—compare to the “true” sampling distribution:\n\n\nCode\nggplot() + \n  geom_histogram(\n    data = tibble(proportions),\n    mapping = aes(proportions, fill = \"sampling distribution\"),\n    color = \"white\",\n    boundary = 0.5,\n    binwidth = 0.005\n  ) + \n  geom_histogram(\n    data = tibble(boot_stats),\n    mapping = aes(boot_stats, fill = \"bootstrap proportions\"),\n    color = \"white\",\n    alpha = 1/2, \n    boundary = 0.5,\n    binwidth = 0.005\n  ) +\n  geom_vline(xintercept = ci95, linetype = \"dashed\") +\n  labs(fill = NULL, title = \"95% Confidence Interval\", y = \"count\") +\n  annotate(\n    \"text\", x = prop_hat, y = 500, size = 5,\n    ## hack for LaTeX expressions in graphs\n    label = latex2exp::TeX(paste(\"\\\\hat{p} =\", prop_hat))\n  )\n\n\n\n\n\nConfidence intervals without simulations (aka using Math)\nWe can also estimate standard errors “analytically” using the following formula:\n\\[\n\\widehat{\\text{se}_p} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\n\nNote the use of the “hat” symbol in \\(\\hat p\\) to indicate that these are estimates from a dataset.\n\n\n\nCode\nse_hat &lt;- sqrt(prop_hat * (1 - prop_hat) / 1000)\nse_hat\n\n\n[1] 0.01580604\n\n\nAssuming that the normal distribution kicks in because of the CLT, we can estimate the 95% confidence interval using the qnorm() function:\n\n\nCode\nqnorm(c(0.025, 0.975), mean = prop_hat, sd = se_hat)\n\n\n[1] 0.4820207 0.5439793\n\n\nIn this case, both confidence intervals are identical. But we did have to make more assumptions (e.g., using a normal distribution)! And we also had to use formulas that we might not understand (e.g., the formula for \\(\\widehat{\\text{se}_p}\\) )!\n\n\nCode\nprop_hat &lt;- mean(one_dataset)\nse_hat &lt;- sqrt(prop_hat * (1 - prop_hat) / 1000)\nci95 &lt;- qnorm(c(0.025, 0.975), mean = prop_hat, sd = se_hat)\n\nggplot() +\n  stat_function(\n    geom = \"area\",\n    fun = \\(x) dnorm(x, 0.53, sd = sqrt(0.53*0.47/1000)),\n    fill = \"steelblue1\"\n  ) +\n  stat_function(\n    geom = \"area\",\n    fun = \\(x) dnorm(x, prop_hat, se_hat),\n    fill = \"pink\", alpha = 4/5\n  ) + \n  geom_vline(xintercept = ci95, linetype = \"dashed\") +\n  scale_x_continuous(breaks = seq(0.43, 0.6, 0.02), limits = c(0.43, 0.6)) +\n  labs(x = \"proportions\", y = \"density\", title = \"95% Confidence Interval\")\n\n\n\n\n\n\n\n6.1.4 NHST\nNull Hypothesis Statistical Testing also involves the construction of sampling distributions—i.e., we first construct the sampling distribution of the “null model.”\nThis procedure can be quite mechanical:\n\nWe assume a hypothesis we would like to refute: a null hypothesis often denoted as \\(H_0\\).\nWe choose a test statistic (e.g., an average, a proportion), which is some function of the observed data.\nWe derive the sampling distribution of the test statistic, given the null hypothesis. This distribution is also called the reference distribution.\nWe ask whether the observed value of the test statistic is likely to occur under the reference distribution. This probability is also known as a \\(p\\)-value.\nThe \\(p\\)-value is the probability that, under the null hypothesis, we observe a value of the test statistic at least as extreme as the one we actually observed.\nWe reject the null hypothesis if the \\(p\\)-value is less than or equal to the confidence level \\(\\alpha\\). Otherwise, we retain the null hypothesis (i.e., we fail to reject the null hypothesis).\n\nFor example, continuing the previous example, suppose that our null model ( \\(H_0\\) ) is that the proportion of people who vote Democrat and Republican is the same—i.e., that \\(p = 0.5\\).\nWe can simulate the null distribution as follows:\n\n\nCode\nS &lt;- 1e4 ## number of simulated draws\npoll_size &lt;- 1000 ## sample size\n\ndraws &lt;- rbinom(S, size = poll_size, prob = 0.50)\nnull &lt;- draws / poll_size\n\n\n\nThis is almost the exact same code that we used before. Only the prob argument has changed.\n\nWe can overlay the null distribution on top of the sampling distribution for the “true” parameter as follows:\n\n\nCode\nggplot() + \n  geom_histogram(\n    data = tibble(proportions),\n    mapping = aes(proportions, fill = \"True Sampling Distribution\"),\n    color = \"white\", boundary = 0.5, binwidth = 0.005,\n  ) +\n  geom_histogram(\n    data = tibble(null),\n    mapping = aes(null, fill = \"Null Sampling Distribution\"),\n    color = \"white\", boundary = 0.5, binwidth = 0.005,\n    alpha = 1/2\n  ) +\n  scale_x_continuous(breaks = seq(0.43, 0.61, 0.02), limits = c(0.43, 0.61)) +\n  labs(fill = NULL)\n\n\n\n\n\n\n\n6.1.5 Exercise\n\n\n\n\n\n\nWhat is the probability of observing the “true” value ( \\(p = 0.53\\) ) under the null?\nWhat is the probability of observing prop_hat under the null? Is this statistically significant if the confidence level ( \\(\\alpha\\) ) is set to 0.05?\n\n\n\n\n\n\n\n\n\nHint: As a reminder, this is how I calculated prop_hat earlier:\n\n\nCode\nset.seed(321)\none_dataset &lt;- rbinom(poll_size, size = 1, prob = 0.53)\nprop_hat &lt;- mean(one_dataset)\n\n\nHere, prop_hat is 0.513. If you remove the set.seed(321) line you will get a different value of prop_hat. I encourage you to play around with this.\n\n\n\n\n\n\n\n\n\nHint: You can reach the same decision with NHST and confidence intervals. If your \\(H_0\\) is included within the confidence interval, we then fail to reject the null.\n\n\n\n\n\n6.1.6 Difference in Proportions\nWe can repeat everything we did before for other scenarios; for example, a difference in proportions.\nSuppose we have to groups of people (e.g., men and women) who differ in the proportion of voting Democrat.\nFor example, we can assume that \\(p_1 = 0.5\\) and that \\(p_2 = 0.6\\).\nHere, the true difference in proportions is \\(\\theta = p_1 - p_2 = -0.1\\).\nThe following chunk creates a sampling distribution for this new parameter \\(\\theta\\).\n\n\nCode\np1 &lt;- 0.5\nn1 &lt;- 120\np2 &lt;- 0.6\nn2 &lt;- 90\n\nS &lt;- 1e5\ndraws1 &lt;- rbinom(S, size = n1, prob = p1) \nproportions1 &lt;- draws1 / n1 \ndraws2 &lt;- rbinom(S, size = n2, prob = p2)\nproportions2 &lt;- draws2 / n2\ntheta_distribution &lt;- proportions1 - proportions2\n\n\nAs before, the sampling distribution is centered around the true value of \\(-0.1\\)\n\n\nCode\nmean(theta_distribution)\n\n\n[1] -0.09997294\n\n\nAnd the standard error is given by:\n\n\nCode\nsd(theta_distribution)\n\n\n[1] 0.06896817\n\n\n\n\nCode\ntibble(theta = theta_distribution) |&gt; \n  ggplot(aes(theta)) + \n  geom_histogram(color = \"white\", boundary = 0.5, binwidth = 0.025) +\n  labs(title = \"Sampling Distribution of differences in proportions\")\n\n\n\n\n\n\n\n6.1.7 Exercise\n\n\n\n\n\n\nThe formula for calculating the standard error of a difference in proportions is given by:\n\\[\n\\sigma_{\\hat p_1 - \\hat p_2} = \\sqrt{\\frac{p_1 (1 - p_1)}{n_1} + \\frac{p_2 (1 - p_2)}{n_2}}\n\\]\nVerify that this standard error corresponds to sd(theta_distribution).\n\n\n\n\n\n6.1.8 Exercise\n\n\n\n\n\n\nComparison of proportions.\nA randomized experiment is performed within a survey. 1000 people are contacted. Half the people contacted are promised a $5 incentive to participate, and half are not promised an incentive. The result is a 50% response rate among the treated group and 40% response rate among the control group. Give an estimate and standard error of the difference in proportions.\n\n\n\n\n\n\n\n\n\nHint: Because this is a randomized experiment, we usually refer to this difference in proportions as an average treatment effect, which we interpret as a causal effect."
  },
  {
    "objectID": "week6.html#the-data-is-known",
    "href": "week6.html#the-data-is-known",
    "title": "6  Week 6",
    "section": "6.2 The Data is Known",
    "text": "6.2 The Data is Known\nHere we will work with GSS data.\nCopy the following chunk of code to get the data frame we will be using.\n\n\nCode\ngss18 &lt;- gss_get_yr(2018) \n\nd &lt;- gss18 |&gt; \n  select(sex, attend, polviews) |&gt; \n  haven::zap_missing() |&gt; \n  mutate(sex = as_factor(sex)) |&gt; \n  haven::zap_labels() |&gt; \n  drop_na()\n\nglimpse(d)\n\n\nRows: 2,235\nColumns: 3\n$ sex      &lt;fct&gt; male, male, female, male, female, female, male, female, male,…\n$ attend   &lt;dbl&gt; 5, 2, 6, 8, 4, 7, 7, 0, 4, 5, 0, 3, 0, 7, 1, 0, 4, 5, 2, 7, 1…\n$ polviews &lt;dbl&gt; 6, 5, 4, 7, 3, 4, 5, 4, 6, 4, 4, 3, 2, 5, 2, 6, 2, 4, 6, 6, 4…\n\n\n\n6.2.1 Exercise\n\n\n\n\n\n\nGo to the GSS website and describe the values of attend and polviews—e.g., what does a value of “4” mean in polviews.\n\n\n\n\n\n6.2.2 Exercise\n\n\n\n\n\n\nRepeat what we did in class with Steve, but compare the weekly variable to a new variable call conservative.\nhttps://github.com/vaiseys/soc-stats-1/blob/main/demos/day-12.R\n\n\n\n\n\n\n\n\n\nHint: You will have to create the conservative value yourself by modifying the polviews variable.\n\n\n\n\n\n6.2.3 Exercise\n\n\n\n\n\n\nIs the difference in proportions between conservative and weekly statistically significant?\n\n\n\n\n\n6.2.4 Exercise\n\n\n\n\n\n\nInstead of discretizing the polviews and attend (which is what we did to create weekly and conservative), let’s try interpreting their original values.\nI made the following plot using geom_tile to make the point that contingency tables are hard to interpret.\nTry to describe what’s going on here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint: You can get the same values in this plot with the tabyl() function from the janitor package.\n\n\n\n\n\n6.2.5 Bonus\n\n\n\n\n\n\nDo a Chi square test on polviews and attend using the infer package.\nThe null hypothesis is that these variables are “independent” of each other.\n\n\n\n\n\n\n\n\n\nHint: You may have to transform both polviews and attend into “factor” variables.\n\n\nCode\nd &lt;- d |&gt; \n  mutate(polviews = as_factor(polviews),\n         attend = as_factor(attend))"
  },
  {
    "objectID": "week7.html#contingency-tables",
    "href": "week7.html#contingency-tables",
    "title": "7  Week 7",
    "section": "7.1 Contingency Tables",
    "text": "7.1 Contingency Tables\n\n7.1.1 Exercise\nLoad the GSS data for 2018, just like we did last week.\n\n\nCode\nlibrary(tidyverse)\nlibrary(gssr)\ngss18 &lt;- gss_get_yr(2018) \n\nd &lt;- gss18 |&gt; \n  select(attend, polviews) |&gt; \n  haven::zap_missing() |&gt; \n  haven::zap_labels() |&gt; \n  mutate(weekly = if_else(attend &gt;= 7, 1L, 0L),\n         conservative = if_else(polviews &gt;= 5, 1L, 0L)) |&gt; \n  drop_na() \n\n\nNow take a look a this table:\n\n\nCode\ntable(conservative = d$conservative, weekly = d$weekly)\n\n\n            weekly\nconservative    0    1\n           0 1243  257\n           1  495  240\n\n\n\n\n\n\n\n\nIn this sample:\n\nWhat is the probability that a person attends religious services weekly?\nI’m asking for \\(\\Pr(W)\\)\nWhat is the probability that a person does not attend religious services weekly?\nI’m asking for \\(\\Pr(W^C)\\)\nWhat is the probability that a person is conservative, given that they attend religious services weekly?\nI’m asking for \\(\\Pr(C \\mid W)\\)\n\\(\\Pr(W \\mid C)\\)\n\\(\\Pr(C \\mid W^\\complement)\\)\n\\(\\Pr(W \\text{ and } C)\\)\n\\(\\Pr(W^\\complement \\text{ and } C^\\complement)\\)\n\nFinally, if we assume that \\(W\\) and \\(C\\) are independent:\n\nWhat would be the joint probability of \\(W\\) and \\(C\\) ?\n\n\n\n\n\n\n\n\n\n\nHint: I introduced the superscript \\(\\complement\\) notation, which means “complement.”\nYou can think of \\(C\\) as “conservative” and \\(C^\\complement\\) as “not conservative.”\n\n\n\n\n\n7.1.2 Exercise\n\n\n\n\n\n\nChoose 2 new variables in the GSS.\n\nShow the contingency table for these new variables.\nModify the exercise 7.1.1 so that it applies to your new contingency table and answer all 8 questions.\n\n\n\n\n\n\n\n\n\n\nSuggestion: You should really make your life easier and either choose two binary variables or create them."
  },
  {
    "objectID": "week7.html#making-sense-of-dependence",
    "href": "week7.html#making-sense-of-dependence",
    "title": "7  Week 7",
    "section": "7.2 Making Sense of Dependence",
    "text": "7.2 Making Sense of Dependence\n\n7.2.1 Exercise\n\n\n\n\n\n\nTake another look at the weekly vs conservative contingency table.\nFocus on these two probabilities:\n\\[\n\\begin{align}\n\\Pr(W \\mid C) &&\\text{and} && \\Pr(W \\mid C^\\complement)\n\\end{align}\n\\]\nCompare these probabilities using the following summary statistics:\n\nDifference in Probabilities\nRelative Risk Ratio\nOdds Ratio\nLog Odds Ratio\n\n\n\n\n\n\n7.2.2 Exercise\n\n\n\n\n\n\nLast week we used the infer package to draw a “simulation-based bootstrap distribution” for the difference in proportions between \\(\\Pr(W \\mid C)\\) and \\(\\Pr(W \\mid C^\\complement)\\).\nNow we mix things up a little bit.\n\nDraw a sampling distribution for the “relative risk ratio” (Hint: infer calls this “ratio of props”)\nDraw a sampling distribution for the “odds ratio”\n\n\n\n\n\n\n7.2.3 Exercise\n\n\n\n\n\n\nRepeat exercise 7.2.1 using the contingency table you created in exercise 7.1.2\n\n\n\n\n\n7.2.4 Exercise\n\n\n\n\n\n\nRepeat exercise 7.2.2 using the contingency table you created in exercise 7.1.2\n\n\n\n\n\n\n\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton University Press."
  },
  {
    "objectID": "solutions-01.html#data-structures",
    "href": "solutions-01.html#data-structures",
    "title": "8  Solutions 1",
    "section": "8.1 Data Structures",
    "text": "8.1 Data Structures\n\n8.1.1 Exercise\nmtcars is a data frame. You can verify this by looking at its class:\n\n\nCode\nclass(mtcars)\n\n\n[1] \"data.frame\"\n\n\nA “data frame” is basically a list of atomic vectors each of which have the same length()—i.e., the number of rows in the dataset. This is why typeof(mtcars) produces “list.”\n\n\nCode\ntypeof(mtcars)\n\n\n[1] \"list\"\n\n\n\n\nCode\nnrow(mtcars) ## number of observations\n\n\n[1] 32\n\n\nCode\nncol(mtcars) ## number of columns\n\n\n[1] 11\n\n\nCode\nlength(mtcars) ## not number of observations, but number of columns\n\n\n[1] 11\n\n\nCode\ndim(mtcars) ## number of rows and columns\n\n\n[1] 32 11\n\n\nCode\nrownames(mtcars)\n\n\n [1] \"Mazda RX4\"           \"Mazda RX4 Wag\"       \"Datsun 710\"         \n [4] \"Hornet 4 Drive\"      \"Hornet Sportabout\"   \"Valiant\"            \n [7] \"Duster 360\"          \"Merc 240D\"           \"Merc 230\"           \n[10] \"Merc 280\"            \"Merc 280C\"           \"Merc 450SE\"         \n[13] \"Merc 450SL\"          \"Merc 450SLC\"         \"Cadillac Fleetwood\" \n[16] \"Lincoln Continental\" \"Chrysler Imperial\"   \"Fiat 128\"           \n[19] \"Honda Civic\"         \"Toyota Corolla\"      \"Toyota Corona\"      \n[22] \"Dodge Challenger\"    \"AMC Javelin\"         \"Camaro Z28\"         \n[25] \"Pontiac Firebird\"    \"Fiat X1-9\"           \"Porsche 914-2\"      \n[28] \"Lotus Europa\"        \"Ford Pantera L\"      \"Ferrari Dino\"       \n[31] \"Maserati Bora\"       \"Volvo 142E\"         \n\n\nCode\ncolnames(mtcars)\n\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\n\n\n8.1.2 Exercise\n\n\nCode\nT &lt;- 123\nT\n\n\n[1] 123\n\n\nCode\nTRUE &lt;- 123\n\n\nError in TRUE &lt;- 123: invalid (do_set) left-hand side to assignment\n\n\nYou can assign any value to T, but R won’t let you modify TRUE because it is a “reserved keyword.” Other reserved keywords include: function, for, FALSE, Inf, NULL, NA, among others.\n\n\n8.1.3 Exercise\nTest your knowledge of the vector coercion rules by predicting the output of the following uses of c():\n\n\nCode\nc(1, FALSE) ## numeric (or \"double\")\n\n\n[1] 1 0\n\n\nCode\nc(\"a\", 1) ## character\n\n\n[1] \"a\" \"1\"\n\n\nCode\nc(TRUE, 1L) ## numeric (or \"integer\")\n\n\n[1] 1 1\n\n\nYup.\n\n\n8.1.4 Exercise\nas.integer() coerces TRUE to 1 and FALSE to 0.\n\n\nCode\nas.integer(c(TRUE, FALSE))\n\n\n[1] 1 0\n\n\nThis is the most common (and useful) form of coercion you’ll see.\n\n\n8.1.5 Exercise\nImplicit coercion of logicals to numericals.\n\n\nCode\nx &lt;- sample(c(TRUE, FALSE), size = 75, replace = TRUE)\nsum(x)  ## number of TRUE values\n\n\n[1] 39\n\n\nCode\nmean(x) ## proportion of TRUE values\n\n\n[1] 0.52\n\n\nCode\nsum(x) / length(x) ## verifying the mean function\n\n\n[1] 0.52\n\n\n\n\n8.1.6 Exercise\nThe difference between mtcars[\"mpg\"] and mtcars[[\"mpg\"]] is that the first selects a column in a data frame, while the second extracts the column. mtcars[[\"mpg\"]] is equivalent to mtcars$mpg.\n\n\n8.1.7 Exercise\nletters is a built-in object in R that contains the 26 letters of English alphabet.\nUsing the [ operator, do the following:\nExtract the 17th value of letters\n\n\nCode\nletters[17]\n\n\n[1] \"q\"\n\n\nCreate a sequence of even numbers from 2 to 26 and use that to subset letters\n\n\nCode\nletters[seq(2, 26, by = 2)]\n\n\n [1] \"b\" \"d\" \"f\" \"h\" \"j\" \"l\" \"n\" \"p\" \"r\" \"t\" \"v\" \"x\" \"z\"\n\n\nUse 8:12 to subset letters.\n\n\nCode\nletters[8:12]\n\n\n[1] \"h\" \"i\" \"j\" \"k\" \"l\"\n\n\n\n\n8.1.8 Exercise\n\n\nCode\nletters[18] &lt;- NA\nletters\n\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" NA  \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n\n\n\n8.1.9 Exercise\nSubset mtcars so that we only see the observations for which cyl == 4.\n\n\nCode\nmtcars[mtcars$cyl == 4, ]\n\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nSubset mtcars so that we only see the observations for which mpg is greater than 23.\n\n\nCode\nmtcars[mtcars$mpg &gt; 23, ]\n\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2"
  },
  {
    "objectID": "solutions-01.html#search",
    "href": "solutions-01.html#search",
    "title": "8  Solutions 1",
    "section": "8.2 Search",
    "text": "8.2 Search\n\n8.2.1 Exercise\n\nUsing what I told you earlier about the search() function, explain why you get two different errors. What is going on? What is R doing when you type table(year)? (You might want to type search() into the console again). In what package does R find the year object?\n\nFirst, R searched for an object called year and didn’t find anyting.\nSecond, R searched for year and found a function with the same name in the lubridate package."
  },
  {
    "objectID": "solutions-01.html#dplyr",
    "href": "solutions-01.html#dplyr",
    "title": "8  Solutions 1",
    "section": "8.3 dplyr",
    "text": "8.3 dplyr\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(palmerpenguins)\n\n\n\n8.3.1 Exercise\n\n\nCode\ni &lt;- seq(2, nrow(penguins), by = 2)\n\npenguins |&gt; \n  slice(i)\n\n\n# A tibble: 172 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.5          17.4               186        3800\n 2 Adelie  Torgersen           NA            NA                  NA          NA\n 3 Adelie  Torgersen           39.3          20.6               190        3650\n 4 Adelie  Torgersen           39.2          19.6               195        4675\n 5 Adelie  Torgersen           42            20.2               190        4250\n 6 Adelie  Torgersen           37.8          17.3               180        3700\n 7 Adelie  Torgersen           38.6          21.2               191        3800\n 8 Adelie  Torgersen           36.6          17.8               185        3700\n 9 Adelie  Torgersen           42.5          20.7               197        4500\n10 Adelie  Torgersen           46            21.5               194        4200\n# ℹ 162 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nCode\npenguins |&gt; \n  slice(seq(3, nrow(penguins), by = 3))\n\n\n# A tibble: 114 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           40.3          18                 195        3250\n 2 Adelie  Torgersen           39.3          20.6               190        3650\n 3 Adelie  Torgersen           34.1          18.1               193        3475\n 4 Adelie  Torgersen           37.8          17.3               180        3700\n 5 Adelie  Torgersen           34.6          21.1               198        4400\n 6 Adelie  Torgersen           42.5          20.7               197        4500\n 7 Adelie  Biscoe              37.8          18.3               174        3400\n 8 Adelie  Biscoe              38.2          18.1               185        3950\n 9 Adelie  Biscoe              40.6          18.6               183        3550\n10 Adelie  Biscoe              40.5          18.9               180        3950\n# ℹ 104 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n8.3.2 Exercise\n\n\nCode\npenguins |&gt; \n  filter(species == \"Gentoo\", island == \"Biscoe\", body_mass_g &gt;= 5000, body_mass_g &lt;= 5500)\n\n\n# A tibble: 39 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           47.6          14.5               215        5400\n 2 Gentoo  Biscoe           46.7          15.3               219        5200\n 3 Gentoo  Biscoe           46.8          15.4               215        5150\n 4 Gentoo  Biscoe           48.7          15.1               222        5350\n 5 Gentoo  Biscoe           45.1          14.5               215        5000\n 6 Gentoo  Biscoe           46.3          15.8               215        5050\n 7 Gentoo  Biscoe           42.9          13.1               215        5000\n 8 Gentoo  Biscoe           46.1          15.1               215        5100\n 9 Gentoo  Biscoe           47.3          15.3               222        5250\n10 Gentoo  Biscoe           45.1          14.5               207        5050\n# ℹ 29 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "solutions-02.html#rows",
    "href": "solutions-02.html#rows",
    "title": "9  Solutions 2",
    "section": "9.1 Rows",
    "text": "9.1 Rows\n\n9.1.1 4.2.5.1\n\nIn a single pipeline for each condition, find all flights that meet the condition:\n\nHad an arrival delay of two or more hours\n\n\nCode\nflights |&gt; \n  filter(arr_delay &gt;= 120)\n\n\n# A tibble: 10,200 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      811            630       101     1047            830\n 2  2013     1     1      848           1835       853     1001           1950\n 3  2013     1     1      957            733       144     1056            853\n 4  2013     1     1     1114            900       134     1447           1222\n 5  2013     1     1     1505           1310       115     1638           1431\n 6  2013     1     1     1525           1340       105     1831           1626\n 7  2013     1     1     1549           1445        64     1912           1656\n 8  2013     1     1     1558           1359       119     1718           1515\n 9  2013     1     1     1732           1630        62     2028           1825\n10  2013     1     1     1803           1620       103     2008           1750\n# ℹ 10,190 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nFlew to Houston (IAH or HOU)\n\n\nCode\nflights |&gt; \n  filter(dest == \"IAH\" | dest == \"HOU\")\n\n\n# A tibble: 9,313 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      623            627        -4      933            932\n 4  2013     1     1      728            732        -4     1041           1038\n 5  2013     1     1      739            739         0     1104           1038\n 6  2013     1     1      908            908         0     1228           1219\n 7  2013     1     1     1028           1026         2     1350           1339\n 8  2013     1     1     1044           1045        -1     1352           1351\n 9  2013     1     1     1114            900       134     1447           1222\n10  2013     1     1     1205           1200         5     1503           1505\n# ℹ 9,303 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nWere operated by United, American, or Delta\n\n\nCode\nflights |&gt; \n  ## hint: check the nycflights13::airlines dataset\n  filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\n\n\n# A tibble: 139,504 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      554            600        -6      812            837\n 5  2013     1     1      554            558        -4      740            728\n 6  2013     1     1      558            600        -2      753            745\n 7  2013     1     1      558            600        -2      924            917\n 8  2013     1     1      558            600        -2      923            937\n 9  2013     1     1      559            600        -1      941            910\n10  2013     1     1      559            600        -1      854            902\n# ℹ 139,494 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nDeparted in summer (July, August, and September)\n\n\nCode\nflights |&gt; \n  filter(month %in% 7:9)\n\n\n# A tibble: 86,326 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     7     1        1           2029       212      236           2359\n 2  2013     7     1        2           2359         3      344            344\n 3  2013     7     1       29           2245       104      151              1\n 4  2013     7     1       43           2130       193      322             14\n 5  2013     7     1       44           2150       174      300            100\n 6  2013     7     1       46           2051       235      304           2358\n 7  2013     7     1       48           2001       287      308           2305\n 8  2013     7     1       58           2155       183      335             43\n 9  2013     7     1      100           2146       194      327             30\n10  2013     7     1      100           2245       135      337            135\n# ℹ 86,316 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nArrived more than two hours late, but didn’t leave late\n\n\nCode\nflights |&gt; \n  ## I will also accept dep_delay &lt;= 0\n  filter(arr_delay &gt; 120 & dep_delay &lt;= 5)\n\n\n# A tibble: 36 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1    27     1419           1420        -1     1754           1550\n 2  2013    10     7     1350           1350         0     1736           1526\n 3  2013    10     7     1357           1359        -2     1858           1654\n 4  2013    10    16      657            700        -3     1258           1056\n 5  2013    11     1      658            700        -2     1329           1015\n 6  2013     3     8     1246           1245         1     1552           1350\n 7  2013     3    18     1844           1847        -3       39           2219\n 8  2013     4    17     1635           1640        -5     2049           1845\n 9  2013     4    18      558            600        -2     1149            850\n10  2013     4    18      655            700        -5     1213            950\n# ℹ 26 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\n\nCode\nflights |&gt; \n  # this also works:\n  # filter(dep_delay &gt;= 60 & dep_delay - arr_delay &gt; 30)\n  filter(dep_delay &gt;= 60, dep_delay - arr_delay &gt; 30)\n\n\n# A tibble: 1,844 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1     2205           1720       285       46           2040\n 2  2013     1     1     2326           2130       116      131             18\n 3  2013     1     3     1503           1221       162     1803           1555\n 4  2013     1     3     1839           1700        99     2056           1950\n 5  2013     1     3     1850           1745        65     2148           2120\n 6  2013     1     3     1941           1759       102     2246           2139\n 7  2013     1     3     1950           1845        65     2228           2227\n 8  2013     1     3     2015           1915        60     2135           2111\n 9  2013     1     3     2257           2000       177       45           2224\n10  2013     1     4     1917           1700       137     2135           1950\n# ℹ 1,834 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n9.1.2 4.2.5.2\n\nSort flights to find the flights with longest departure delays. Find the flights that left earliest in the morning.\n\n\n\nCode\nflights |&gt; \n  arrange(desc(dep_delay))\n\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nCode\nflights |&gt; \n  arrange(dep_time)\n\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1    13        1           2249        72      108           2357\n 2  2013     1    31        1           2100       181      124           2225\n 3  2013    11    13        1           2359         2      442            440\n 4  2013    12    16        1           2359         2      447            437\n 5  2013    12    20        1           2359         2      430            440\n 6  2013    12    26        1           2359         2      437            440\n 7  2013    12    30        1           2359         2      441            437\n 8  2013     2    11        1           2100       181      111           2225\n 9  2013     2    24        1           2245        76      121           2354\n10  2013     3     8        1           2355         6      431            440\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n9.1.3 4.2.5.3\n\nSort flights to find the fastest flights. (Hint: Try including a math calculation inside of your function.)\n\nThe fastest flights will travel the longest distances in the least amount of time. Thus, I will have to calculate km/h. (Miles per hour is fine too I guess).\n\n\nCode\nto_kmh &lt;- function(distance, minutes) {\n  (distance * 1.60934) / (minutes / 60)\n}\n\nflights |&gt; \n  mutate(speed = to_kmh(distance, air_time)) |&gt;\n  relocate(speed) |&gt; \n  arrange(desc(speed))\n\n\n# A tibble: 336,776 × 20\n   speed  year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1 1132.  2013     5    25     1709           1700         9     1923\n 2 1047.  2013     7     2     1558           1513        45     1745\n 3 1043.  2013     5    13     2040           2025        15     2225\n 4 1032.  2013     3    23     1914           1910         4     2045\n 5  952.  2013     1    12     1559           1600        -1     1849\n 6  908.  2013    11    17      650            655        -5     1059\n 7  897.  2013     2    21     2355           2358        -3      412\n 8  896.  2013    11    17      759            800        -1     1212\n 9  892.  2013    11    16     2003           1925        38       17\n10  892.  2013    11    16     2349           2359       -10      402\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n9.1.4 4.2.5.4\n\nWas there a flight on every day of 2013?\n\n\n\nCode\nflights |&gt; \n  distinct(month, day) |&gt; \n  summarize(num_days = n())\n\n\n# A tibble: 1 × 1\n  num_days\n     &lt;int&gt;\n1      365\n\n\nYes!\n\n\n9.1.5 4.2.5.5\n\nWhich flights traveled the farthest distance? Which traveled the least distance?\n\n\n\nCode\nflights |&gt; \n  arrange(desc(distance))\n\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      857            900        -3     1516           1530\n 2  2013     1     2      909            900         9     1525           1530\n 3  2013     1     3      914            900        14     1504           1530\n 4  2013     1     4      900            900         0     1516           1530\n 5  2013     1     5      858            900        -2     1519           1530\n 6  2013     1     6     1019            900        79     1558           1530\n 7  2013     1     7     1042            900       102     1620           1530\n 8  2013     1     8      901            900         1     1504           1530\n 9  2013     1     9      641            900      1301     1242           1530\n10  2013     1    10      859            900        -1     1449           1530\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nCode\nflights |&gt; \n  arrange(distance)\n\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     7    27       NA            106        NA       NA            245\n 2  2013     1     3     2127           2129        -2     2222           2224\n 3  2013     1     4     1240           1200        40     1333           1306\n 4  2013     1     4     1829           1615       134     1937           1721\n 5  2013     1     4     2128           2129        -1     2218           2224\n 6  2013     1     5     1155           1200        -5     1241           1306\n 7  2013     1     6     2125           2129        -4     2224           2224\n 8  2013     1     7     2124           2129        -5     2212           2224\n 9  2013     1     8     2127           2130        -3     2304           2225\n10  2013     1     9     2126           2129        -3     2217           2224\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n9.1.6 4.2.5.6\n\nDoes it matter what order you used filter() and arrange() if you’re using both? Why/why not? Think about the results and how much work the functions would have to do.\n\nNo, it doesn’t matter. Filter does logical subsetting, and it doesn’t matter if we re-arrange the data before or after subsetting."
  },
  {
    "objectID": "solutions-02.html#columns",
    "href": "solutions-02.html#columns",
    "title": "9  Solutions 2",
    "section": "9.2 Columns",
    "text": "9.2 Columns\n\n9.2.1 4.3.5.1\n\nCompare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?\n\nsched_dep_time = dep_time - dep_delay\n\n\n9.2.2 4.3.5.2\n\nBrainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.\n\n\n\nCode\nflights |&gt; \n  select(dep_time, dep_delay, arr_time, arr_delay)\n\n\n# A tibble: 336,776 × 4\n   dep_time dep_delay arr_time arr_delay\n      &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1      517         2      830        11\n 2      533         4      850        20\n 3      542         2      923        33\n 4      544        -1     1004       -18\n 5      554        -6      812       -25\n 6      554        -4      740        12\n 7      555        -5      913        19\n 8      557        -3      709       -14\n 9      557        -3      838        -8\n10      558        -2      753         8\n# ℹ 336,766 more rows\n\n\nCode\nflights |&gt; \n  select(starts_with(\"dep_\"), starts_with(\"arr_\"))\n\n\n# A tibble: 336,776 × 4\n   dep_time dep_delay arr_time arr_delay\n      &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1      517         2      830        11\n 2      533         4      850        20\n 3      542         2      923        33\n 4      544        -1     1004       -18\n 5      554        -6      812       -25\n 6      554        -4      740        12\n 7      555        -5      913        19\n 8      557        -3      709       -14\n 9      557        -3      838        -8\n10      558        -2      753         8\n# ℹ 336,766 more rows\n\n\nCode\nflights |&gt; \n  select(dep_time:arr_delay) |&gt; \n  ## or ! instead of -\n  select(-c(2, 5))\n\n\n# A tibble: 336,776 × 4\n   dep_time dep_delay arr_time arr_delay\n      &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1      517         2      830        11\n 2      533         4      850        20\n 3      542         2      923        33\n 4      544        -1     1004       -18\n 5      554        -6      812       -25\n 6      554        -4      740        12\n 7      555        -5      913        19\n 8      557        -3      709       -14\n 9      557        -3      838        -8\n10      558        -2      753         8\n# ℹ 336,766 more rows\n\n\nCode\nflights |&gt; \n  select(matches(\"(^arr|^dep)_(time$|delay$)\"))\n\n\n# A tibble: 336,776 × 4\n   dep_time dep_delay arr_time arr_delay\n      &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1      517         2      830        11\n 2      533         4      850        20\n 3      542         2      923        33\n 4      544        -1     1004       -18\n 5      554        -6      812       -25\n 6      554        -4      740        12\n 7      555        -5      913        19\n 8      557        -3      709       -14\n 9      557        -3      838        -8\n10      558        -2      753         8\n# ℹ 336,766 more rows\n\n\n\n\n9.2.3 4.3.5.3\n\nWhat happens if you specify the name of the same variable multiple times in a select() call?\n\nYou only get the variable once!\n\n\nCode\nflights |&gt; \n  select(day, day, day)\n\n\n# A tibble: 336,776 × 1\n     day\n   &lt;int&gt;\n 1     1\n 2     1\n 3     1\n 4     1\n 5     1\n 6     1\n 7     1\n 8     1\n 9     1\n10     1\n# ℹ 336,766 more rows\n\n\nNote that this is different from “base” subsetting with character vectors.\n\n\nCode\nflights[, c(\"day\", \"day\", \"day\")]\n\n\n# A tibble: 336,776 × 3\n     day   day   day\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1     1     1\n 2     1     1     1\n 3     1     1     1\n 4     1     1     1\n 5     1     1     1\n 6     1     1     1\n 7     1     1     1\n 8     1     1     1\n 9     1     1     1\n10     1     1     1\n# ℹ 336,766 more rows\n\n\n\n\n9.2.4 4.3.5.4\n\nWhat does the any_of() function do? Why might it be helpful in conjunction with this vector?\n\n\nCode\nvariables &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\n\n\nIt allows you to specify the name of variables for subsetting outside the dplyr pipeline.\n\n\nCode\nvariables &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\nflights |&gt; \n  select(any_of(variables))\n\n\n# A tibble: 336,776 × 5\n    year month   day dep_delay arr_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1  2013     1     1         2        11\n 2  2013     1     1         4        20\n 3  2013     1     1         2        33\n 4  2013     1     1        -1       -18\n 5  2013     1     1        -6       -25\n 6  2013     1     1        -4        12\n 7  2013     1     1        -5        19\n 8  2013     1     1        -3       -14\n 9  2013     1     1        -3        -8\n10  2013     1     1        -2         8\n# ℹ 336,766 more rows\n\n\n\n\n9.2.5 4.3.5.5\n\nDoes the result of running the following code surprise you? How do the select helpers deal with upper and lower case by default? How can you change that default?\n\n\n\nCode\nflights |&gt; \n  select(contains(\"TIME\"))\n\n\n# A tibble: 336,776 × 6\n   dep_time sched_dep_time arr_time sched_arr_time air_time time_hour          \n      &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;dbl&gt; &lt;dttm&gt;             \n 1      517            515      830            819      227 2013-01-01 05:00:00\n 2      533            529      850            830      227 2013-01-01 05:00:00\n 3      542            540      923            850      160 2013-01-01 05:00:00\n 4      544            545     1004           1022      183 2013-01-01 05:00:00\n 5      554            600      812            837      116 2013-01-01 06:00:00\n 6      554            558      740            728      150 2013-01-01 05:00:00\n 7      555            600      913            854      158 2013-01-01 06:00:00\n 8      557            600      709            723       53 2013-01-01 06:00:00\n 9      557            600      838            846      140 2013-01-01 06:00:00\n10      558            600      753            745      138 2013-01-01 06:00:00\n# ℹ 336,766 more rows\n\n\nYes, it’s surprising to me. The selection helpers have the argument ignore.case = TRUE as a default.\n\n\nCode\nflights |&gt; \n  select(contains(\"TIME\", ignore.case = FALSE))\n\n\n# A tibble: 336,776 × 0\n\n\n\n\n9.2.6 4.3.5.6\n\nRename air_time to air_time_min to indicate units of measurement and move it to the beginning of the data frame.\n\n\n\nCode\nflights |&gt; \n  rename(air_time_min = air_time) |&gt; \n  relocate(air_time_min)\n\n\n# A tibble: 336,776 × 19\n   air_time_min  year month   day dep_time sched_dep_time dep_delay arr_time\n          &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1          227  2013     1     1      517            515         2      830\n 2          227  2013     1     1      533            529         4      850\n 3          160  2013     1     1      542            540         2      923\n 4          183  2013     1     1      544            545        -1     1004\n 5          116  2013     1     1      554            600        -6      812\n 6          150  2013     1     1      554            558        -4      740\n 7          158  2013     1     1      555            600        -5      913\n 8           53  2013     1     1      557            600        -3      709\n 9          140  2013     1     1      557            600        -3      838\n10          138  2013     1     1      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n9.2.7 4.3.5.7\n\nWhy doesn’t the following work, and what does the error mean?\n\n\n\nCode\nflights |&gt; \n  select(tailnum) |&gt; \n  arrange(arr_delay)\n\n\nError in `arrange()`:\nℹ In argument: `..1 = arr_delay`.\nCaused by error:\n! object 'arr_delay' not found\n\n\nLine #2 drops every variable in the data frame except tailnum. Thus, there’s no arr_delay object for arrange() to work with."
  },
  {
    "objectID": "solutions-02.html#groups",
    "href": "solutions-02.html#groups",
    "title": "9  Solutions 2",
    "section": "9.3 Groups",
    "text": "9.3 Groups\n\n9.3.1 4.5.7.1\n\nWhich carrier has the worst average delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights |&gt; group_by(carrier, dest) |&gt; summarize(n()))\n\n\n\nCode\nflights |&gt; \n  group_by(carrier) |&gt; \n  summarize(avg_dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt; \n  arrange(desc(avg_dep_delay))\n\n\n# A tibble: 16 × 2\n   carrier avg_dep_delay\n   &lt;chr&gt;           &lt;dbl&gt;\n 1 F9              20.2 \n 2 EV              20.0 \n 3 YV              19.0 \n 4 FL              18.7 \n 5 WN              17.7 \n 6 9E              16.7 \n 7 B6              13.0 \n 8 VX              12.9 \n 9 OO              12.6 \n10 UA              12.1 \n11 MQ              10.6 \n12 DL               9.26\n13 AA               8.59\n14 AS               5.80\n15 HA               4.90\n16 US               3.78\n\n\nCode\nflights |&gt; \n  filter(carrier == \"F9\") |&gt; \n  count(carrier, dest, origin) \n\n\n# A tibble: 1 × 4\n  carrier dest  origin     n\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;\n1 F9      DEN   LGA      685\n\n\nCode\nflights |&gt; \n  count(carrier, dest) |&gt; \n  filter(dest == \"DEN\")\n\n\n# A tibble: 5 × 3\n  carrier dest      n\n  &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;\n1 B6      DEN     338\n2 DL      DEN    1043\n3 F9      DEN     685\n4 UA      DEN    3796\n5 WN      DEN    1404\n\n\nCode\nflights |&gt; \n  group_by(carrier, dest, origin) |&gt; \n  summarize(n = n(), avg_dd = mean(dep_delay, na.rm = TRUE)) |&gt; \n  filter(dest == \"DEN\" | carrier == \"F9\") |&gt; \n  arrange(avg_dd)\n\n\n`summarise()` has grouped output by 'carrier', 'dest'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 8 × 5\n# Groups:   carrier, dest [5]\n  carrier dest  origin     n avg_dd\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt;\n1 DL      DEN   LGA      678   9.82\n2 UA      DEN   LGA     1626  10.6 \n3 UA      DEN   EWR     2170  14.1 \n4 WN      DEN   LGA      715  15.4 \n5 DL      DEN   JFK      365  16.6 \n6 F9      DEN   LGA      685  20.2 \n7 B6      DEN   JFK      338  23.9 \n8 WN      DEN   EWR      689  24.4 \n\n\nFrontier Airlines (F9) is headquartered in Denver and this dataset only records flights from LGA to DEN. This means there’s no variance in F9 accross airports. We might be able to disentangle the effect of airport and carrier, but we will have to make assumptions.\n\n\n9.3.2 4.5.7.2\n\nFind the flights that are most delayed upon departure from each destination.\n\n\n\nCode\nflights |&gt; \n  group_by(dest) |&gt; \n  slice_max(dep_delay) |&gt; \n  relocate(dest, carrier, dep_delay) |&gt; \n  arrange(desc(dep_delay))\n\n\n# A tibble: 105 × 19\n# Groups:   dest [105]\n   dest  carrier dep_delay  year month   day dep_time sched_dep_time arr_time\n   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;int&gt;\n 1 HNL   HA           1301  2013     1     9      641            900     1242\n 2 CMH   MQ           1137  2013     6    15     1432           1935     1607\n 3 ORD   MQ           1126  2013     1    10     1121           1635     1239\n 4 SFO   AA           1014  2013     9    20     1139           1845     1457\n 5 CVG   MQ           1005  2013     7    22      845           1600     1044\n 6 TPA   DL            960  2013     4    10     1100           1900     1342\n 7 MSP   DL            911  2013     3    17     2321            810      135\n 8 PDX   DL            899  2013     6    27      959           1900     1236\n 9 ATL   DL            898  2013     7    22     2257            759      121\n10 MIA   AA            896  2013    12     5      756           1700     1058\n# ℹ 95 more rows\n# ℹ 10 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,\n#   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n9.3.3 4.5.7.3\n\nHow do delays vary over the course of the day. Illustrate your answer with a plot.\n\n\n\nCode\nflights |&gt; \n  group_by(hour) |&gt; \n  filter(!is.na(dep_delay)) |&gt; \n  summarize(\n    avg = mean(dep_delay),\n    se = sd(dep_delay) / sqrt(n())\n  ) |&gt; \n  ggplot(aes(hour, avg)) + \n  geom_pointrange(aes(ymin = avg - 2*se, ymax = avg + 2*se), size = 1/10) +\n  theme_light()\n\n\n\n\n\n\n\n9.3.4 4.5.7.4\n\nWhat happens if you supply a negative n to slice_min() and friends?\n\nThe documentation for this behavior is not great and I can’t think of a single example in which I’d use those functions that way instead of using arrange().\n\n\nCode\nA &lt;- flights |&gt; \n  slice_min(dep_delay, n = -1)\n\nB &lt;- flights |&gt; \n  arrange(dep_delay)\n\nidentical(A, B)\n\n\n[1] TRUE\n\n\nCode\nC &lt;- flights |&gt; \n  slice_max(dep_delay, n = -1) \n\nD &lt;- flights |&gt; \n  arrange(desc(dep_delay))\n\nidentical(C, D)\n\n\n[1] TRUE\n\n\n\n\n9.3.5 4.5.7.5\n\nExplain what count() does in terms of the dplyr verbs you just learned. What does the sort argument to count() do?\n\nCount is a short-hand for group_by + summarize. The sort argument accomplishes the same as arrange.\n\n\nCode\nflights |&gt; \n  group_by(origin) |&gt; \n  summarize(n = n()) |&gt; \n  arrange(desc(n))\n\n\n# A tibble: 3 × 2\n  origin      n\n  &lt;chr&gt;   &lt;int&gt;\n1 EWR    120835\n2 JFK    111279\n3 LGA    104662\n\n\nCode\nflights |&gt; \n  count(origin, sort = TRUE)\n\n\n# A tibble: 3 × 2\n  origin      n\n  &lt;chr&gt;   &lt;int&gt;\n1 EWR    120835\n2 JFK    111279\n3 LGA    104662\n\n\n\n\n9.3.6 4.5.7.6\n\nSuppose we have the following tiny data frame:\n\n\nCode\ndf &lt;- tibble(\n  x = 1:5,\n  y = c(\"a\", \"b\", \"a\", \"a\", \"b\"),\n  z = c(\"K\", \"K\", \"L\", \"L\", \"K\")\n)\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what group_by() does.\n\n\nCode\ndf |&gt;\n  group_by(y)\n\n\n\ntwo groups?\n\n\nCode\ndf |&gt; group_by(y)\n\n\n# A tibble: 5 × 3\n# Groups:   y [2]\n      x y     z    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     K    \n2     2 b     K    \n3     3 a     L    \n4     4 a     L    \n5     5 b     K    \n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what arrange() does. Also comment on how it’s different from the group_by() in part (a)?\n\n\nCode\ndf |&gt;\n  arrange(y)\n\n\n\nI assume that y will be a, a, a, b, b and that x will be 1, 3, 4, 2, 5\n\n\nCode\ndf |&gt;\n  arrange(y)\n\n\n# A tibble: 5 × 3\n      x y     z    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     K    \n2     3 a     L    \n3     4 a     L    \n4     2 b     K    \n5     5 b     K    \n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does.\n\n\nCode\ndf |&gt;\n  group_by(y) |&gt;\n  summarize(mean_x = mean(x))\n\n\n\nI assume the new data frame will have two columns (x, y) and two rows (y = 2.67, y = 3.5)\n\n\nCode\ndf |&gt;\n  group_by(y) |&gt;\n  summarize(mean_x = mean(x))\n\n\n# A tibble: 2 × 2\n  y     mean_x\n  &lt;chr&gt;  &lt;dbl&gt;\n1 a       2.67\n2 b       3.5 \n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does. Then, comment on what the message says.\n\n\nCode\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x))\n\n\n\nI assume there will be 3 columns (x, y, z) and three rows (x = c(1, 3.5, 3.5)).\n\n\nCode\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x))\n\n\n`summarise()` has grouped output by 'y'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 3 × 3\n# Groups:   y [2]\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does. How is the output different from the one in part (d).\n\n\nCode\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x), .groups = \"drop\")\n\n\n# A tibble: 3 × 3\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\n\nI assume the same answer, except the output won’t be “grouped.”\n\n\nCode\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x), .groups = \"drop\")\n\n\n# A tibble: 3 × 3\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\n\nWrite down what you think the outputs will look like, then check if you were correct, and describe what each pipeline does. How are the outputs of the two pipelines different?\n\n\nCode\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x))\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  mutate(mean_x = mean(x))\n\n\n\nThe results will be similar, except that the first pipeline will have 3 rows and the second one will have 5 rows.\n\n\nCode\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x))\n\n\n`summarise()` has grouped output by 'y'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 3 × 3\n# Groups:   y [2]\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\nCode\ndf |&gt;\n  group_by(y, z) |&gt;\n  mutate(mean_x = mean(x))\n\n\n# A tibble: 5 × 4\n# Groups:   y, z [3]\n      x y     z     mean_x\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     1 a     K        1  \n2     2 b     K        3.5\n3     3 a     L        3.5\n4     4 a     L        3.5\n5     5 b     K        3.5"
  },
  {
    "objectID": "solutions-03.html#ggplot",
    "href": "solutions-03.html#ggplot",
    "title": "10  Solutions 3",
    "section": "10.1 ggplot",
    "text": "10.1 ggplot\n\n10.1.1 Exercise\nModify the code below to make the points larger squares and slightly transparent. See ?geom_point for more information on the point layer.\n\n\nCode\nggplot(faithful) + \n  geom_point(aes(x = eruptions, y = waiting), shape = 15, size = 10, alpha = 1/5)\n\n\n\n\n\n\n\n10.1.2 Exercise\nColor the two visible clusters in the histogram with different colors.\n\n\nCode\nfaithful |&gt; \n  ggplot() + \n  geom_histogram(aes(x = eruptions, fill = eruptions &gt; 3.2))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n10.1.3 Exercise\nAdd a line that separates the two point distributions. See ?geom_abline for how to draw straight lines from a slope and intercept.\n\n\nCode\nggplot(faithful) + \n  geom_point(aes(x = eruptions, y = waiting)) + \n  geom_abline(intercept = 125, slope = -20)\n\n\n\n\n\n\n\n10.1.4 Exercise\nUse RColorBrewer::display.brewer.all() to see all the different palettes from Color Brewer and pick your favorite. Modify the code below to use it.\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, fill = class), shape = 21) + \n  scale_fill_brewer(type = 'qual', palette = \"Spectral\")\n\n\n\n\n\n\n\n10.1.5 Exercise\nModify the code below to create a bubble chart (scatterplot with size mapped to a continuous variable) showing cyl with size. Make sure that only the present amount of cylinders (4, 5, 6, and 8) are present in the legend.\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class, size = cyl)) + \n  scale_color_brewer(type = 'qual') + \n  scale_size_area(breaks = c(4, 5, 6, 8)) ## ensures 0 is mapped to 0 size\n\n\n\n\n\n\n\n10.1.6 Exercise\nModify the code below so that color is no longer mapped to the discrete class variable, but to the continuous cty variable. What happens to the guide?\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = cty, size = cty)) + \n  scale_size_area() \n\n\n\n\n\nThe type of guide can be controlled with the guide argument in the scale, or with the guides() function. Continuous colors have a gradient color bar by default, but setting it to legend will turn it back to the standard look. What happens when multiple aesthetics are mapped to the same variable and uses the guide type?\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = cty, size = cty)) + \n  scale_size_area() + \n  guides(color = \"legend\")\n\n\n\n\n\n\n\n10.1.7 Exercise\nOne of the great things about facets is that they share the axes between the different panels. Sometimes this is undesirable though, and the behavior can be changed with the scales argument. Experiment with the different possible settings in the plot below:\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_wrap(~ drv, scales = \"fixed\") ## default\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_wrap(~ drv, scales = \"free\")\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_wrap(~ drv, scales = \"free_x\")\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_wrap(~ drv, scales = \"free_y\")\n\n\n\n\n\n\n\n10.1.8 Exercise\nThemes can be overwhelming, especially as you often try to optimize for beauty while you learn. To remove the last part of the equation, the exercise is to take the plot given below and make it as hideous as possible using the theme function. Go absolutely crazy, but take note of the effect as you change different settings.\n\n\nCode\nmpg |&gt; \n  ggplot(aes(y = class, fill = drv)) + \n  geom_bar() + \n  facet_wrap(~year) + \n  labs(\n    title = \"Number of car models per class\",\n    caption = \"source: http://fueleconomy.gov\",\n    x = 'Number of cars',\n    y = NULL\n  ) +\n  theme(\n    text = element_text(\"Comic Sans MS\", color = \"orange\"),\n    axis.text = element_text(color = \"white\", size = 20),\n    panel.background = element_rect(\"yellow\"),\n    legend.background = element_rect(\n      fill = \"yellow\", linetype = \"dotted\", color = \"white\", linewidth = 2),\n    strip.text = element_text(face = 'bold', hjust = 0, angle = 180),\n    plot.background = element_rect(\"black\")\n  )"
  },
  {
    "objectID": "solutions-03.html#simulation",
    "href": "solutions-03.html#simulation",
    "title": "10  Solutions 3",
    "section": "10.2 Simulation",
    "text": "10.2 Simulation\n\n\nCode\nurl &lt;- \"https://raw.githubusercontent.com/acastroaraujo/socStats/main/simulation_function_week3.R\"\nsource(url)\n\n\nsims &lt;- simulation_votes(dem_prob_pop = 0.52, sample_size = 300, num_sims = 500)\n\nresults &lt;- sims |&gt; \n  group_by(id) |&gt; \n  summarize(dem_prop = mean(vote == \"Dem\"))\n\nglimpse(results)\n\n\nRows: 500\nColumns: 2\n$ id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ dem_prop &lt;dbl&gt; 0.5300000, 0.5533333, 0.4933333, 0.5500000, 0.5366667, 0.5166…\n\n\n\n10.2.1 Exercise\nIn the simulation above, what is the average dem_prop? What is the standard deviation of dem_prop? How does this change for different values of sample_size?\n\n\nCode\nmean(results$dem_prop)\n\n\n[1] 0.52048\n\n\nThis is almost the same as dem_prob_pop!\n\n\nCode\nsd(results$dem_prop)\n\n\n[1] 0.02904462\n\n\nNow I will increase the sample size by a factor of 2.\n\n\nCode\nsims &lt;- simulation_votes(dem_prob_pop = 0.52, sample_size = 600, num_sims = 500)\n\nresults &lt;- sims |&gt; \n  group_by(id) |&gt; \n  summarize(dem_prop = mean(vote == \"Dem\"))\n\nmean(results$dem_prop)\n\n\n[1] 0.5209533\n\n\nCode\nsd(results$dem_prop)\n\n\n[1] 0.01998279\n\n\nThe mean is still pretty much the same, but the standard deviation is now much smaller. (Decreasing the sample size will produce larger standard deviations.)\n\n\n10.2.2 Exercise\nCreate five different simulations with different values of sample_size (e.g., 50, 200, 500, 1000, 2000). Put them together into a single dataset and then visualize the results using boxplots. What is going on?\n\n\nCode\nss &lt;- c(50, 200, 500, 1000, 2000)\noutput_list &lt;- lapply(ss, \\(ss) simulation_votes(0.52, ss, num_sims = 500)) \n\nbind_rows(output_list) |&gt; \n  group_by(id, sample_size) |&gt; \n  summarize(dem_prop = mean(vote == \"Dem\")) |&gt; \n  ggplot(aes(sample_size, dem_prop)) + \n  geom_boxplot()\n\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\n\n\n\n\nCode\nbind_rows(output_list) |&gt; \n  mutate(sample_size = as.numeric(sample_size)) |&gt; \n  group_by(id, sample_size) |&gt; \n  summarize(dem_prop = mean(vote == \"Dem\")) |&gt; \n  ggplot(aes(sample_size, dem_prop, group = sample_size)) + \n  geom_boxplot()\n\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n10.2.3 Exercise\nCreate five different simulations with different values of dem_prob_pop (e.g., 0.49, 0.52, 0.55, 0.58). Put them together into a single dataset and then visualize the results using boxplots. What is going on?\n\n\nCode\np &lt;- c(.49, .52, .55, .58, .63)\noutput_list &lt;- lapply(p, \\(x) simulation_votes(x, sample_size = 300, num_sims = 500)) \n\n\nbind_rows(output_list) |&gt; \n  group_by(id, dem_prob_pop) |&gt; \n  summarize(dem_prop = mean(vote == \"Dem\")) |&gt; \n  ggplot(aes(dem_prob_pop, dem_prop)) + \n  geom_boxplot()\n\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n10.2.4 Extra\nMega simulation pretty graph.\n\n\nCode\ngrid &lt;- tidyr::expand_grid(p, ss)\n\nout &lt;- map2(grid$p, grid$ss, function(dem_prob, sample_size) {\n  simulation_votes(dem_prob, sample_size, num_sims = 500)\n})\n\ndf &lt;- bind_rows(out) |&gt; \n  group_by(dem_prob_pop, sample_size, id) |&gt; \n  summarize(prop = mean(vote == \"Dem\"))\n\ndf$dem_prob_pop &lt;- factor(df$dem_prob_pop, levels = p)\ndf$sample_size &lt;- factor(df$sample_size, levels = ss)\n\ndf |&gt; \n  mutate(strip_label = paste(\"\\\"True\\\" Value:\", dem_prob_pop)) |&gt; \n  ggplot(aes(prop, sample_size)) + \n  geom_boxplot() + \n  labs(y = \"Sample Size\", x = \"Observed Proportions\") + \n  facet_wrap(~ strip_label, ncol = 1) + \n  theme_light(base_family = \"Optima\") + \n  theme(strip.background = element_rect(fill = \"steelblue\"))"
  },
  {
    "objectID": "solutions-04.html#exercise",
    "href": "solutions-04.html#exercise",
    "title": "11  Solutions 4",
    "section": "11.1 Exercise",
    "text": "11.1 Exercise\n\n\nCode\nx &lt;- rnorm(100000, mean = 0, sd = 1)\n\n\nThe probability that \\(x\\) is within one standard deviation \\(\\sigma\\) away from the mean is roughly 68%.\n\n\nCode\nmean(x &gt;= -1 & x &lt;= 1)\n\n\n[1] 0.68277\n\n\nThe probability that \\(x\\) is within two standard deviations \\(2\\sigma\\) away from the mean is roughly 95%.\n\n\nCode\nmean(x &gt;= -2 & x &lt;= 2)\n\n\n[1] 0.95483\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe actual value is closer to 1.96 standard deviations away from the mean instead of “2.”\n\n\nCode\nquantile(x, probs = c(0.025, 0.975))\n\n\n     2.5%     97.5% \n-1.957089  1.956044 \n\n\nCode\nqnorm(c(0.025, 0.975), mean = 0, sd = 1)\n\n\n[1] -1.959964  1.959964"
  },
  {
    "objectID": "solutions-04.html#exercise-1",
    "href": "solutions-04.html#exercise-1",
    "title": "11  Solutions 4",
    "section": "11.2 Exercise",
    "text": "11.2 Exercise\n\n\nCode\nquantile(x)\n\n\n          0%          25%          50%          75%         100% \n-4.600188668 -0.677650951 -0.004541977  0.674231122  4.262262328 \n\n\nThe quantile() function calculates the 25th, 50th, and 75th percentiles of a vector of numbers.\nIt also includes the minimum and maximum values:\n\n\nCode\nmin(x)\n\n\n[1] -4.600189\n\n\nCode\nmax(x)\n\n\n[1] 4.262262\n\n\nHey! I hope you remember that the median is the same as the 50th percentile.\n\n\nCode\nmedian(x)\n\n\n[1] -0.004541977"
  },
  {
    "objectID": "solutions-04.html#exercise-2",
    "href": "solutions-04.html#exercise-2",
    "title": "11  Solutions 4",
    "section": "11.3 Exercise",
    "text": "11.3 Exercise\n\n\nCode\nquantile(x, probs = c(0.005, 0.995))\n\n\n     0.5%     99.5% \n-2.566942  2.561872"
  },
  {
    "objectID": "solutions-04.html#exercise-3",
    "href": "solutions-04.html#exercise-3",
    "title": "11  Solutions 4",
    "section": "11.4 Exercise",
    "text": "11.4 Exercise\n\n\nCode\nmean(x &gt; -2.576 & x &lt; 2.576)\n\n\n[1] 0.99029"
  },
  {
    "objectID": "solutions-04.html#exercise-4",
    "href": "solutions-04.html#exercise-4",
    "title": "11  Solutions 4",
    "section": "11.5 Exercise",
    "text": "11.5 Exercise\nLet \\(x = x_1 + \\dots + x_{20}\\), the sum of 20 independent uniform(0, 1) random variables. In R, create 1000 simulations of \\(x\\) and plot their histogram.\n\n\nCode\nd &lt;- tibble(id = 1:1e3) |&gt; \n  rowwise() |&gt; \n  mutate(x = sum(runif(20))) |&gt; \n  ungroup() ## this might save you trouble later on!\n\nd |&gt; ggplot(aes(x)) + geom_histogram(color = \"white\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nBonus\n\n\nCode\nsd(d$x)\n\n\n[1] 1.274413\n\n\nThe real standard error is:\n\n\nCode\nsqrt(20/12)\n\n\n[1] 1.290994\n\n\nAn estimate of the standard error for one sample:\n\n\nCode\nsd(runif(20, min = 0, max = 1)) * sqrt(20)\n\n\n[1] 1.100416\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote\nThere is no “uniquely correct” way of simulating data.\nFor example, this is the way I would usually do it:\n\n\nCode\nN &lt;- 20\nS &lt;- 1e3\nx &lt;- replicate(S, sum(runif(N, min = 0, max = 1)))\n\ntibble(x) |&gt; \n  ggplot(aes(x)) + geom_histogram(color = \"white\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "solutions-04.html#exercise-5",
    "href": "solutions-04.html#exercise-5",
    "title": "11  Solutions 4",
    "section": "11.6 Exercise",
    "text": "11.6 Exercise\nMany introductory statistics textbooks say that “sample sizes equal to or greater than 30 are often considered sufficient for the CLT to hold.” I’ve seen a lot of confusion with regards to this. For example, I’ve heard people say that if the sample size is 30 then the data is normally distributed. Absolutely not! It means that the sampling distribution will converge to a normal distribution. This means that sample sizes less than 30 will not produce “normal” sampling distributions, and we can verify this using simulations.\n\n\n\n\n\n\nTip\n\n\n\nThis relevant for calculating “statistical significance,” which we’ll cover on week 5.\n\n\nThe following code picks up follows Steve’s simulation that we did in class:\n\n\nCode\nsvy_size &lt;- 15 ## this is different!\nest_prop &lt;- 1/3\nnum_sims &lt;- 10e3\n\nsims &lt;- replicate(num_sims, mean(rbinom(svy_size, 1, prob = est_prop)))\nd &lt;- tibble(prop = sims) |&gt; \n  rowid_to_column(\"sim_num\")\n\nd |&gt; summarize(mean = mean(prop), se = sd(prop))\n\n\n# A tibble: 1 × 2\n   mean    se\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.332 0.120\n\n\nThe “mean” value is still the same, but the standard error is bigger because our sample size is 15 instead of 2247. So far, so good.\nNow lets calculate the standard error:\n\n\nCode\nstd_error &lt;- sqrt((1/3) * (2/3) / svy_size)\nstd_error\n\n\n[1] 0.1217161\n\n\nHey, this is pretty much the same as the standard error we calculated in the simulation!\n\n\nCode\nsd(d$prop)\n\n\n[1] 0.1197762\n\n\nFinally, I know that in any normal distribution approximately 68% of the values of should be within one standard deviation away from the mean.\n\n\nCode\nmean(d$prop &gt; 1/3 - sd(d$prop) & d$prop &lt; 1/3 + sd(d$prop))\n\n\n[1] 0.5954\n\n\nOh no! The coverage is all wrong. This means that the sampling distribution we created does not have the properties of a normal distribution. This must be what people mean when they say that “sample sizes equal to or greater than 30 are often considered sufficient for the CLT to hold.”\n\n\n\n\n\n\nTip\n\n\n\nBtw, that previous line of code is a little bit clunky. The dplyr package has a useful function called between() which may be easier to understand.\n\n\nCode\nlower &lt;- 1/3 - sd(d$prop)\nupper &lt;- 1/3 + sd(d$prop)\nmean(between(d$prop, lower, upper))\n\n\n[1] 0.5954"
  },
  {
    "objectID": "solutions-04.html#extra",
    "href": "solutions-04.html#extra",
    "title": "11  Solutions 4",
    "section": "11.7 Extra",
    "text": "11.7 Extra\nLet’s plot how does the sample size affect the number of outcomes that fall within one standard deviations away from the mean.\nThis is how I built my simulation.\nStep. 1 Do it once!\nIs the “true” value inside the estimated 95% confidence interval?\n\n\nCode\ntrue_value &lt;- 1/3\nsample_size &lt;- 300\n\nx &lt;- rbinom(sample_size, 1, prob = true_value)\nestimate &lt;- mean(x)\nestimate\n\n\n[1] 0.33\n\n\nCode\nstd_error_estimate &lt;- sqrt(estimate * (1 - estimate) / sample_size)\nstd_error_estimate\n\n\n[1] 0.02714774\n\n\nCode\n## Is the \"true\" value inside the estimated 95% confidence interval??\nlower &lt;- estimate - 2 * std_error_estimate\nupper &lt;- estimate + 2 * std_error_estimate\n\n# true_value &gt; lower & true_value &lt; upper\nbetween(true_value, lower, upper)\n\n\n[1] TRUE\n\n\nStep 2. Do it 10,000 times!\nWhat percentage of the time is the “true” value inside the 95% confidence interval?\n\n\nCode\ntrue_value &lt;- 1/3\nsample_size &lt;- 300 ## play around with this!\nnum_sims &lt;- 10e3\n\nsims &lt;- replicate(num_sims, {\n  \n  x &lt;- rbinom(sample_size, 1, prob = true_value)\n  estimate &lt;- mean(x)\n  std_error_estimate &lt;- sqrt(estimate * (1 - estimate) / sample_size)\n  ## Is the \"true\" value inside the estimated 95% confidence interval??\n  lower &lt;- estimate - 2 * std_error_estimate\n  upper &lt;- estimate + 2 * std_error_estimate\n  between(true_value, lower, upper)\n  \n})\n\nmean(sims)\n\n\n[1] 0.9491\n\n\nStep 3. Do it 10,000 times for different sample sizes.\n\n\nCode\nsimulation &lt;- function(N, P, S) {\n  \n  out &lt;- replicate(S, expr = {\n  \n    x &lt;- rbinom(N, 1, prob = P)\n    estimate &lt;- mean(x)\n    std_error_estimate &lt;- sqrt(estimate * (1 - estimate) / N)\n    ## Is the \"true\" value inside the estimated 95% confidence interval??\n    lower &lt;- estimate - 2 * std_error_estimate\n    upper &lt;- estimate + 2 * std_error_estimate\n    dplyr::between(P, lower, upper)\n    \n  })\n  \n  return(mean(out))\n  \n}\n\nss &lt;- seq(5, 150, by = 1)\nout &lt;- map_dbl(ss, \\(x) simulation(N = x, P = 1/3, S = 5e3), .progress = TRUE)\n\ntibble(sample = ss, coverage = out) |&gt; \n  ggplot(aes(sample, coverage)) + \n  geom_rect(xmin = 0, ymin = 0, xmax = 30, ymax = Inf, alpha = 1/5) +\n  geom_hline(yintercept = 0.95) + \n  geom_line()"
  },
  {
    "objectID": "solutions-05.html#section",
    "href": "solutions-05.html#section",
    "title": "12  Solutions 5",
    "section": "12.1 ",
    "text": "12.1"
  },
  {
    "objectID": "solutions-06.html#part-i",
    "href": "solutions-06.html#part-i",
    "title": "13  Solutions 6",
    "section": "13.1 Part I",
    "text": "13.1 Part I\n\n13.1.1 Exercise\n\n\n\n\n\n\nChoosing sample size.\nYou are designing a survey to estimate the proportion of individuals who will vote Democrat. Assuming that respondents are a simple random sample of the voting population, how many people do you need to poll so that the standard error is less than 5 percentage points?\n\n\n\nThe answer is somewhat ambiguous because you have to make an assumption about the true population parameter. In this example I had been using \\(p = 0.53\\). In general, people tend to choose \\(p = 0.5\\) because it’s the most conservative estimate—i.e., it gives the maximum standard error.\nYou could have answered this with some algebra—i.e. by solving for \\(n\\) here:\n\\[\n\\begin{align}\n0.05 &gt; \\sqrt{\\frac{p (1-p)}{n}} && \\longrightarrow &&\nn &gt; \\frac{p(1-p)}{0.05^2}\n\\end{align}\n\\]\nOr you could have answered this by playing around with the following simulation until you reached the numbers of size = 100.\n\n\nCode\ndraws &lt;- rbinom(1e4, size = 100, prob = 0.53)\nproportions &lt;- draws / 100\nsd(proportions)\n\n\n[1] 0.04989916\n\n\nHere’s a plot:\n\n\nCode\nggplot() +\n  xlim(10, 400) +\n  ylim(0, 0.18) +\n  geom_function(fun = \\(x) sqrt(0.5^2 / x)) + \n  geom_hline(yintercept = c(0.05, 0.025), linetype = \"dashed\") + \n  geom_vline(xintercept = 100, linetype = \"dashed\") +\n  labs(y = \"Standard Error\", x = \"Sample Size\", title = \"Standard Error, Sample Size, and Proportion\")\n\n\n\n\n\nNote that halving the standard error requires multiplying the sample size by a factor of four!\n\n\n13.1.2 Exercise\n\n\n\n\n\n\nWhat is the probability of observing the “true” value ( \\(p = 0.53\\) ) under the null?\nWhat is the probability of observing prop_hat under the null? Is this statistically significant if the confidence level ( \\(\\alpha\\) ) is set to 0.05?\n\n\n\nIn this exercise, we had a sample size of 1000.\nThe sampling distribution for the null ( \\(H_0\\) ) of \\(p = 0.5\\) is as follows:\n\n\nCode\nS &lt;- 1e4 ## number of simulated draws\npoll_size &lt;- 1000 ## sample size\n\ndraws &lt;- rbinom(S, size = poll_size, prob = 0.50)\nnull &lt;- draws / poll_size\n\ntibble(p = null) |&gt; \n  ggplot(aes(p)) + \n  geom_histogram(color = \"white\", boundary = 0.5, binwidth = 0.005) + \n  annotate(\"rect\", xmin = 0.53, ymin = 0, xmax = Inf, ymax = Inf, fill = \"pink\", alpha = 1/4) + \n  scale_x_continuous(n.breaks = 10)\n\n\n\n\n\nAnd the probability is computed as follows:\n\n\nCode\nmean(null &gt; 0.53)\n\n\n[1] 0.0251\n\n\nThe prop_hat value was estimated at 0.513.\n\n\nCode\nmean(null &gt; 0.513)\n\n\n[1] 0.1959\n\n\nNote that the sample from which prop_hat was calculated was drawn from the “true” sampling distribution. The p-value is estimated to be 0.2, so we—mistakenly—failed to reject the “null hypothesis.” This mistake is usually called “Type II error” or “false negative.”\nTake note of the shaded areas in the following graph:\n\n\nCode\ndraws &lt;- rbinom(S, size = poll_size, prob = 0.53)\ntrue_dist &lt;- draws / poll_size\n\ng &lt;- tibble(true = true_dist, null = null) |&gt; \n  ggplot() +\n  geom_histogram(aes(true, fill = \"true\"), color = \"white\", boundary = 0.5, binwidth = 0.005) +\n  geom_histogram(aes(null, fill = \"null\"), color = \"white\", boundary = 0.5, binwidth = 0.005, alpha = 1/2) +\n  labs(x = \"proportion\", fill = \"Sampling\\nDistribution\")\n\ng\n\n\n\n\n\nIn this question, setting the confidence interval as \\(\\alpha = 0.05\\) means that we will compare the estimated proportion to whatever value produces a tail area probability of 0.05. I fiddled around with the null distribution and decided that this value was 0.526.\n\n\nCode\nmean(null &gt; 0.526)\n\n\n[1] 0.0459\n\n\nCode\n## of the \"analytical version\":\nqnorm(0.95, mean = 0.5, sd = 0.0158)\n\n\n[1] 0.5259887\n\n\nCode\ng + geom_vline(xintercept = qnorm(0.95, mean = 0.5, sd = 0.0158), linetype = \"dashed\")\n\n\n\n\n\nNote that by setting the confidence level to 0.05, we are also inadvertently setting the probability of a “false negative.”\n\n\nCode\nmean(true_dist &lt; qnorm(0.95, mean = 0.5, sd = 0.0158))\n\n\n[1] 0.3884\n\n\nIt’s almost 40%\nPeople don’t usually think about type II error because it is outside their control—i.e., it requires for us to know the “true” value. But that doesn’t mean we shouldn’t carefully think about such things!\n\n\n13.1.3 Exercise\n\n\n\n\n\n\nThe formula for calculating the standard error of a difference in proportions is given by:\n\\[\n\\sigma_{\\hat p_1 - \\hat p_2} = \\sqrt{\\frac{p_1 (1 - p_1)}{n_1} + \\frac{p_2 (1 - p_2)}{n_2}}\n\\]\nVerify that this standard error corresponds to sd(theta_distribution).\n\n\n\nIf you didn’t get the same answer, then you made some kind of weird mistake!\n\n\n13.1.4 Exercise\n\n\n\n\n\n\nComparison of proportions.\nA randomized experiment is performed within a survey. 1000 people are contacted. Half the people contacted are promised a $5 incentive to participate, and half are not promised an incentive. The result is a 50% response rate among the treated group and 40% response rate among the control group. Give an estimate and standard error of the difference in proportions.\n\n\n\nUsing math to answer this is OK.\nBut this is how I would have liked you to answer this question:\n\n\nCode\np_treatment &lt;- 0.5\np_control &lt;- 0.4\n\nS &lt;- 10e4 ## number of simulated draws\n\ntreatment &lt;- rbinom(S, 500, p_treatment) / 500\ncontrol &lt;- rbinom(S, 500, p_control) / 500\neffect &lt;- treatment - control\nmean(effect)\n\n\n[1] 0.09995086\n\n\nCode\nsd(effect)\n\n\n[1] 0.03129886\n\n\nUsing the formula:\n\n\nCode\nsqrt((0.5*0.4/500) + (0.5^2/500))\n\n\n[1] 0.03"
  },
  {
    "objectID": "solutions-06.html#part-ii",
    "href": "solutions-06.html#part-ii",
    "title": "13  Solutions 6",
    "section": "13.2 Part II",
    "text": "13.2 Part II\n\n\nCode\ngss18 &lt;- gss_get_yr(2018) \n\n\nFetching: https://gss.norc.org/documents/stata/2018_stata.zip\n\n\nCode\nd &lt;- gss18 |&gt; \n  select(sex, attend, polviews) |&gt; \n  haven::zap_missing() |&gt; \n  mutate(sex = as_factor(sex)) |&gt; \n  haven::zap_labels() |&gt; \n  drop_na()\n\nglimpse(d)\n\n\nRows: 2,235\nColumns: 3\n$ sex      &lt;fct&gt; male, male, female, male, female, female, male, female, male,…\n$ attend   &lt;dbl&gt; 5, 2, 6, 8, 4, 7, 7, 0, 4, 5, 0, 3, 0, 7, 1, 0, 4, 5, 2, 7, 1…\n$ polviews &lt;dbl&gt; 6, 5, 4, 7, 3, 4, 5, 4, 6, 4, 4, 3, 2, 5, 2, 6, 2, 4, 6, 6, 4…\n\n\n\n13.2.1 Exercise\n\n\n\n\n\n\nGo to the GSS website and describe the values of attend and polviews—e.g., what does a value of “4” mean in polviews.\n\n\n\nJust describe the variables!\nhttps://gssdataexplorer.norc.org/variables/vfilter\n\n\n13.2.2 Exercise\n\n\n\n\n\n\nRepeat what we did in class with Steve, but compare the weekly variable to a new variable call conservative.\nhttps://github.com/vaiseys/soc-stats-1/blob/main/demos/day-12.R\n\n\n\n\n\nCode\nd &lt;- d |&gt; \n  mutate(female = if_else(sex == \"female\", 1L, 0L),\n         weekly = if_else(attend &gt;= 7, 1L, 0L),\n         conservative = if_else(polviews &gt;= 5, 1L, 0L)) |&gt; \n  drop_na() \n\nd |&gt; \n  tabyl(conservative, weekly) |&gt; \n  adorn_percentages(\"row\") |&gt;\n  adorn_pct_formatting(digits = 2) |&gt; \n  adorn_ns()\n\n\n conservative              0            1\n            0 82.87% (1,243) 17.13% (257)\n            1 67.35%   (495) 32.65% (240)\n\n\nCode\nd |&gt; \n  group_by(conservative) |&gt; \n  mutate(conservative = if_else(conservative == 1L, \"conservative\", \"other\")) |&gt; \n  summarise(percent = mean(weekly)) |&gt; \n  ggplot(aes(conservative, percent)) + \n  geom_col(width = 1/2) + \n  scale_y_continuous(labels = scales::percent_format(1)) + \n  labs(y = NULL, title = \"Percent of People who Attend Church Weekly\")\n\n\n\n\n\n\n\n\n\nCode\nboot &lt;- d |&gt; \n  mutate(conservative = if_else(conservative == 1L, \"conservative\", \"other\")) |&gt; \n  specify(weekly ~ conservative) |&gt; \n  generate(reps = 1e4, type = \"bootstrap\") |&gt; \n  calculate(stat = \"diff in means\", order = c(\"conservative\", \"other\"))\n\nci &lt;- get_confidence_interval(boot)\n\n\nUsing `level = 0.95` to compute confidence interval.\n\n\nCode\nboot |&gt; \n  visualize() +\n  shade_ci(ci)\n\n\n\n\n\n\n\n\n\nCode\nobs_diff &lt;- mean(d$weekly[d$conservative == 1L]) - mean(d$weekly[d$conservative == 0L])\n\nd_summary &lt;- d |&gt; \n  group_by(conservative) |&gt; \n  summarize(weekly_percent = mean(weekly)) |&gt; \n  pivot_wider(names_from = conservative, values_from = weekly_percent, names_prefix = \"conservative_\")\n\nd_summary &lt;- d_summary |&gt; \n  mutate(obs_diff = conservative_1 - conservative_0)\n\nd_summary$obs_diff\n\n\n[1] 0.1551973\n\n\nCode\nnull &lt;- d |&gt; \n  mutate(conservative = if_else(conservative == 1L, \"conservative\", \"other\")) |&gt; \n  specify(weekly ~ conservative) |&gt; \n  hypothesize(null = \"independence\") |&gt; \n  generate(reps = 1e4, type = \"permute\") |&gt; \n  calculate(stat = \"diff in means\", order = c(\"conservative\", \"other\")) \n\nnull\n\n\nResponse: weekly (numeric)\nExplanatory: conservative (factor)\nNull Hypothesis: independence\n# A tibble: 10,000 × 2\n   replicate      stat\n       &lt;int&gt;     &lt;dbl&gt;\n 1         1 -0.0131  \n 2         2 -0.00293 \n 3         3  0.0153  \n 4         4 -0.000898\n 5         5  0.0194  \n 6         6 -0.0110  \n 7         7  0.0214  \n 8         8 -0.0232  \n 9         9  0.00721 \n10        10  0.0133  \n# ℹ 9,990 more rows\n\n\nCode\nnull |&gt; \n  visualize() + \n  geom_vline(xintercept = d_summary$obs_diff, linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n\n\n13.2.3 Exercise\n\n\n\n\n\n\nIs the difference in proportions between conservative and weekly statistically significant?\n\n\n\nYes.\n\n\n13.2.4 Exercise\n\n\n\n\n\n\nInstead of discretizing the polviews and attend (which is what we did to create weekly and conservative), let’s try interpreting their original values.\nI made the following plot using geom_tile to make the point that contingency tables are hard to interpret.\nTry to describe what’s going on here.\n\n\n\n\n\n\n\n\n\n\n\n\nGood luck!\nPleas note that most people think of themselves as “moderate” and that very few of them attend religious services. There is, however, a big chunk of people that identify as conservative ( \\(n = 104\\) ) that attend every week.\n\n\n13.2.5 Bonus\n\n\n\n\n\n\nDo a Chi square test on polviews and attend using the infer package.\nThe null hypothesis is that these variables are “independent” of each other.\n\n\n\n\n\nCode\nd &lt;- d |&gt; \n  mutate(polviews = as_factor(polviews), attend = as_factor(attend))\n\nglimpse(d)\n\n\nRows: 2,235\nColumns: 6\n$ sex          &lt;fct&gt; male, male, female, male, female, female, male, female, m…\n$ attend       &lt;fct&gt; 5, 2, 6, 8, 4, 7, 7, 0, 4, 5, 0, 3, 0, 7, 1, 0, 4, 5, 2, …\n$ polviews     &lt;fct&gt; 6, 5, 4, 7, 3, 4, 5, 4, 6, 4, 4, 3, 2, 5, 2, 6, 2, 4, 6, …\n$ female       &lt;int&gt; 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, …\n$ weekly       &lt;int&gt; 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ conservative &lt;int&gt; 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, …\n\n\nCode\nd |&gt; \n  specify(attend ~ polviews) |&gt;\n  calculate(stat = \"Chisq\")\n\n\nResponse: attend (factor)\nExplanatory: polviews (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  232.\n\n\nCode\nchisq.test(d$attend, d$polviews)\n\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  d$attend and d$polviews\nX-squared = 231.99, df = 48, p-value &lt; 2.2e-16\n\n\nCode\nnull &lt;-  d |&gt; \n  specify(attend ~ polviews) |&gt;\n  hypothesise(null = \"independence\") |&gt; \n  generate(reps = 1e3, type = \"permute\") |&gt; \n  calculate(stat = \"Chisq\") \n\nnull |&gt; \n  infer::visualise()\n\n\n\n\n\nCode\nnull |&gt; \n  infer::visualise() +\n  geom_vline(xintercept = 231.9905, linetype = \"dashed\")\n\n\n\n\n\n\\(\\chi^2\\) test without the infer package:\n\n\nCode\nobserved &lt;- table(attend = d$attend, polviews = d$polviews)\nobserved\n\n\n      polviews\nattend   1   2   3   4   5   6   7\n     0  69 128  77 248  66  74  16\n     1   5  17  18  52  20  14   0\n     2   9  30  37 130  41  36   8\n     3   7  27  28 104  23  35   7\n     4   3  11  21  53  23  23   9\n     5   9  18  19  74  31  23   8\n     6   3  10  11  25  13  20   5\n     7  12  24  34 125  51 104  25\n     8   5  12  10  35  15  24  21\n\n\nCode\n## using the formula (this is a little \"hack\" you might have not seen before)\nn &lt;- sum(observed)\ncols &lt;- colSums(observed) \nrows &lt;- rowSums(observed) \nexpected &lt;- outer(rows, cols) / n\nexpected\n\n\n          1        2         3         4        5         6         7\n0 37.009396 84.02953 77.355705 256.63893 85.84966 107.08456 30.032215\n1  6.877852 15.61611 14.375839  47.69396 15.95436  19.90067  5.581208\n2 15.884564 36.06577 33.201342 110.15034 36.84698  45.96107 12.889933\n3 12.609396 28.62953 26.355705  87.43893 29.24966  36.48456 10.232215\n4  7.805817 17.72304 16.315436  54.12886 18.10694  22.58568  6.334228\n5  9.934676 22.55660 20.765101  68.89128 23.04519  28.74541  8.061745\n6  4.748993 10.78255  9.926174  32.93154 11.01611  13.74094  3.853691\n7 20.469799 46.47651 42.785235 141.94631 47.48322  59.22819 16.610738\n8  6.659508 15.12036 13.919463  46.17987 15.44787  19.26890  5.404027\n\n\nCode\n## chi-square statistic\nsum((observed - expected)^2 / expected)\n\n\n[1] 231.9905\n\n\nCode\n## null distribution\nnull &lt;- replicate(1e4, {\n  ob_draw &lt;- table(attend = sample(d$attend), polviews = sample(d$polviews))\n  sum((ob_draw - expected)^2 / expected)\n})\n\ntibble(null) |&gt; \n  ggplot(aes(null)) + \n  geom_histogram(color = \"white\") +\n  geom_vline(linetype = \"dashed\", xintercept = sum((observed - expected)^2 / expected)) + \n  labs(title = latex2exp::TeX(\"$\\\\chi^2$ Simulation-Based Null Distribution\"))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nBack to the interpretation:\n\n\nCode\nas_tibble(observed - expected, n = \"x\") |&gt; \n  ggplot(aes(attend, polviews, fill = x^2)) + \n  geom_tile(color = \"white\", linewidth = 1.5, show.legend = FALSE) +\n  geom_text(aes(label = round(x, 2))) +\n  scale_fill_gradient(low = \"white\") +\n  theme_minimal(base_line_size = 0, base_family = \"Optima\") +\n  labs(title = \"Observed - Expected Counts\")"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Blitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to\nProbability. CRC Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and\nOther Stories. Cambridge University Press.\n\n\nHealy, Kieran. 2018. Data\nVisualization: A Practical Introduction.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social\nScience: An Introduction in Tidyverse. Princeton University Press.\n\n\nIsmay, Chester, and Albert Y. Kim. 2019. Statistical Inference via\nData Science: A ModernDive into r and the Tidyverse. CRC Press.\n\n\nLlaudet, Elena, and Kosuke Imai. 2022. Data Analysis for Social\nScience: A Friendly and Practical Introduction. Princeton\nUniversity Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in r and Stan. CRC press.\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in\nStatistical Inference. Springer.\n\n\nWickham, Hadley. 2019. Advanced R. CRC Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. \" O’Reilly Media,\nInc.\"."
  },
  {
    "objectID": "week7.html#four-ways-to-make-sense-of-dependence",
    "href": "week7.html#four-ways-to-make-sense-of-dependence",
    "title": "7  Week 7",
    "section": "7.2 Four Ways to Make Sense of Dependence",
    "text": "7.2 Four Ways to Make Sense of Dependence\n\n7.2.1 Exercise\n\n\n\n\n\n\nTake another look at the weekly vs conservative contingency table.\nFocus on these two probabilities:\n\\[\n\\begin{align}\n\\Pr(W \\mid C) &&\\text{and} && \\Pr(W \\mid C^\\complement)\n\\end{align}\n\\]\nCompare these probabilities using the following summary statistics:\n\nDifference in Probabilities\nRelative Risk Ratio\nOdds Ratio\nLog Odds Ratio\n\n\n\n\n\n\n7.2.2 Exercise\n\n\n\n\n\n\nLast week we used the infer package to draw a “simulation-based bootstrap distribution” for the difference in proportions between \\(\\Pr(W \\mid C)\\) and \\(\\Pr(W \\mid C^\\complement)\\).\nNow we mix things up a little bit.\n\nDraw a sampling distribution for the “relative risk ratio” (Hint: infer calls this “ratio of props”)\nDraw a sampling distribution for the “odds ratio”\n\n\n\n\n\n\n7.2.3 Exercise\n\n\n\n\n\n\nRepeat exercise 7.2.1 using the contingency table you created in exercise 7.1.2\n\n\n\n\n\n7.2.4 Exercise\n\n\n\n\n\n\nRepeat exercise 7.2.2 using the contingency table you created in exercise 7.1.2\n\n\n\n\n\n\n\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton University Press."
  }
]