[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Statistics I (Exercises)",
    "section": "",
    "text": "Preface\nSyllabus\nHi everyone, I will be uploading the homework questions to this website.\nFeel free to reach out to me with any questions."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Social Statistics I (Exercises)",
    "section": "Resources",
    "text": "Resources\nThese are my personal recommendations for resources to start getting interested in statistics. It’s somewhat incomplete—e.g., there are no dedicated textbooks to causal inference, which is what we’ll cover next semester.\nClass Resources:\n\nR for Data Science (Wickham et al. 2023)\nI learned a lot using the first edition of this book. Feel free to skip some chapters on a first pass and come back to them if you think you might need them (e.g., strings, regular expressions, webscraping).\nAlso, I suggest you start with chapters 29 and 30.\nThe tidyverse style guide\nIt will help you write pretty code.\nData Visualization (Healy 2018)\nIt will help you make good graphs.\n\nGood books to play around with:\n\nData Analysis for Social Science: A Friendly and Practical Introduction (Llaudet and Imai 2022)\nVery introductory but useful.\nStatistical Inference via Data Science (Ismay and Kim 2019)\nIt’s good!\nQuantitative Social Science: An Introduction in Tidyverse (Imai and Williams 2022)\nI read this one a while ago, before it was re-written in tidyverse dialect. The chapters on probability and uncertainty are a great self-contained introduction to probability and statistical inference.\nRegression and Other Stories (Gelman et al. 2020)\nThis one seems a little too advanced for a first pass, but not advanced enough for a second pass? I like it a lot though.\n\nAdvanced Resources:\n\nAdvanced R (Wickham 2019).\nThis one is good for those of you that have a background in computer science and are looking for reasons to like R. It’s also good for those of you who finished (most of) R4DS and want to learn more general programming ideas.\nIntroduction to probability (Blitzstein and Hwang 2019)\nCovers more probability theory than what you’ll probably need, but it’s a fascinating topic and it’s very accessible.\nStatistical Rethinking (McElreath 2020)\nIt’s what the cool kids are into these days.\n\n\n\n\n\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to Probability. CRC Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nHealy, Kieran. 2018. Data Visualization: A Practical Introduction.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social Science: An Introduction in Tidyverse. Princeton University Press.\n\n\nIsmay, Chester, and Albert Y. Kim. 2019. Statistical Inference via Data Science: A ModernDive into r and the Tidyverse. CRC Press.\n\n\nLlaudet, Elena, and Kosuke Imai. 2022. Data Analysis for Social Science: A Friendly and Practical Introduction. Princeton University Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. CRC press.\n\n\nWickham, Hadley. 2019. Advanced R. CRC Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "sec1.html",
    "href": "sec1.html",
    "title": "Computing preliminaries",
    "section": "",
    "text": "That time we learned how to program in R, write simple reports using .qmd documents, and review some high school math."
  },
  {
    "objectID": "week1.html#data-structures",
    "href": "week1.html#data-structures",
    "title": "1  Week 1",
    "section": "1.1 Data Structures",
    "text": "1.1 Data Structures\n\n1.1.1 Vectors\nR has two kinds of vectors:\n\nAtomic vectors. All elements of the vector must have the same type (e.g., logical, integer, double, character).1 They are homogeneous.\nSee Figure 1.1.\nNote that integer and double are collectively known as numeric.\nLists. The elements of the vector can have different types. They can be heterogeneous.\n\nAll data structures—e.g., data frames, matrices, factors, dates, and more complex model objects—are built on top of these. All of these will have an additional class attribute. For example, a “data frame” (e.g., mtcars) is basically a list of atomic vectors that have the same length().\n\n\n\nFigure 1.1: Atomic vectors\nhttps://adv-r.hadley.nz/vectors-chap.html#atomic-vectors\n\n\n\n\n\n\n\n\nExercise 1.1\nTry typing typeof(mtcars) and class(mtcars) in the console to see what happens.\nNow type the following chunks of code into your console and understand what they do:\n\nnrow(mtcars)\nncol(mtcars)\nlength(mtcars)\ndim(mtcars)\nrownames(mtcars)\ncolnames(mtcars)\n\nBriefly describe what each of these do.\n\n\n\nNote. The absence of a vector is usually represented with NULL (as opposed to NA which is used to represent the absence of a value in a vector). NULL typically behaves like a vector of length 0.\nCreating vectors of length 1 (scalars).\nEach of the four primary types depicted in Figure 1.1 can be created using a special syntax:\n\nLogicals can be written in full (TRUE or FALSE), or abbreviated (T or F).\nDoubles are the default for numbers (123). They can also be specified in decimal (0.1234) and scientific (1.23e4).\nThere are three special values unique to doubles: Inf, -Inf, and NaN (not a number). Don’t worry about these for now!\nIntegers are written similarly to doubles but must be followed by L (1234L)\nStrings are surrounded by \" (\"hi\") or ' ('bye').\n\n\n\n\n\n\n\nExercise 1.2\nI suggest you always use long-form when creating logical vectors. Try assigning a different value to TRUE and to T.\n\n\nCode\nT &lt;- 123\nTRUE &lt;- 123\n\n\nWhat just happened?\nExercise 1.3\nImplicit coercion\nYou can create atomic vectors of any length with c() for “concatenate”.\nFor example:\n\n\nCode\nlgl &lt;- c(TRUE, FALSE, NA)\nint &lt;- c(1L, 6L, NA, 10L)\ndbl &lt;- c(1, NA, 2.5, 4.5)\nchr &lt;- c(NA, \"these are\", \"some strings\")\n\n\nRecall that atomic vectors are homogeneous. If you try to concatenate vectors of different types you will end up discovering implicit coercion. Basically, different types will be coerced in the following order: logical → integer → double → character.\nFor example, a logical and a character combine into a character:\n\n\nCode\nstr(c(TRUE, \"chr\")) ## str() is (almost) identical to dplyr::glimpse()\n\n\n chr [1:2] \"TRUE\" \"chr\"\n\n\nTest your knowledge of the vector coercion rules by predicting the output of the following uses of c():\n\n\nCode\nc(1, FALSE)\nc(\"a\", 1)\nc(TRUE, 1L)\n\n\nExercise 1.4\nExplicit coercion\nExplicit coercion happens when you call a function like as.logical(), as.integer(), as.double(), or as.character(). Use as.integer() on FALSE and TRUE, what values do they get coerced to?\nExercise 1.5\nThe most common form of implicit coercion\nThe following chunk of code creates a logical vector of size 75.\n\n\nCode\nx &lt;- sample(c(TRUE, FALSE), size = 75, replace = TRUE)\nstr(x)\n\n\n logi [1:75] FALSE FALSE FALSE TRUE FALSE TRUE ...\n\n\nUse sum(x) to get the number of TRUE values. Use mean(x) to get the proportion of TRUE values. Verify that mean(x) and sum(x) / length(x) give the same value.\n\n\n\nWe will usually use logical operators to transform a variable and then do the kinds of calculations in Exercise 1.5.\nFor example:\n\n\nCode\n## the proportion of cars in the dataset with more than 3 carburators\nmean(mtcars$carb &gt; 3)\n\n\n[1] 0.375\n\n\nSequences\nWe will sometimes create sequences of integers for various purposes (e.g., subsetting). For example, we can use the seq() to create a sequence of even numbers this way:\n\n\nCode\nseq(from = 2, to = 26, by = 2)\n\n\n [1]  2  4  6  8 10 12 14 16 18 20 22 24 26\n\n\nYou can create a simple sequence of numbers from x1 to x2 by using the : operator this way:\n\n\nCode\n1:10\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseq(1, 10, by = 1)\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\n1.1.2 Subsetting\nVectors\nAs a reminder, you can subset named lists (and therefore data frames) with the $ operator.\nFor example:\n\n\nCode\nmtcars$mpg\n\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n\nCode\nx &lt;- list(chr, lgl, letters)\nstr(x)\n\n\nList of 3\n $ : chr [1:3] NA \"these are\" \"some strings\"\n $ : logi [1:3] TRUE FALSE NA\n $ : chr [1:26] \"a\" \"b\" \"c\" \"d\" ...\n\n\nCode\nnames(x) &lt;- c(\"chr\", \"lgl\", \"alphabet\")\nstr(x)\n\n\nList of 3\n $ chr     : chr [1:3] NA \"these are\" \"some strings\"\n $ lgl     : logi [1:3] TRUE FALSE NA\n $ alphabet: chr [1:26] \"a\" \"b\" \"c\" \"d\" ...\n\n\nCode\nx$alphabet\n\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n\nYou can also do this using the [ and [[ operators, but this time you have to put the name in quotation marks.\nThus:\n\n\nCode\nmtcars[[mpg]]\n\n\nError in (function(x, i, exact) if (is.matrix(i)) as.matrix(x)[[i]] else .subset2(x, : object 'mpg' not found\n\n\nCode\nmtcars[[\"mpg\"]]\n\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n\n\n\n\n\n\n\nExercise 1.6\nWhat is the difference between mtcars[\"mpg\"] and mtcars[[\"mpg\"]]? More generally, what is the difference between the [ and [[ operators?\nWhich of the following two is TRUE?\n\n\nCode\nidentical(mtcars[\"mpg\"], mtcars$mpg)\nidentical(mtcars[[\"mpg\"]], mtcars$mpg)\n\n\n\n\n\nYou will be subsetting different kinds of things in R—mostly data frames—using integers and logicals.\n\n\n\n\n\n\nExercise 1.7\nletters is a built-in object in R that contains the 26 letters of English alphabet.\nUsing the [ operator, do the following:\n\nExtract the 17th value of letters\nCreate a sequence of even numbers from 2 to 26 and use that to subset letters\nUse 8:12 to subset letters.\n\nThis is known as integer subsetting.\nWhat happens if instead of [ you use [[?\n\n\n\nSubsetting + Assignment\nYou can use the assignment operator &lt;- in combination with subsetting to replace the values of a vector.\nFor example:\n\n\nCode\ndbl\n\n\n[1] 1.0  NA 2.5 4.5\n\n\nCode\ndbl[1] &lt;- 10\ndbl\n\n\n[1] 10.0   NA  2.5  4.5\n\n\nCode\ndbl[is.na(dbl)] &lt;- 0\ndbl\n\n\n[1] 10.0  0.0  2.5  4.5\n\n\nMake sure you understand what is.na() is doing here. Did we just do “integer” or “logical” subsetting?\n\n\n\n\n\n\nExercise 1.8\nNow that you know all this\nReplace the 18th value of letters with a missing value (NA).\n\n\n\n\n\n1.1.3 The most common error you’ll see\n\n\nCode\nmean[1:5]\n\n\nError in mean[1:5]: object of type 'closure' is not subsettable\n\n\nThis just means that you have tried to subset a function, and functions are most definitely not vectors.\nThis will happen for example if you think you created a dataset called df and try to extract a column:\n\n\nCode\ndf$col\n\n\nError in df$col: object of type 'closure' is not subsettable\n\n\n\n\n1.1.4 Data Frames\nThe most obvious use case $, [, or [[ is in the context of working with data frames.\nHere we will use the [ operator behaves differently when used on some objects—e.g., data frames and matrices.\n\nWhen subsetting with a single index, data frames behave like lists and index the columns, so mtcars[1:2] selects the first two columns.\nWhen subsetting with two indices, mtcars[1:3, ] selects the first three rows (and all the columns); mtcars[5, ] selects the fifth row and all columns; mtcars[1:5, \"cyl\"] selects the cyl column and the first five rows. Matrices behave in the same way.\n\nLogical subsetting\nThe most common way of using logicals to subset data frames is to learn some Boolean operators—e.g., &lt;, &lt;=, &gt;, &gt;=, !=, and ==.\nFor example, we can create a logical vector that tests for mtcars$wt values greater than 4.\n\n\nCode\nmtcars$wt &gt; 4\n\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[13] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nAnd then we can subset with [:\n\n\nCode\nmtcars[mtcars$wt &gt; 4, ]\n\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n\n\n\n\n\n\n\n\nExercise 1.9\nSubset mtcars so that we only see the observations for which cyl == 4.\nSubset mtcars so that we only see the observations for which mpg is greater than 23.\n\n\n\nSometimes it will be easier to use the %in% operator to test for many conditions at the same time.\nFor example:\n\n\nCode\nmtcars[mtcars$carb %in% c(3, 6, 8), ]\n\n\n               mpg cyl  disp  hp drat   wt qsec vs am gear carb\nMerc 450SE    16.4   8 275.8 180 3.07 4.07 17.4  0  0    3    3\nMerc 450SL    17.3   8 275.8 180 3.07 3.73 17.6  0  0    3    3\nMerc 450SLC   15.2   8 275.8 180 3.07 3.78 18.0  0  0    3    3\nFerrari Dino  19.7   6 145.0 175 3.62 2.77 15.5  0  1    5    6\nMaserati Bora 15.0   8 301.0 335 3.54 3.57 14.6  0  1    5    8\n\n\nAlternatively we could have done this:\n\n\nCode\nmtcars[mtcars$carb == 3 | mtcars$carb == 6 | mtcars$carb == 8, ]\n\n\n               mpg cyl  disp  hp drat   wt qsec vs am gear carb\nMerc 450SE    16.4   8 275.8 180 3.07 4.07 17.4  0  0    3    3\nMerc 450SL    17.3   8 275.8 180 3.07 3.73 17.6  0  0    3    3\nMerc 450SLC   15.2   8 275.8 180 3.07 3.78 18.0  0  0    3    3\nFerrari Dino  19.7   6 145.0 175 3.62 2.77 15.5  0  1    5    6\nMaserati Bora 15.0   8 301.0 335 3.54 3.57 14.6  0  1    5    8\n\n\nWhich do you prefer?"
  },
  {
    "objectID": "week1.html#search",
    "href": "week1.html#search",
    "title": "1  Week 1",
    "section": "1.2 Search",
    "text": "1.2 Search\nIn this section I will introduce two functions that are pretty much useless except for the fact that they will help use understand how R finds “objects” in your R session: search() and find().\nType search() into your console. I you are like me—and haven’t loaded any package yet—you should see the exact same output as this:\n\n\nCode\nsearch()\n\n\n[1] \".GlobalEnv\"        \"package:stats\"     \"package:graphics\" \n[4] \"package:grDevices\" \"package:utils\"     \"package:datasets\" \n[7] \"package:methods\"   \"Autoloads\"         \"package:base\"     \n\n\n.GlobalEnv is the what’s known as the “global environment.” Any variable that shows up in your Environment pane is stored there.\nType names(.GlobalEnv) into the console and see what shows up. You’ll notice that there’s an object called .Random.seed in .GlobalEnv that doesn’t show up in your Environment pane. That’s because RStudio “hides” any variable that has a . prefix.\n\n\nCode\n.secret_var &lt;- 1:10\n.secret_var\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nYou should not be able to see .secret_var in the Environment pane and yet it’s there!\nAny time you type something in R, it will proceed to search for it sequentially: first, in .GlobalEnv, then in the built-in stats package, then in graphics, and so on until it reaches the base package.2\nSo, if you type asdfasdfasdf into the console, R will search all these environments and produce an error once it comes out empty handed.\n\n\nCode\nasdfasdfasdf\n\n\nError in eval(expr, envir, enclos): object 'asdfasdfasdf' not found\n\n\nBut if you type mtcars, R will search all these environments until it finds mtcars living in the built-in datasets package.\nYou can verify that this is the case using the find() function.\n\n\nCode\nfind(\"mtcars\")\n\n\n[1] \"package:datasets\"\n\n\nNow, suppose you decide to create an object called mtcars, which then gets saved to the global environment.\n\n\nCode\nmtcars &lt;- \"this is not the mtcars dataset\"\nmtcars\n\n\n[1] \"this is not the mtcars dataset\"\n\n\nIf you now type find(\"mtcars\") into the console you’ll see the names of two environments in the order that R searches for mtcars.\n\n\nCode\nfind(\"mtcars\")\n\n\n[1] \".GlobalEnv\"       \"package:datasets\"\n\n\nYou can still access the original mtcars dataset using the :: operator like this:\n\n\nCode\nstr(datasets::mtcars)\n\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\nIn fact, you can use pkgname::obj to access any object in any package (even if you haven’t loaded it yet).\nFor example:\n\n\nCode\ndplyr::glimpse(datasets::mtcars)\n\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\n1.2.1 Errors!\nSuppose you want to analyze the penguins dataset contained in the palmerpenguins package.\nYou will have to type the following into your console once if you haven’t already:\n\n\nCode\ninstall.packages(\"palmerpenguins\")\n\n\nYou want to use the table() function to count up the number of times a specific year shows up in the dataset.\n\n\nCode\n# library(tidyverse)\n# library(palmerpenguins)\ntable(year)\n\n\nError in table(year): object 'year' not found\n\n\nOh no, an error!\nAfter realizing that you “commented out” the library(pkg) lines, you remove the # symbols and do this:\n\n\nCode\nlibrary(tidyverse)\nlibrary(palmerpenguins)\ntable(year)\n\n\nError in unique.default(x, nmax = nmax): unique() applies only to vectors\n\n\nOh no, a different error!\nYou realize that table() has no way of knowing that you wanted to access the year variable in the penguins dataset. You forgot to subset!\nThis should work:\n\n\nCode\ntable(penguins$year)\n\n\n\n2007 2008 2009 \n 110  114  120 \n\n\n\n\n\n\n\n\nExercise 1.10\nUsing what I told you earlier about the search() function, explain why you get two different errors. What is going on? What is R doing when you type table(year)? (You might want to type search() into the console again). In what package does R find the year object?"
  },
  {
    "objectID": "week1.html#dplyr-subsetting",
    "href": "week1.html#dplyr-subsetting",
    "title": "1  Week 1",
    "section": "1.3 dplyr subsetting",
    "text": "1.3 dplyr subsetting\nYou will almost never subset data frames in the way we did for the previous exercises. In fact, we won’t use “base R” much. Part of the reason we use tidyverse instead of base R is because the documentation for the tidyverse is excellent. Also, the error messages are easier to understand.\nWe will subset data frames using two functions contained in the dplyr package:\n\nslice() for integer subsetting.\nfilter() for logical subsetting.\n\n\nI recommend you follow these two links.\n\nDon’t forget to library(tidyverse) if you haven’t done so already.\n\n\nCode\nlibrary(tidyverse)\n\n\n\n\n\n\n\n\nExercise 1.11\nUse slice() to extract the even-numbered rows in the penguins dataset.\nIt will look something like this:\n\n\nCode\npenguins |&gt; \n  slice(\"SOME NUMERIC VECTOR GOES HERE\")\n\n\nNow use slice() to extract every third row—i.e., row 3, 6, 9, and so on.\nExercise 1.12\nUse filter() to extract the observations in the penguins dataset for which species == \"Gentoo\", island == \"Biscoe\", and body_mass_g is between 5,000 and 5,500."
  },
  {
    "objectID": "week1.html#footnotes",
    "href": "week1.html#footnotes",
    "title": "1  Week 1",
    "section": "",
    "text": "Technically, there are two other types of atomic vectors: complex and raw. I don’t think you’ll see much of these.↩︎\nIgnore “Autoloads” and “tools:rstudio” (this last one shows up when you type search() in the console if you are using RStudio). This is not important!↩︎"
  },
  {
    "objectID": "week2.html#communication",
    "href": "week2.html#communication",
    "title": "2  Week 2",
    "section": "2.1 Communication",
    "text": "2.1 Communication\nSkim Chapter 29 in R for Data Science (Wickham et al. 2023).1\nYou will then need to do the following:\n\nInstall Zotero in your computer if you haven’t already.\nRead Citations from Zotero and stop reading when you get to “Group Libraries.” This is 3 paragraphs.\nGo to this repository and download any CSL file you want—e.g., I am using the this one. Save the file in your Project folder.\nInstall the following packages:\n\n\nCode\ninstall.packages(\"modelsummary\")\ninstall.packages(\"flextable\")\ninstall.packages(\"tinytex\")\n\n\nYou should also type this into the console after installing tinytex:\n\n\nCode\ntinytex::install_tinytex()\n\n\n\n\n\n\n\n\n\nExercise\nDownload this .qmd file and put it into your project folder.\n\nChange Figure 1 (the cat picture) to an image of your liking. Adjust the caption accordingly.\nChange Equation 1 to a different equation.\nHint: If your not familiar with writing these sorts of equations, you can ask ChatGPT to generate the latex code for a different equation—e.g., “the normal distribution.”\nChange the “citations paragraph” at the end to so that it corresponds to yourself—i.e., different name, different citations.\nEdit the YAML file so that it includes your name (name), the appropriate date (date), a different font (mainfont), and whatever csl file you decided to go with (csl).\nToggle between the Source and Visual editors and try to understand what is going on.\nCreate a pdf and an html file.\nHint: This is how my .pdf and .html files look like.\nUpload the .qmd, .pdf, and .html files to your github repository."
  },
  {
    "objectID": "week2.html#data-wrangling",
    "href": "week2.html#data-wrangling",
    "title": "2  Week 2",
    "section": "2.2 Data Wrangling",
    "text": "2.2 Data Wrangling\n\n\n\n\n\n\nMultiple Exercises\nRead Chapter 4 of R4DS (Wickham et al. 2023) and complete the following exercises:\n\n4.2.5: all six exercises\n4.3.5: all seven exercises\n4.5.7: all six exercises\n\nAnswer these exercises in a quarto document and upload both .qmd and .html files to your github repository, just as you did for last week’s homework.\n\n\n\nYes, this means that for this week you will upload five different files to your github repository!\n\n\n\n\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "week2.html#footnotes",
    "href": "week2.html#footnotes",
    "title": "2  Week 2",
    "section": "",
    "text": "Some additional resources to skim:\n\nhttps://quarto.org/docs/get-started/authoring/rstudio.html\nhttps://quarto.org/docs/visual-editor/technical.html\n\n↩︎"
  },
  {
    "objectID": "sec2.html",
    "href": "sec2.html",
    "title": "Data, sampling, and probability",
    "section": "",
    "text": "That time we learned a little bit about probability and simulation. This stuff is important, we can’t really understand statistics without probability, as hinted at in Figure 1.\n\n\n\nFigure 1: Source: Wasserman (2004)\n\n\n\n\n\n\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in Statistical Inference. Springer."
  },
  {
    "objectID": "week3.html#ggplot2",
    "href": "week3.html#ggplot2",
    "title": "3  Week 3",
    "section": "3.1 ggplot2",
    "text": "3.1 ggplot2\n\n3.1.1 Introduction\nWe will look at the basic ggplot2 use using the faithful dataset, giving information on the eruption pattern of the Old Faithful geyser in Yellowstone National Park.\n\n\nCode\nlibrary(tidyverse)  ## ggplot2 is part of the tidyverse\ndata(\"faithful\")    ## this creates a copy of `faithful` in your global environment.\n\n# Basic scatterplot\nggplot(\n  data = faithful, \n  mapping = aes(x = eruptions, y = waiting)\n  ) + \n  geom_point()\n\n\n\n\n\nCode\n# Data and mapping can be given both as global (in ggplot()) or per layer\nggplot() + \n  geom_point(mapping = aes(x = eruptions, y = waiting), data = faithful)\n\n\n\n\n\nIf an aesthetic is linked to data it is put into aes()\n\n\nCode\nfaithful |&gt; \n  ggplot() + \n  geom_point(aes(x = eruptions, y = waiting, color = eruptions &lt; 3))\n\n\n\n\n\nIf you simple want to set it to a value, put it outside of aes()\n\n\nCode\nggplot(faithful) + \n  geom_point(aes(x = eruptions, y = waiting),\n             color = 'steelblue')\n\n\n\n\n\nSome geoms only need a single mapping and will calculate the rest for you—e.g., histograms and boxplots.\n\n\nCode\nggplot(faithful) + \n  geom_histogram(aes(x = eruptions))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFinally, geoms are drawn in the order they are added. The point layer is thus drawn on top of the density contours in the example below:\n\n\nCode\nggplot(faithful, aes(x = eruptions, y = waiting)) + \n  geom_density_2d() + \n  geom_point()\n\n\n\n\n\n\n3.1.1.1 Exercise\n\n\n\n\n\n\nModify the code below to make the points larger squares and slightly transparent. See ?geom_point for more information on the point layer.\n\n\n\n\n\nCode\nggplot(faithful) + \n  geom_point(aes(x = eruptions, y = waiting))\n\n\n\n\n\n\n\n\nHint 1: transparency is controlled with alpha, and shape with shape\nHint 2: remember the difference between mapping and setting aesthetics\nHint 3: the shape argument can also be controlled via the following numeric values\n\n\n\n\n\nNote that shapes 21 to 25 can be assigned color (for the stroke) and fill values (shown above as pink).\n\n\n\n\n\n3.1.1.2 Exercise\n\n\n\n\n\n\nColor the two visible clusters in the histogram with different colors.\n\n\n\n\n\nCode\nggplot(faithful) + \n  geom_histogram(aes(x = eruptions))\n\n\n\n\n\n\n\n\nHint 1: For polygons you can map two different color-like aesthetics: color (the color of the stroke) and fill (the fill color)\n\n\n\n\n\n3.1.1.3 Exercise\n\n\n\n\n\n\nAdd a line that separates the two point distributions. See ?geom_abline for how to draw straight lines from a slope and intercept.\n\n\n\n\n\nCode\nggplot(faithful) + \n  geom_point(aes(x = eruptions, y = waiting))\n\n\n\n\n\n3.1.2 The “Statistics” Layer\nWe will use the mpg dataset giving information about fuel economy on different car models.\nEvery geom has a stat. This is why new data (count) can appear when using geom_bar().\n\n\nCode\ndata(\"mpg\")  ## this dataset lives in the ggplot2 package\n\nmpg |&gt; \n  ggplot(aes(x = class)) + \n  geom_bar()\n\n\n\n\n\nThe stat can be overwritten. If we have pre-computed count we don’t want any additional computations to perform and we use the identity stat to leave the data alone.\n\n\nCode\nmpg_counted &lt;- mpg |&gt; \n  group_by(class) |&gt; \n  summarize(count = n())\n  \nmpg_counted |&gt; \n  ggplot() + \n  geom_bar(aes(x = class, y = count), stat = 'identity')\n\n\n\n\n\nMost obvious “geom” + “stat” combinations have a dedicated geom constructor. For example, the one above is available directly as geom_col().\n\n\nCode\nggplot(mpg_counted) + ## `mpg_counted` is a summarized version of `mpg`\n  geom_col(aes(x = class, y = count))\n\n\n\n\n\n\n\n\n\n\n\nTypical geoms that rely heavily on “statistics layers” are geom_boxplot(), geom_density(), geom_smooth(), and geom_jitter().\n\n\n\nThis is the most confusing aspect about ggplot2. Thomas Pederson says we should think about it as a “data transformation” pipeline that sits in between the input data and the geom we want to use. Don’t worry about it for now! We will skip the exercises here!\n\n\n3.1.3 Scales\nScales define how the mapping you specify inside aes() should happen. All mappings have an associated scale even if not specified explicitly.\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class))\n\n\n\n\n\nWe can take control by adding one explicitly. All scales follow the same naming conventions—i.e., scale_&lt;aes&gt;_&lt;type&gt;.\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class)) + \n  scale_color_brewer(type = 'qual')\n\n\n\n\n\nPositional mappings (x and y) also have associated scales.\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  scale_x_continuous(breaks = c(3, 5, 6)) + \n  scale_y_log10()\n\n\n\n\n\n\n3.1.3.1 Exercise\n\n\n\n\n\n\nUse RColorBrewer::display.brewer.all() to see all the different palettes from Color Brewer and pick your favorite. Modify the code below to use it.\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class)) + \n  scale_color_brewer(type = 'qual')\n\n\n\n\n3.1.3.2 Exercise\n\n\n\n\n\n\nModify the code below to create a bubble chart (scatterplot with size mapped to a continuous variable) showing cyl with size. Make sure that only the present amount of cylinders (4, 5, 6, and 8) are present in the legend.\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class)) + \n  scale_color_brewer(type = 'qual')\n\n\n\n\n\n\n\n\nHint: The breaks argument in the scale is used to control which values are present in the legend.\n\n\n\n\n\n3.1.3.3 Exercise\n\n\n\n\n\n\nModify the code below so that color is no longer mapped to the discrete class variable, but to the continuous cty variable. What happens to the guide?\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class, size = cty))\n\n\n\n\n\n\n\n\nThe type of guide can be controlled with the guide argument in the scale, or with the guides() function. Continuous colors have a gradient color bar by default, but setting it to legend will turn it back to the standard look. What happens when multiple aesthetics are mapped to the same variable and uses the guide type?\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = cty, size = cty))\n\n\n\n\n\n3.1.4 Facets\nThe facet defines how data is split among panels. The default facet (facet_null()) puts all the data in a single panel, while facet_wrap() and facet_grid() allows you to specify different types of small multiples.\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_wrap(~ class)\n\n\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_grid(year ~ drv)\n\n\n\n\n\n\n3.1.4.1 Exercise\n\n\n\n\n\n\nOne of the great things about facets is that they share the axes between the different panels. Sometimes this is undesirable though, and the behavior can be changed with the scales argument. Experiment with the different possible settings in the plot below:\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_wrap(~ drv)\n\n\n\n\n\n3.1.5 Theme\nTheming defines the feel and look of your final visualization and is something you will normally defer to the final polishing of the plot. It is very easy to change looks with a pre-built theme\n\n\nCode\nggplot(mpg) + \n  geom_bar(aes(y = class)) + \n  facet_wrap(~year) + \n  theme_minimal()\n\n\n\n\n\nFurther adjustments can be done in the end to get exactly the look you want\n\n\nCode\nggplot(mpg) + \n  geom_bar(aes(y = class)) + \n  facet_wrap(~year) + \n  labs(title = \"Number of car models per class\",\n       caption = \"source: http://fueleconomy.gov\",\n       x = NULL,\n       y = NULL) +\n  scale_x_continuous(expand = c(0, NA)) + \n  theme_minimal() + \n  theme(\n    text = element_text('Avenir Next Condensed'),\n    strip.text = element_text(face = 'bold', hjust = 0),\n    plot.caption = element_text(face = 'italic'),\n    panel.grid.major = element_line('white', linewidth = 0.5),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.ontop = TRUE\n  )\n\n\n\n\n\n\n3.1.5.1 Exercise\n\n\n\n\n\n\nThemes can be overwhelming, especially as you often try to optimize for beauty while you learn. To remove the last part of the equation, the exercise is to take the plot given below and make it as hideous as possible using the theme function. Go absolutely crazy, but take note of the effect as you change different settings.\n\n\n\n\n\nCode\nmpg |&gt; \n  ggplot(aes(y = class, fill = drv)) + \n  geom_bar() + \n  facet_wrap(~year) + \n  labs(\n    title = \"Number of car models per class\",\n    caption = \"source: http://fueleconomy.gov\",\n    x = 'Number of cars',\n    y = NULL\n  )"
  },
  {
    "objectID": "week3.html#simulation",
    "href": "week3.html#simulation",
    "title": "3  Week 3",
    "section": "3.2 Simulation",
    "text": "3.2 Simulation\n\n3.2.1 Voting Poll Example\nThere are many correct ways of simulating data.\nThis exercise takes off right were Steve left off (here). I have modified this code somewhat so that it’s easier to use for visualization—i.e., I created a function called simulation_votes(). We may eventually learn more of this, but for the moment let me explain how this function works.\nFirst, save the function to your global environment with the source() function.\n\n\nCode\nurl &lt;- \"https://raw.githubusercontent.com/acastroaraujo/socStats/main/simulation_function_week3.R\"\nsource(url)\n\n\nSecond, choose three parameters for the simulation:\n\ndem_prob_pop: the probability that the person will vote for “Democrats” in the population\nsample_size: the number of people in each “poll” (or sample)\nnum_sims: number of simulations\n\nThird, inspect the simulation data set.\n\n\nCode\nsims &lt;- simulation_votes(dem_prob_pop = 0.75, sample_size = 90, num_sims = 1e3)\nsims\n\n\n# A tibble: 90,000 × 5\n      id vote  dem_prob_pop sample_size num_sims\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;   \n 1     1 Rep   0.75         90          1000    \n 2     1 Dem   0.75         90          1000    \n 3     1 Dem   0.75         90          1000    \n 4     1 Dem   0.75         90          1000    \n 5     1 Dem   0.75         90          1000    \n 6     1 Dem   0.75         90          1000    \n 7     1 Dem   0.75         90          1000    \n 8     1 Rep   0.75         90          1000    \n 9     1 Dem   0.75         90          1000    \n10     1 Dem   0.75         90          1000    \n# ℹ 89,990 more rows\n\n\nThe following chunk of code replicates what we did in class:\n\n\nCode\n## First I'll set up the ggplot2 theme I personally like best.\n## You might not have this font if you are on a Windows computer\ntheme_set(theme_light(base_family = \"Avenir Next Condensed\")) \n\nsims &lt;- simulation_votes(dem_prob_pop = 0.52, sample_size = 300, num_sims = 500)\n\nresults &lt;- sims |&gt; \n  group_by(id) |&gt; \n  summarize(dem_prop = mean(vote == \"Dem\"))\n\nresults\n\n\n# A tibble: 500 × 2\n      id dem_prop\n   &lt;int&gt;    &lt;dbl&gt;\n 1     1    0.507\n 2     2    0.533\n 3     3    0.49 \n 4     4    0.557\n 5     5    0.54 \n 6     6    0.49 \n 7     7    0.487\n 8     8    0.527\n 9     9    0.523\n10    10    0.547\n# ℹ 490 more rows\n\n\nCode\n# plot the results\n\nresults |&gt; \n  ggplot(aes(x = dem_prop)) +\n  geom_histogram(color = \"white\", boundary = .5, binwidth = .01) +\n  labs(title = \"Simulation\", subtitle = \"dem_prob = 0.52, sample_size = 300, num_sim = 500\")\n\n\n\n\n\nCode\n# how often does the poll predict the winner?\nmean(results$dem_prop &gt; 0.5)\n\n\n[1] 0.764\n\n\nCode\n# shade the same plot\nresults &lt;- results |&gt; \n  mutate(winner = if_else(dem_prop &gt; 0.5, \"Dem\", \"Rep\"))\n\nresults |&gt; \n  ggplot(aes(x = dem_prop, fill = winner)) +\n  geom_histogram(color = \"white\", boundary = .5, binwidth = .01) +\n  scale_fill_brewer(palette = \"Set1\", direction = -1)\n\n\n\n\n\nCode\n# strip plot\nresults |&gt; \n  ggplot(aes(dem_prop, \"\")) +\n  geom_boxplot(outlier.shape = NA) + \n  geom_jitter(height = 1/5, alpha = 0.2)\n\n\n\n\n\nCode\n# density plot\nresults |&gt; \n  ggplot(aes(dem_prop)) + \n  geom_density(fill = \"grey90\") + \n  geom_vline(xintercept = 0.5, linetype = \"dashed\")\n\n\n\n\n\n\n3.2.1.1 Exercise\n\n\n\n\n\n\nIn the simulation above, what is the average dem_prop? What is the standard deviation of dem_prop? How does this change for different values of sample_size?\n\n\n\n\n\n3.2.1.2 Exercise\n\n\n\n\n\n\nCreate five different simulations with different values of sample_size (e.g., 50, 200, 500, 1000, 2000). Put them together into a single dataset and then visualize the results using boxplots. What is going on?\n\n\n\n\n\n\n\n\n\nHint: You can stack together different datasets using the bind_rows() function in the dplyr package.\nHint: You will have to group_by(id, sample_size) to calculate dem_prop.\n\n\n\n\n\n3.2.1.3 Exercise\n\n\n\n\n\n\nCreate five different simulations with different values of dem_prob_pop (e.g., 0.49, 0.52, 0.55, 0.58). Put them together into a single dataset and then visualize the results using boxplots. What is going on?\n\n\n\n\n\n\n\n\n\nHealy, Kieran. 2018. Data Visualization: A Practical Introduction.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "week4.html#exercise",
    "href": "week4.html#exercise",
    "title": "4  Week 4",
    "section": "4.1 Exercise",
    "text": "4.1 Exercise\nSo far, we have generated data using the sample() and rbinom() functions. R has a built-in function for generating data from normal distributions, it’s called rnorm().\nRecall that all normal distributions exhibit the following behavior:\n\nThe probability that \\(x\\) is within one standard deviation \\(\\sigma\\) away from the mean is roughly 68%.\nThe probability that \\(x\\) is within two standard deviations \\(2\\sigma\\) away from the mean is roughly 95%.\nThe probability that \\(x\\) is above the mean ( \\(x \\geq \\mu\\) ) is 50%.\nThe probability that \\(x\\) is below the mean ( \\(x \\leq \\mu\\) ) is also 50%.\n\nThe following chunk generates 100,000 observations from a normal distribution with mean = 0 and standard deviation = 1.\n\n\nCode\nx &lt;- rnorm(100000, mean = 0, sd = 1)\n\n\n\n\n\n\n\n\nUse the mean() function to verify that x exhibits those four behaviors.\n\n\n\n\n\n\n\n\n\nHint: Remember that you can coerce numeric variables into TRUE and FALSE values using logical operators (==, &gt;, &lt;, etc.)"
  },
  {
    "objectID": "week4.html#exercise-1",
    "href": "week4.html#exercise-1",
    "title": "4  Week 4",
    "section": "4.2 Exercise",
    "text": "4.2 Exercise\n\n\n\n\n\n\nUse the quantile() function on the x created in the previous exercise. Explain the results."
  },
  {
    "objectID": "week4.html#exercise-2",
    "href": "week4.html#exercise-2",
    "title": "4  Week 4",
    "section": "4.3 Exercise",
    "text": "4.3 Exercise\n\n\n\n\n\n\nNow modify the probs argument in the quantile() to find the 0.5% and 99.5% percentiles of x.\n\n\n\n\n\n\n\n\n\nHint: If you Google “99 percent confidence interval” you should see a table that foreshadows the answer—i.e., you should get some number roughly equal to -2.576 and 2.576 respectively."
  },
  {
    "objectID": "week4.html#exercise-3",
    "href": "week4.html#exercise-3",
    "title": "4  Week 4",
    "section": "4.4 Exercise",
    "text": "4.4 Exercise\n\n\n\n\n\n\nUse the mean() function to verify that the probability of \\(x\\) being between \\(-2.576\\) and \\(2.576\\) is roughly 99%.\n\n\n\n\n\n\n\n\n\nHint: The logical operator for “AND” is &."
  },
  {
    "objectID": "week4.html#exercise-4",
    "href": "week4.html#exercise-4",
    "title": "4  Week 4",
    "section": "4.5 Exercise",
    "text": "4.5 Exercise\nCentral Limit Theorem\n\n\n\n\n\n\nLet \\(x = x_1 + \\dots + x_{20}\\), the sum of 20 independent uniform(0, 1) random variables. In R, create 1000 simulations of \\(x\\) and plot their histogram.\n\n\n\n\n\n\n\n\n\nHint: You can simulate the sum of 20 independent uniform random variables using the following code:\n\n\nCode\nsum(runif(n = 20, min = 0, max = 1))\n\n\n[1] 10.72799\n\n\nThe simulation you create will have 1000 instances of a value like this.\n\n\n\nThe sampling distribution is the set of possible sample “statistics” or “data summaries” (e.g., means) that could have been observed, if the data collection process had been repeated many many times. There’s an important theorem in statistics called the Central Limit Theorem (CLT) that proves that this sampling distribution converges to a normal distribution when the sample size is big enough, regardless of how our data looks like.\nThe standard error is how we estimate of the the standard deviation of the sampling distribution.\n\n\n\n\n\n\nBonus:\nUse the sd() function on the sampling distribution simulated earlier, then compare this value to the standard error given by the formula for the standard error of a sum of 20 uniform(0, 1) random variables:\n\\[\n\\text{se}(x) = \\sqrt{\\frac{20}{12}}\n\\]\nSee that? We can calculate the standard errors of different “data summaries” without having to think too much about the math.1"
  },
  {
    "objectID": "week4.html#exercise-5",
    "href": "week4.html#exercise-5",
    "title": "4  Week 4",
    "section": "4.6 Exercise",
    "text": "4.6 Exercise\nSimulation study of CLT\nMany introductory statistics textbooks say that “sample sizes equal to or greater than 30 are often considered sufficient for the CLT to hold.”\n\n\n\n\n\n\nWrite down intuitively what you think this means.\n\n\n\nNow lets revisit last class’ simulation exercise.\nhttps://github.com/vaiseys/soc-stats-1/blob/main/demos/day-08.R\nYou’ll notice that the sample size is controlled by the following object:\n\n\nCode\nsvy_size &lt;- 2247  # number of people in each \"poll\"\n\n\nAnd the “true” population proportion is given by:\n\n\nCode\nest_prop &lt;- 1/3\n\n\nIn class, we also calculated the standard error “analytically” using the following mathematical equation:\n\\[\n\\text{se}_x = \\sqrt{\\frac{p (1-p)}{n}}\n\\]\nOr in code:\n\n\nCode\nstd_error &lt;- sqrt((1/3) * (2/3) / svy_size)\nstd_error\n\n\n[1] 0.009944712\n\n\nThis means that the 95% confidence interval should be roughly 2 standard errors away from 0.33\n\n\nCode\n0.33 - 1.96*std_error ## \"1.96\" is roughly \"2\"!\n\n\n[1] 0.3105084\n\n\nCode\n0.33 + 1.96*std_error\n\n\n[1] 0.3494916\n\n\nIn Steve’s simulation, this is given by ci95.\n\n\n\n\n\n\nRepeat Steve’s simulation with different values of svy_size. Is the 95% confidence interval still roughly 2 standard errors away from 0.33?\nDo these results change your initial interpretation of the idea that “sample sizes equal to or greater than 30 are often considered sufficient for the CLT to hold” ?"
  },
  {
    "objectID": "week4.html#footnotes",
    "href": "week4.html#footnotes",
    "title": "4  Week 4",
    "section": "",
    "text": "We can estimate the standard error for one sample using sd(runif(20)) * sqrt(20). The answer should be close to the “true” value (1.29), but it shouldn’t match.↩︎"
  },
  {
    "objectID": "solutions-01.html#data-structures",
    "href": "solutions-01.html#data-structures",
    "title": "5  Solutions 1",
    "section": "5.1 Data Structures",
    "text": "5.1 Data Structures\n\n5.1.1 Exercise\nmtcars is a data frame. You can verify this by looking at its class:\n\n\nCode\nclass(mtcars)\n\n\n[1] \"data.frame\"\n\n\nA “data frame” is basically a list of atomic vectors each of which have the same length()—i.e., the number of rows in the dataset. This is why typeof(mtcars) produces “list.”\n\n\nCode\ntypeof(mtcars)\n\n\n[1] \"list\"\n\n\n\n\nCode\nnrow(mtcars) ## number of observations\n\n\n[1] 32\n\n\nCode\nncol(mtcars) ## number of columns\n\n\n[1] 11\n\n\nCode\nlength(mtcars) ## not number of observations, but number of columns\n\n\n[1] 11\n\n\nCode\ndim(mtcars) ## number of rows and columns\n\n\n[1] 32 11\n\n\nCode\nrownames(mtcars)\n\n\n [1] \"Mazda RX4\"           \"Mazda RX4 Wag\"       \"Datsun 710\"         \n [4] \"Hornet 4 Drive\"      \"Hornet Sportabout\"   \"Valiant\"            \n [7] \"Duster 360\"          \"Merc 240D\"           \"Merc 230\"           \n[10] \"Merc 280\"            \"Merc 280C\"           \"Merc 450SE\"         \n[13] \"Merc 450SL\"          \"Merc 450SLC\"         \"Cadillac Fleetwood\" \n[16] \"Lincoln Continental\" \"Chrysler Imperial\"   \"Fiat 128\"           \n[19] \"Honda Civic\"         \"Toyota Corolla\"      \"Toyota Corona\"      \n[22] \"Dodge Challenger\"    \"AMC Javelin\"         \"Camaro Z28\"         \n[25] \"Pontiac Firebird\"    \"Fiat X1-9\"           \"Porsche 914-2\"      \n[28] \"Lotus Europa\"        \"Ford Pantera L\"      \"Ferrari Dino\"       \n[31] \"Maserati Bora\"       \"Volvo 142E\"         \n\n\nCode\ncolnames(mtcars)\n\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\n\n\n5.1.2 Exercise\n\n\nCode\nT &lt;- 123\nT\n\n\n[1] 123\n\n\nCode\nTRUE &lt;- 123\n\n\nError in TRUE &lt;- 123: invalid (do_set) left-hand side to assignment\n\n\nYou can assign any value to T, but R won’t let you modify TRUE because it is a “reserved keyword.” Other reserved keywords include: function, for, FALSE, Inf, NULL, NA, among others.\n\n\n5.1.3 Exercise\nTest your knowledge of the vector coercion rules by predicting the output of the following uses of c():\n\n\nCode\nc(1, FALSE) ## numeric (or \"double\")\n\n\n[1] 1 0\n\n\nCode\nc(\"a\", 1) ## character\n\n\n[1] \"a\" \"1\"\n\n\nCode\nc(TRUE, 1L) ## numeric (or \"integer\")\n\n\n[1] 1 1\n\n\nYup.\n\n\n5.1.4 Exercise\nas.integer() coerces TRUE to 1 and FALSE to 0.\n\n\nCode\nas.integer(c(TRUE, FALSE))\n\n\n[1] 1 0\n\n\nThis is the most common (and useful) form of coercion you’ll see.\n\n\n5.1.5 Exercise\nImplicit coercion of logicals to numericals.\n\n\nCode\nx &lt;- sample(c(TRUE, FALSE), size = 75, replace = TRUE)\nsum(x)  ## number of TRUE values\n\n\n[1] 39\n\n\nCode\nmean(x) ## proportion of TRUE values\n\n\n[1] 0.52\n\n\nCode\nsum(x) / length(x) ## verifying the mean function\n\n\n[1] 0.52\n\n\n\n\n5.1.6 Exercise\nThe difference between mtcars[\"mpg\"] and mtcars[[\"mpg\"]] is that the first selects a column in a data frame, while the second extracts the column. mtcars[[\"mpg\"]] is equivalent to mtcars$mpg.\n\n\n5.1.7 Exercise\nletters is a built-in object in R that contains the 26 letters of English alphabet.\nUsing the [ operator, do the following:\nExtract the 17th value of letters\n\n\nCode\nletters[17]\n\n\n[1] \"q\"\n\n\nCreate a sequence of even numbers from 2 to 26 and use that to subset letters\n\n\nCode\nletters[seq(2, 26, by = 2)]\n\n\n [1] \"b\" \"d\" \"f\" \"h\" \"j\" \"l\" \"n\" \"p\" \"r\" \"t\" \"v\" \"x\" \"z\"\n\n\nUse 8:12 to subset letters.\n\n\nCode\nletters[8:12]\n\n\n[1] \"h\" \"i\" \"j\" \"k\" \"l\"\n\n\n\n\n5.1.8 Exercise\n\n\nCode\nletters[18] &lt;- NA\nletters\n\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" NA  \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n\n\n\n5.1.9 Exercise\nSubset mtcars so that we only see the observations for which cyl == 4.\n\n\nCode\nmtcars[mtcars$cyl == 4, ]\n\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nSubset mtcars so that we only see the observations for which mpg is greater than 23.\n\n\nCode\nmtcars[mtcars$mpg &gt; 23, ]\n\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2"
  },
  {
    "objectID": "solutions-01.html#search",
    "href": "solutions-01.html#search",
    "title": "5  Solutions 1",
    "section": "5.2 Search",
    "text": "5.2 Search\n\n5.2.1 Exercise\n\nUsing what I told you earlier about the search() function, explain why you get two different errors. What is going on? What is R doing when you type table(year)? (You might want to type search() into the console again). In what package does R find the year object?\n\nFirst, R searched for an object called year and didn’t find anyting.\nSecond, R searched for year and found a function with the same name in the lubridate package."
  },
  {
    "objectID": "solutions-01.html#dplyr",
    "href": "solutions-01.html#dplyr",
    "title": "5  Solutions 1",
    "section": "5.3 dplyr",
    "text": "5.3 dplyr\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(palmerpenguins)\n\n\n\n5.3.1 Exercise\n\n\nCode\ni &lt;- seq(2, nrow(penguins), by = 2)\n\npenguins |&gt; \n  slice(i)\n\n\n# A tibble: 172 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.5          17.4               186        3800\n 2 Adelie  Torgersen           NA            NA                  NA          NA\n 3 Adelie  Torgersen           39.3          20.6               190        3650\n 4 Adelie  Torgersen           39.2          19.6               195        4675\n 5 Adelie  Torgersen           42            20.2               190        4250\n 6 Adelie  Torgersen           37.8          17.3               180        3700\n 7 Adelie  Torgersen           38.6          21.2               191        3800\n 8 Adelie  Torgersen           36.6          17.8               185        3700\n 9 Adelie  Torgersen           42.5          20.7               197        4500\n10 Adelie  Torgersen           46            21.5               194        4200\n# ℹ 162 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nCode\npenguins |&gt; \n  slice(seq(3, nrow(penguins), by = 3))\n\n\n# A tibble: 114 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           40.3          18                 195        3250\n 2 Adelie  Torgersen           39.3          20.6               190        3650\n 3 Adelie  Torgersen           34.1          18.1               193        3475\n 4 Adelie  Torgersen           37.8          17.3               180        3700\n 5 Adelie  Torgersen           34.6          21.1               198        4400\n 6 Adelie  Torgersen           42.5          20.7               197        4500\n 7 Adelie  Biscoe              37.8          18.3               174        3400\n 8 Adelie  Biscoe              38.2          18.1               185        3950\n 9 Adelie  Biscoe              40.6          18.6               183        3550\n10 Adelie  Biscoe              40.5          18.9               180        3950\n# ℹ 104 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n5.3.2 Exercise\n\n\nCode\npenguins |&gt; \n  filter(species == \"Gentoo\", island == \"Biscoe\", body_mass_g &gt;= 5000, body_mass_g &lt;= 5500)\n\n\n# A tibble: 39 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           47.6          14.5               215        5400\n 2 Gentoo  Biscoe           46.7          15.3               219        5200\n 3 Gentoo  Biscoe           46.8          15.4               215        5150\n 4 Gentoo  Biscoe           48.7          15.1               222        5350\n 5 Gentoo  Biscoe           45.1          14.5               215        5000\n 6 Gentoo  Biscoe           46.3          15.8               215        5050\n 7 Gentoo  Biscoe           42.9          13.1               215        5000\n 8 Gentoo  Biscoe           46.1          15.1               215        5100\n 9 Gentoo  Biscoe           47.3          15.3               222        5250\n10 Gentoo  Biscoe           45.1          14.5               207        5050\n# ℹ 29 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "solutions-02.html#rows",
    "href": "solutions-02.html#rows",
    "title": "6  Solutions 2",
    "section": "6.1 Rows",
    "text": "6.1 Rows\n\n6.1.1 4.2.5.1\n\nIn a single pipeline for each condition, find all flights that meet the condition:\n\nHad an arrival delay of two or more hours\n\n\nCode\nflights |&gt; \n  filter(arr_delay &gt;= 120)\n\n\n# A tibble: 10,200 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      811            630       101     1047            830\n 2  2013     1     1      848           1835       853     1001           1950\n 3  2013     1     1      957            733       144     1056            853\n 4  2013     1     1     1114            900       134     1447           1222\n 5  2013     1     1     1505           1310       115     1638           1431\n 6  2013     1     1     1525           1340       105     1831           1626\n 7  2013     1     1     1549           1445        64     1912           1656\n 8  2013     1     1     1558           1359       119     1718           1515\n 9  2013     1     1     1732           1630        62     2028           1825\n10  2013     1     1     1803           1620       103     2008           1750\n# ℹ 10,190 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nFlew to Houston (IAH or HOU)\n\n\nCode\nflights |&gt; \n  filter(dest == \"IAH\" | dest == \"HOU\")\n\n\n# A tibble: 9,313 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      623            627        -4      933            932\n 4  2013     1     1      728            732        -4     1041           1038\n 5  2013     1     1      739            739         0     1104           1038\n 6  2013     1     1      908            908         0     1228           1219\n 7  2013     1     1     1028           1026         2     1350           1339\n 8  2013     1     1     1044           1045        -1     1352           1351\n 9  2013     1     1     1114            900       134     1447           1222\n10  2013     1     1     1205           1200         5     1503           1505\n# ℹ 9,303 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nWere operated by United, American, or Delta\n\n\nCode\nflights |&gt; \n  ## hint: check the nycflights13::airlines dataset\n  filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\n\n\n# A tibble: 139,504 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      554            600        -6      812            837\n 5  2013     1     1      554            558        -4      740            728\n 6  2013     1     1      558            600        -2      753            745\n 7  2013     1     1      558            600        -2      924            917\n 8  2013     1     1      558            600        -2      923            937\n 9  2013     1     1      559            600        -1      941            910\n10  2013     1     1      559            600        -1      854            902\n# ℹ 139,494 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nDeparted in summer (July, August, and September)\n\n\nCode\nflights |&gt; \n  filter(month %in% 7:9)\n\n\n# A tibble: 86,326 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     7     1        1           2029       212      236           2359\n 2  2013     7     1        2           2359         3      344            344\n 3  2013     7     1       29           2245       104      151              1\n 4  2013     7     1       43           2130       193      322             14\n 5  2013     7     1       44           2150       174      300            100\n 6  2013     7     1       46           2051       235      304           2358\n 7  2013     7     1       48           2001       287      308           2305\n 8  2013     7     1       58           2155       183      335             43\n 9  2013     7     1      100           2146       194      327             30\n10  2013     7     1      100           2245       135      337            135\n# ℹ 86,316 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nArrived more than two hours late, but didn’t leave late\n\n\nCode\nflights |&gt; \n  ## I will also accept dep_delay &lt;= 0\n  filter(arr_delay &gt; 120 & dep_delay &lt;= 5)\n\n\n# A tibble: 36 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1    27     1419           1420        -1     1754           1550\n 2  2013    10     7     1350           1350         0     1736           1526\n 3  2013    10     7     1357           1359        -2     1858           1654\n 4  2013    10    16      657            700        -3     1258           1056\n 5  2013    11     1      658            700        -2     1329           1015\n 6  2013     3     8     1246           1245         1     1552           1350\n 7  2013     3    18     1844           1847        -3       39           2219\n 8  2013     4    17     1635           1640        -5     2049           1845\n 9  2013     4    18      558            600        -2     1149            850\n10  2013     4    18      655            700        -5     1213            950\n# ℹ 26 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\n\nCode\nflights |&gt; \n  # this also works:\n  # filter(dep_delay &gt;= 60 & dep_delay - arr_delay &gt; 30)\n  filter(dep_delay &gt;= 60, dep_delay - arr_delay &gt; 30)\n\n\n# A tibble: 1,844 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1     2205           1720       285       46           2040\n 2  2013     1     1     2326           2130       116      131             18\n 3  2013     1     3     1503           1221       162     1803           1555\n 4  2013     1     3     1839           1700        99     2056           1950\n 5  2013     1     3     1850           1745        65     2148           2120\n 6  2013     1     3     1941           1759       102     2246           2139\n 7  2013     1     3     1950           1845        65     2228           2227\n 8  2013     1     3     2015           1915        60     2135           2111\n 9  2013     1     3     2257           2000       177       45           2224\n10  2013     1     4     1917           1700       137     2135           1950\n# ℹ 1,834 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n6.1.2 4.2.5.2\n\nSort flights to find the flights with longest departure delays. Find the flights that left earliest in the morning.\n\n\n\nCode\nflights |&gt; \n  arrange(desc(dep_delay))\n\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nCode\nflights |&gt; \n  arrange(dep_time)\n\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1    13        1           2249        72      108           2357\n 2  2013     1    31        1           2100       181      124           2225\n 3  2013    11    13        1           2359         2      442            440\n 4  2013    12    16        1           2359         2      447            437\n 5  2013    12    20        1           2359         2      430            440\n 6  2013    12    26        1           2359         2      437            440\n 7  2013    12    30        1           2359         2      441            437\n 8  2013     2    11        1           2100       181      111           2225\n 9  2013     2    24        1           2245        76      121           2354\n10  2013     3     8        1           2355         6      431            440\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n6.1.3 4.2.5.3\n\nSort flights to find the fastest flights. (Hint: Try including a math calculation inside of your function.)\n\nThe fastest flights will travel the longest distances in the least amount of time. Thus, I will have to calculate km/h. (Miles per hour is fine too I guess).\n\n\nCode\nto_kmh &lt;- function(distance, minutes) {\n  (distance * 1.60934) / (minutes / 60)\n}\n\nflights |&gt; \n  mutate(speed = to_kmh(distance, air_time)) |&gt;\n  relocate(speed) |&gt; \n  arrange(desc(speed))\n\n\n# A tibble: 336,776 × 20\n   speed  year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1 1132.  2013     5    25     1709           1700         9     1923\n 2 1047.  2013     7     2     1558           1513        45     1745\n 3 1043.  2013     5    13     2040           2025        15     2225\n 4 1032.  2013     3    23     1914           1910         4     2045\n 5  952.  2013     1    12     1559           1600        -1     1849\n 6  908.  2013    11    17      650            655        -5     1059\n 7  897.  2013     2    21     2355           2358        -3      412\n 8  896.  2013    11    17      759            800        -1     1212\n 9  892.  2013    11    16     2003           1925        38       17\n10  892.  2013    11    16     2349           2359       -10      402\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n6.1.4 4.2.5.4\n\nWas there a flight on every day of 2013?\n\n\n\nCode\nflights |&gt; \n  distinct(month, day) |&gt; \n  summarize(num_days = n())\n\n\n# A tibble: 1 × 1\n  num_days\n     &lt;int&gt;\n1      365\n\n\nYes!\n\n\n6.1.5 4.2.5.5\n\nWhich flights traveled the farthest distance? Which traveled the least distance?\n\n\n\nCode\nflights |&gt; \n  arrange(desc(distance))\n\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      857            900        -3     1516           1530\n 2  2013     1     2      909            900         9     1525           1530\n 3  2013     1     3      914            900        14     1504           1530\n 4  2013     1     4      900            900         0     1516           1530\n 5  2013     1     5      858            900        -2     1519           1530\n 6  2013     1     6     1019            900        79     1558           1530\n 7  2013     1     7     1042            900       102     1620           1530\n 8  2013     1     8      901            900         1     1504           1530\n 9  2013     1     9      641            900      1301     1242           1530\n10  2013     1    10      859            900        -1     1449           1530\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nCode\nflights |&gt; \n  arrange(distance)\n\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     7    27       NA            106        NA       NA            245\n 2  2013     1     3     2127           2129        -2     2222           2224\n 3  2013     1     4     1240           1200        40     1333           1306\n 4  2013     1     4     1829           1615       134     1937           1721\n 5  2013     1     4     2128           2129        -1     2218           2224\n 6  2013     1     5     1155           1200        -5     1241           1306\n 7  2013     1     6     2125           2129        -4     2224           2224\n 8  2013     1     7     2124           2129        -5     2212           2224\n 9  2013     1     8     2127           2130        -3     2304           2225\n10  2013     1     9     2126           2129        -3     2217           2224\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n6.1.6 4.2.5.6\n\nDoes it matter what order you used filter() and arrange() if you’re using both? Why/why not? Think about the results and how much work the functions would have to do.\n\nNo, it doesn’t matter. Filter does logical subsetting, and it doesn’t matter if we re-arrange the data before or after subsetting."
  },
  {
    "objectID": "solutions-02.html#columns",
    "href": "solutions-02.html#columns",
    "title": "6  Solutions 2",
    "section": "6.2 Columns",
    "text": "6.2 Columns\n\n6.2.1 4.3.5.1\n\nCompare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?\n\nsched_dep_time = dep_time - dep_delay\n\n\n6.2.2 4.3.5.2\n\nBrainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.\n\n\n\nCode\nflights |&gt; \n  select(dep_time, dep_delay, arr_time, arr_delay)\n\n\n# A tibble: 336,776 × 4\n   dep_time dep_delay arr_time arr_delay\n      &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1      517         2      830        11\n 2      533         4      850        20\n 3      542         2      923        33\n 4      544        -1     1004       -18\n 5      554        -6      812       -25\n 6      554        -4      740        12\n 7      555        -5      913        19\n 8      557        -3      709       -14\n 9      557        -3      838        -8\n10      558        -2      753         8\n# ℹ 336,766 more rows\n\n\nCode\nflights |&gt; \n  select(starts_with(\"dep_\"), starts_with(\"arr_\"))\n\n\n# A tibble: 336,776 × 4\n   dep_time dep_delay arr_time arr_delay\n      &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1      517         2      830        11\n 2      533         4      850        20\n 3      542         2      923        33\n 4      544        -1     1004       -18\n 5      554        -6      812       -25\n 6      554        -4      740        12\n 7      555        -5      913        19\n 8      557        -3      709       -14\n 9      557        -3      838        -8\n10      558        -2      753         8\n# ℹ 336,766 more rows\n\n\nCode\nflights |&gt; \n  select(dep_time:arr_delay) |&gt; \n  ## or ! instead of -\n  select(-c(2, 5))\n\n\n# A tibble: 336,776 × 4\n   dep_time dep_delay arr_time arr_delay\n      &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1      517         2      830        11\n 2      533         4      850        20\n 3      542         2      923        33\n 4      544        -1     1004       -18\n 5      554        -6      812       -25\n 6      554        -4      740        12\n 7      555        -5      913        19\n 8      557        -3      709       -14\n 9      557        -3      838        -8\n10      558        -2      753         8\n# ℹ 336,766 more rows\n\n\nCode\nflights |&gt; \n  select(matches(\"(^arr|^dep)_(time$|delay$)\"))\n\n\n# A tibble: 336,776 × 4\n   dep_time dep_delay arr_time arr_delay\n      &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1      517         2      830        11\n 2      533         4      850        20\n 3      542         2      923        33\n 4      544        -1     1004       -18\n 5      554        -6      812       -25\n 6      554        -4      740        12\n 7      555        -5      913        19\n 8      557        -3      709       -14\n 9      557        -3      838        -8\n10      558        -2      753         8\n# ℹ 336,766 more rows\n\n\n\n\n6.2.3 4.3.5.3\n\nWhat happens if you specify the name of the same variable multiple times in a select() call?\n\nYou only get the variable once!\n\n\nCode\nflights |&gt; \n  select(day, day, day)\n\n\n# A tibble: 336,776 × 1\n     day\n   &lt;int&gt;\n 1     1\n 2     1\n 3     1\n 4     1\n 5     1\n 6     1\n 7     1\n 8     1\n 9     1\n10     1\n# ℹ 336,766 more rows\n\n\nNote that this is different from “base” subsetting with character vectors.\n\n\nCode\nflights[, c(\"day\", \"day\", \"day\")]\n\n\n# A tibble: 336,776 × 3\n     day   day   day\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1     1     1\n 2     1     1     1\n 3     1     1     1\n 4     1     1     1\n 5     1     1     1\n 6     1     1     1\n 7     1     1     1\n 8     1     1     1\n 9     1     1     1\n10     1     1     1\n# ℹ 336,766 more rows\n\n\n\n\n6.2.4 4.3.5.4\n\nWhat does the any_of() function do? Why might it be helpful in conjunction with this vector?\n\n\nCode\nvariables &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\n\n\nIt allows you to specify the name of variables for subsetting outside the dplyr pipeline.\n\n\nCode\nvariables &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\nflights |&gt; \n  select(any_of(variables))\n\n\n# A tibble: 336,776 × 5\n    year month   day dep_delay arr_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1  2013     1     1         2        11\n 2  2013     1     1         4        20\n 3  2013     1     1         2        33\n 4  2013     1     1        -1       -18\n 5  2013     1     1        -6       -25\n 6  2013     1     1        -4        12\n 7  2013     1     1        -5        19\n 8  2013     1     1        -3       -14\n 9  2013     1     1        -3        -8\n10  2013     1     1        -2         8\n# ℹ 336,766 more rows\n\n\n\n\n6.2.5 4.3.5.5\n\nDoes the result of running the following code surprise you? How do the select helpers deal with upper and lower case by default? How can you change that default?\n\n\n\nCode\nflights |&gt; \n  select(contains(\"TIME\"))\n\n\n# A tibble: 336,776 × 6\n   dep_time sched_dep_time arr_time sched_arr_time air_time time_hour          \n      &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;dbl&gt; &lt;dttm&gt;             \n 1      517            515      830            819      227 2013-01-01 05:00:00\n 2      533            529      850            830      227 2013-01-01 05:00:00\n 3      542            540      923            850      160 2013-01-01 05:00:00\n 4      544            545     1004           1022      183 2013-01-01 05:00:00\n 5      554            600      812            837      116 2013-01-01 06:00:00\n 6      554            558      740            728      150 2013-01-01 05:00:00\n 7      555            600      913            854      158 2013-01-01 06:00:00\n 8      557            600      709            723       53 2013-01-01 06:00:00\n 9      557            600      838            846      140 2013-01-01 06:00:00\n10      558            600      753            745      138 2013-01-01 06:00:00\n# ℹ 336,766 more rows\n\n\nYes, it’s surprising to me. The selection helpers have the argument ignore.case = TRUE as a default.\n\n\nCode\nflights |&gt; \n  select(contains(\"TIME\", ignore.case = FALSE))\n\n\n# A tibble: 336,776 × 0\n\n\n\n\n6.2.6 4.3.5.6\n\nRename air_time to air_time_min to indicate units of measurement and move it to the beginning of the data frame.\n\n\n\nCode\nflights |&gt; \n  rename(air_time_min = air_time) |&gt; \n  relocate(air_time_min)\n\n\n# A tibble: 336,776 × 19\n   air_time_min  year month   day dep_time sched_dep_time dep_delay arr_time\n          &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1          227  2013     1     1      517            515         2      830\n 2          227  2013     1     1      533            529         4      850\n 3          160  2013     1     1      542            540         2      923\n 4          183  2013     1     1      544            545        -1     1004\n 5          116  2013     1     1      554            600        -6      812\n 6          150  2013     1     1      554            558        -4      740\n 7          158  2013     1     1      555            600        -5      913\n 8           53  2013     1     1      557            600        -3      709\n 9          140  2013     1     1      557            600        -3      838\n10          138  2013     1     1      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n6.2.7 4.3.5.7\n\nWhy doesn’t the following work, and what does the error mean?\n\n\n\nCode\nflights |&gt; \n  select(tailnum) |&gt; \n  arrange(arr_delay)\n\n\nError in `arrange()`:\nℹ In argument: `..1 = arr_delay`.\nCaused by error:\n! object 'arr_delay' not found\n\n\nLine #2 drops every variable in the data frame except tailnum. Thus, there’s no arr_delay object for arrange() to work with."
  },
  {
    "objectID": "solutions-02.html#groups",
    "href": "solutions-02.html#groups",
    "title": "6  Solutions 2",
    "section": "6.3 Groups",
    "text": "6.3 Groups\n\n6.3.1 4.5.7.1\n\nWhich carrier has the worst average delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights |&gt; group_by(carrier, dest) |&gt; summarize(n()))\n\n\n\nCode\nflights |&gt; \n  group_by(carrier) |&gt; \n  summarize(avg_dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt; \n  arrange(desc(avg_dep_delay))\n\n\n# A tibble: 16 × 2\n   carrier avg_dep_delay\n   &lt;chr&gt;           &lt;dbl&gt;\n 1 F9              20.2 \n 2 EV              20.0 \n 3 YV              19.0 \n 4 FL              18.7 \n 5 WN              17.7 \n 6 9E              16.7 \n 7 B6              13.0 \n 8 VX              12.9 \n 9 OO              12.6 \n10 UA              12.1 \n11 MQ              10.6 \n12 DL               9.26\n13 AA               8.59\n14 AS               5.80\n15 HA               4.90\n16 US               3.78\n\n\nCode\nflights |&gt; \n  filter(carrier == \"F9\") |&gt; \n  count(carrier, dest, origin) \n\n\n# A tibble: 1 × 4\n  carrier dest  origin     n\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;\n1 F9      DEN   LGA      685\n\n\nCode\nflights |&gt; \n  count(carrier, dest) |&gt; \n  filter(dest == \"DEN\")\n\n\n# A tibble: 5 × 3\n  carrier dest      n\n  &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;\n1 B6      DEN     338\n2 DL      DEN    1043\n3 F9      DEN     685\n4 UA      DEN    3796\n5 WN      DEN    1404\n\n\nCode\nflights |&gt; \n  group_by(carrier, dest, origin) |&gt; \n  summarize(n = n(), avg_dd = mean(dep_delay, na.rm = TRUE)) |&gt; \n  filter(dest == \"DEN\" | carrier == \"F9\") |&gt; \n  arrange(avg_dd)\n\n\n`summarise()` has grouped output by 'carrier', 'dest'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 8 × 5\n# Groups:   carrier, dest [5]\n  carrier dest  origin     n avg_dd\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt;\n1 DL      DEN   LGA      678   9.82\n2 UA      DEN   LGA     1626  10.6 \n3 UA      DEN   EWR     2170  14.1 \n4 WN      DEN   LGA      715  15.4 \n5 DL      DEN   JFK      365  16.6 \n6 F9      DEN   LGA      685  20.2 \n7 B6      DEN   JFK      338  23.9 \n8 WN      DEN   EWR      689  24.4 \n\n\nFrontier Airlines (F9) is headquartered in Denver and this dataset only records flights from LGA to DEN. This means there’s no variance in F9 accross airports. We might be able to disentangle the effect of airport and carrier, but we will have to make assumptions.\n\n\n6.3.2 4.5.7.2\n\nFind the flights that are most delayed upon departure from each destination.\n\n\n\nCode\nflights |&gt; \n  group_by(dest) |&gt; \n  slice_max(dep_delay) |&gt; \n  relocate(dest, carrier, dep_delay) |&gt; \n  arrange(desc(dep_delay))\n\n\n# A tibble: 105 × 19\n# Groups:   dest [105]\n   dest  carrier dep_delay  year month   day dep_time sched_dep_time arr_time\n   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;int&gt;\n 1 HNL   HA           1301  2013     1     9      641            900     1242\n 2 CMH   MQ           1137  2013     6    15     1432           1935     1607\n 3 ORD   MQ           1126  2013     1    10     1121           1635     1239\n 4 SFO   AA           1014  2013     9    20     1139           1845     1457\n 5 CVG   MQ           1005  2013     7    22      845           1600     1044\n 6 TPA   DL            960  2013     4    10     1100           1900     1342\n 7 MSP   DL            911  2013     3    17     2321            810      135\n 8 PDX   DL            899  2013     6    27      959           1900     1236\n 9 ATL   DL            898  2013     7    22     2257            759      121\n10 MIA   AA            896  2013    12     5      756           1700     1058\n# ℹ 95 more rows\n# ℹ 10 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,\n#   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n6.3.3 4.5.7.3\n\nHow do delays vary over the course of the day. Illustrate your answer with a plot.\n\n\n\nCode\nflights |&gt; \n  group_by(hour) |&gt; \n  filter(!is.na(dep_delay)) |&gt; \n  summarize(\n    avg = mean(dep_delay),\n    se = sd(dep_delay) / sqrt(n())\n  ) |&gt; \n  ggplot(aes(hour, avg)) + \n  geom_pointrange(aes(ymin = avg - 2*se, ymax = avg + 2*se), size = 1/10) +\n  theme_light()\n\n\n\n\n\n\n\n6.3.4 4.5.7.4\n\nWhat happens if you supply a negative n to slice_min() and friends?\n\nThe documentation for this behavior is not great and I can’t think of a single example in which I’d use those functions that way instead of using arrange().\n\n\nCode\nA &lt;- flights |&gt; \n  slice_min(dep_delay, n = -1)\n\nB &lt;- flights |&gt; \n  arrange(dep_delay)\n\nidentical(A, B)\n\n\n[1] TRUE\n\n\nCode\nC &lt;- flights |&gt; \n  slice_max(dep_delay, n = -1) \n\nD &lt;- flights |&gt; \n  arrange(desc(dep_delay))\n\nidentical(C, D)\n\n\n[1] TRUE\n\n\n\n\n6.3.5 4.5.7.5\n\nExplain what count() does in terms of the dplyr verbs you just learned. What does the sort argument to count() do?\n\nCount is a short-hand for group_by + summarize. The sort argument accomplishes the same as arrange.\n\n\nCode\nflights |&gt; \n  group_by(origin) |&gt; \n  summarize(n = n()) |&gt; \n  arrange(desc(n))\n\n\n# A tibble: 3 × 2\n  origin      n\n  &lt;chr&gt;   &lt;int&gt;\n1 EWR    120835\n2 JFK    111279\n3 LGA    104662\n\n\nCode\nflights |&gt; \n  count(origin, sort = TRUE)\n\n\n# A tibble: 3 × 2\n  origin      n\n  &lt;chr&gt;   &lt;int&gt;\n1 EWR    120835\n2 JFK    111279\n3 LGA    104662\n\n\n\n\n6.3.6 4.5.7.6\n\nSuppose we have the following tiny data frame:\n\n\nCode\ndf &lt;- tibble(\n  x = 1:5,\n  y = c(\"a\", \"b\", \"a\", \"a\", \"b\"),\n  z = c(\"K\", \"K\", \"L\", \"L\", \"K\")\n)\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what group_by() does.\n\n\nCode\ndf |&gt;\n  group_by(y)\n\n\n\ntwo groups?\n\n\nCode\ndf |&gt; group_by(y)\n\n\n# A tibble: 5 × 3\n# Groups:   y [2]\n      x y     z    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     K    \n2     2 b     K    \n3     3 a     L    \n4     4 a     L    \n5     5 b     K    \n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what arrange() does. Also comment on how it’s different from the group_by() in part (a)?\n\n\nCode\ndf |&gt;\n  arrange(y)\n\n\n\nI assume that y will be a, a, a, b, b and that x will be 1, 3, 4, 2, 5\n\n\nCode\ndf |&gt;\n  arrange(y)\n\n\n# A tibble: 5 × 3\n      x y     z    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     K    \n2     3 a     L    \n3     4 a     L    \n4     2 b     K    \n5     5 b     K    \n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does.\n\n\nCode\ndf |&gt;\n  group_by(y) |&gt;\n  summarize(mean_x = mean(x))\n\n\n\nI assume the new data frame will have two columns (x, y) and two rows (y = 2.67, y = 3.5)\n\n\nCode\ndf |&gt;\n  group_by(y) |&gt;\n  summarize(mean_x = mean(x))\n\n\n# A tibble: 2 × 2\n  y     mean_x\n  &lt;chr&gt;  &lt;dbl&gt;\n1 a       2.67\n2 b       3.5 \n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does. Then, comment on what the message says.\n\n\nCode\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x))\n\n\n\nI assume there will be 3 columns (x, y, z) and three rows (x = c(1, 3.5, 3.5)).\n\n\nCode\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x))\n\n\n`summarise()` has grouped output by 'y'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 3 × 3\n# Groups:   y [2]\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does. How is the output different from the one in part (d).\n\n\nCode\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x), .groups = \"drop\")\n\n\n# A tibble: 3 × 3\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\n\nI assume the same answer, except the output won’t be “grouped.”\n\n\nCode\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x), .groups = \"drop\")\n\n\n# A tibble: 3 × 3\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\n\nWrite down what you think the outputs will look like, then check if you were correct, and describe what each pipeline does. How are the outputs of the two pipelines different?\n\n\nCode\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x))\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  mutate(mean_x = mean(x))\n\n\n\nThe results will be similar, except that the first pipeline will have 3 rows and the second one will have 5 rows.\n\n\nCode\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x))\n\n\n`summarise()` has grouped output by 'y'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 3 × 3\n# Groups:   y [2]\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\nCode\ndf |&gt;\n  group_by(y, z) |&gt;\n  mutate(mean_x = mean(x))\n\n\n# A tibble: 5 × 4\n# Groups:   y, z [3]\n      x y     z     mean_x\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     1 a     K        1  \n2     2 b     K        3.5\n3     3 a     L        3.5\n4     4 a     L        3.5\n5     5 b     K        3.5"
  },
  {
    "objectID": "solutions-03.html#ggplot",
    "href": "solutions-03.html#ggplot",
    "title": "7  Solutions 3",
    "section": "7.1 ggplot",
    "text": "7.1 ggplot\n\n7.1.1 Exercise\nModify the code below to make the points larger squares and slightly transparent. See ?geom_point for more information on the point layer.\n\n\nCode\nggplot(faithful) + \n  geom_point(aes(x = eruptions, y = waiting), shape = 15, size = 10, alpha = 1/5)\n\n\n\n\n\n\n\n7.1.2 Exercise\nColor the two visible clusters in the histogram with different colors.\n\n\nCode\nfaithful |&gt; \n  ggplot() + \n  geom_histogram(aes(x = eruptions, fill = eruptions &gt; 3.2))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n7.1.3 Exercise\nAdd a line that separates the two point distributions. See ?geom_abline for how to draw straight lines from a slope and intercept.\n\n\nCode\nggplot(faithful) + \n  geom_point(aes(x = eruptions, y = waiting)) + \n  geom_abline(intercept = 125, slope = -20)\n\n\n\n\n\n\n\n7.1.4 Exercise\nUse RColorBrewer::display.brewer.all() to see all the different palettes from Color Brewer and pick your favorite. Modify the code below to use it.\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, fill = class), shape = 21) + \n  scale_fill_brewer(type = 'qual', palette = \"Spectral\")\n\n\n\n\n\n\n\n7.1.5 Exercise\nModify the code below to create a bubble chart (scatterplot with size mapped to a continuous variable) showing cyl with size. Make sure that only the present amount of cylinders (4, 5, 6, and 8) are present in the legend.\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class, size = cyl)) + \n  scale_color_brewer(type = 'qual') + \n  scale_size_area(breaks = c(4, 5, 6, 8)) ## ensures 0 is mapped to 0 size\n\n\n\n\n\n\n\n7.1.6 Exercise\nModify the code below so that color is no longer mapped to the discrete class variable, but to the continuous cty variable. What happens to the guide?\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = cty, size = cty)) + \n  scale_size_area() \n\n\n\n\n\nThe type of guide can be controlled with the guide argument in the scale, or with the guides() function. Continuous colors have a gradient color bar by default, but setting it to legend will turn it back to the standard look. What happens when multiple aesthetics are mapped to the same variable and uses the guide type?\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = cty, size = cty)) + \n  scale_size_area() + \n  guides(color = \"legend\")\n\n\n\n\n\n\n\n7.1.7 Exercise\nOne of the great things about facets is that they share the axes between the different panels. Sometimes this is undesirable though, and the behavior can be changed with the scales argument. Experiment with the different possible settings in the plot below:\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_wrap(~ drv, scales = \"fixed\") ## default\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_wrap(~ drv, scales = \"free\")\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_wrap(~ drv, scales = \"free_x\")\n\n\n\n\n\nCode\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_wrap(~ drv, scales = \"free_y\")\n\n\n\n\n\n\n\n7.1.8 Exercise\nThemes can be overwhelming, especially as you often try to optimize for beauty while you learn. To remove the last part of the equation, the exercise is to take the plot given below and make it as hideous as possible using the theme function. Go absolutely crazy, but take note of the effect as you change different settings.\n\n\nCode\nmpg |&gt; \n  ggplot(aes(y = class, fill = drv)) + \n  geom_bar() + \n  facet_wrap(~year) + \n  labs(\n    title = \"Number of car models per class\",\n    caption = \"source: http://fueleconomy.gov\",\n    x = 'Number of cars',\n    y = NULL\n  ) +\n  theme(\n    text = element_text(\"Comic Sans MS\", color = \"orange\"),\n    axis.text = element_text(color = \"white\", size = 20),\n    panel.background = element_rect(\"yellow\"),\n    legend.background = element_rect(\n      fill = \"yellow\", linetype = \"dotted\", color = \"white\", linewidth = 2),\n    strip.text = element_text(face = 'bold', hjust = 0, angle = 180),\n    plot.background = element_rect(\"black\")\n  )"
  },
  {
    "objectID": "solutions-03.html#simulation",
    "href": "solutions-03.html#simulation",
    "title": "7  Solutions 3",
    "section": "7.2 Simulation",
    "text": "7.2 Simulation\n\n\nCode\nurl &lt;- \"https://raw.githubusercontent.com/acastroaraujo/socStats/main/simulation_function_week3.R\"\nsource(url)\n\n\nsims &lt;- simulation_votes(dem_prob_pop = 0.52, sample_size = 300, num_sims = 500)\n\nresults &lt;- sims |&gt; \n  group_by(id) |&gt; \n  summarize(dem_prop = mean(vote == \"Dem\"))\n\nglimpse(results)\n\n\nRows: 500\nColumns: 2\n$ id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ dem_prop &lt;dbl&gt; 0.5300000, 0.5533333, 0.4933333, 0.5500000, 0.5366667, 0.5166…\n\n\n\n7.2.1 Exercise\nIn the simulation above, what is the average dem_prop? What is the standard deviation of dem_prop? How does this change for different values of sample_size?\n\n\nCode\nmean(results$dem_prop)\n\n\n[1] 0.52048\n\n\nThis is almost the same as dem_prob_pop!\n\n\nCode\nsd(results$dem_prop)\n\n\n[1] 0.02904462\n\n\nNow I will increase the sample size by a factor of 2.\n\n\nCode\nsims &lt;- simulation_votes(dem_prob_pop = 0.52, sample_size = 600, num_sims = 500)\n\nresults &lt;- sims |&gt; \n  group_by(id) |&gt; \n  summarize(dem_prop = mean(vote == \"Dem\"))\n\nmean(results$dem_prop)\n\n\n[1] 0.5209533\n\n\nCode\nsd(results$dem_prop)\n\n\n[1] 0.01998279\n\n\nThe mean is still pretty much the same, but the standard deviation is now much smaller. (Decreasing the sample size will produce larger standard deviations.)\n\n\n7.2.2 Exercise\nCreate five different simulations with different values of sample_size (e.g., 50, 200, 500, 1000, 2000). Put them together into a single dataset and then visualize the results using boxplots. What is going on?\n\n\nCode\nss &lt;- c(50, 200, 500, 1000, 2000)\noutput_list &lt;- lapply(ss, \\(ss) simulation_votes(0.52, ss, num_sims = 500)) \n\nbind_rows(output_list) |&gt; \n  group_by(id, sample_size) |&gt; \n  summarize(dem_prop = mean(vote == \"Dem\")) |&gt; \n  ggplot(aes(sample_size, dem_prop)) + \n  geom_boxplot()\n\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\n\n\n\n\nCode\nbind_rows(output_list) |&gt; \n  mutate(sample_size = as.numeric(sample_size)) |&gt; \n  group_by(id, sample_size) |&gt; \n  summarize(dem_prop = mean(vote == \"Dem\")) |&gt; \n  ggplot(aes(sample_size, dem_prop, group = sample_size)) + \n  geom_boxplot()\n\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n7.2.3 Exercise\nCreate five different simulations with different values of dem_prob_pop (e.g., 0.49, 0.52, 0.55, 0.58). Put them together into a single dataset and then visualize the results using boxplots. What is going on?\n\n\nCode\np &lt;- c(.49, .52, .55, .58, .63)\noutput_list &lt;- lapply(p, \\(x) simulation_votes(x, sample_size = 300, num_sims = 500)) \n\n\nbind_rows(output_list) |&gt; \n  group_by(id, dem_prob_pop) |&gt; \n  summarize(dem_prop = mean(vote == \"Dem\")) |&gt; \n  ggplot(aes(dem_prob_pop, dem_prop)) + \n  geom_boxplot()\n\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n7.2.4 Extra\nMega simulation pretty graph.\n\n\nCode\ngrid &lt;- tidyr::expand_grid(p, ss)\n\nout &lt;- map2(grid$p, grid$ss, function(dem_prob, sample_size) {\n  simulation_votes(dem_prob, sample_size, num_sims = 500)\n})\n\ndf &lt;- bind_rows(out) |&gt; \n  group_by(dem_prob_pop, sample_size, id) |&gt; \n  summarize(prop = mean(vote == \"Dem\"))\n\ndf$dem_prob_pop &lt;- factor(df$dem_prob_pop, levels = p)\ndf$sample_size &lt;- factor(df$sample_size, levels = ss)\n\ndf |&gt; \n  mutate(strip_label = paste(\"\\\"True\\\" Value:\", dem_prob_pop)) |&gt; \n  ggplot(aes(prop, sample_size)) + \n  geom_boxplot() + \n  labs(y = \"Sample Size\", x = \"Observed Proportions\") + \n  facet_wrap(~ strip_label, ncol = 1) + \n  theme_light(base_family = \"Optima\") + \n  theme(strip.background = element_rect(fill = \"steelblue\"))"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Blitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to\nProbability. CRC Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and\nOther Stories. Cambridge University Press.\n\n\nHealy, Kieran. 2018. Data\nVisualization: A Practical Introduction.\n\n\nImai, Kosuke, and Nora Webb Williams. 2022. Quantitative Social\nScience: An Introduction in Tidyverse. Princeton University Press.\n\n\nIsmay, Chester, and Albert Y. Kim. 2019. Statistical Inference via\nData Science: A ModernDive into r and the Tidyverse. CRC Press.\n\n\nLlaudet, Elena, and Kosuke Imai. 2022. Data Analysis for Social\nScience: A Friendly and Practical Introduction. Princeton\nUniversity Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in r and Stan. CRC press.\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in\nStatistical Inference. Springer.\n\n\nWickham, Hadley. 2019. Advanced R. CRC Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. \" O’Reilly Media,\nInc.\"."
  },
  {
    "objectID": "solutions-04.html#exercise",
    "href": "solutions-04.html#exercise",
    "title": "8  Solutions 4",
    "section": "8.1 Exercise",
    "text": "8.1 Exercise\n\n\nCode\nx &lt;- rnorm(100000, mean = 0, sd = 1)\n\n\nThe probability that \\(x\\) is within one standard deviation \\(\\sigma\\) away from the mean is roughly 68%.\n\n\nCode\nmean(x &gt;= -1 & x &lt;= 1)\n\n\n[1] 0.68498\n\n\nThe probability that \\(x\\) is within two standard deviations \\(2\\sigma\\) away from the mean is roughly 95%.\n\n\nCode\nmean(x &gt;= -2 & x &lt;= 2)\n\n\n[1] 0.95539\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe actual value is closer to 1.96 standard deviations away from the mean instead of “2.”\n\n\nCode\nquantile(x, probs = c(0.025, 0.975))\n\n\n     2.5%     97.5% \n-1.955397  1.956256 \n\n\nCode\nqnorm(c(0.025, 0.975), mean = 0, sd = 1)\n\n\n[1] -1.959964  1.959964"
  },
  {
    "objectID": "solutions-04.html#exercise-1",
    "href": "solutions-04.html#exercise-1",
    "title": "8  Solutions 4",
    "section": "8.2 Exercise",
    "text": "8.2 Exercise\n\n\nCode\nquantile(x)\n\n\n           0%           25%           50%           75%          100% \n-4.8795838435 -0.6695851352  0.0001649382  0.6686857020  4.1235503329 \n\n\nThe quantile() function calculates the 25th, 50th, and 75th percentiles of a vector of numbers.\nIt also includes the minimum and maximum values:\n\n\nCode\nmin(x)\n\n\n[1] -4.879584\n\n\nCode\nmax(x)\n\n\n[1] 4.12355\n\n\nHey! I hope you remember that the median is the same as the 50th percentile.\n\n\nCode\nmedian(x)\n\n\n[1] 0.0001649382"
  },
  {
    "objectID": "solutions-04.html#exercise-2",
    "href": "solutions-04.html#exercise-2",
    "title": "8  Solutions 4",
    "section": "8.3 Exercise",
    "text": "8.3 Exercise\n\n\nCode\nquantile(x, probs = c(0.005, 0.995))\n\n\n     0.5%     99.5% \n-2.554854  2.556632"
  },
  {
    "objectID": "solutions-04.html#exercise-3",
    "href": "solutions-04.html#exercise-3",
    "title": "8  Solutions 4",
    "section": "8.4 Exercise",
    "text": "8.4 Exercise\nLet \\(x = x_1 + \\dots + x_{20}\\), the sum of 20 independent uniform(0, 1) random variables. In R, create 1000 simulations of \\(x\\) and plot their histogram.\n\n\nCode\nd &lt;- tibble(id = 1:1e3) |&gt; \n  rowwise() |&gt; \n  mutate(x = sum(runif(20))) |&gt; \n  ungroup() ## this might save you trouble later on!\n\nd |&gt; ggplot(aes(x)) + geom_histogram(color = \"white\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nBonus\n\n\nCode\nsd(d$x)\n\n\n[1] 1.318354\n\n\nThe real standard error is:\n\n\nCode\nsqrt(20/12)\n\n\n[1] 1.290994\n\n\nAn estimate of the standard error for one sample:\n\n\nCode\nsd(runif(20, min = 0, max = 1)) * sqrt(20)\n\n\n[1] 1.249974\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote\nThere is no “uniquely correct” way of simulating data.\nFor example, this is the way I would usually do it:\n\n\nCode\nN &lt;- 20\nS &lt;- 1e3\nx &lt;- replicate(S, sum(runif(N, min = 0, max = 1)))\n\ntibble(x) |&gt; \n  ggplot(aes(x)) + geom_histogram(color = \"white\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "solutions-04.html#exercise-4",
    "href": "solutions-04.html#exercise-4",
    "title": "8  Solutions 4",
    "section": "8.5 Exercise",
    "text": "8.5 Exercise\nMany introductory statistics textbooks say that “sample sizes equal to or greater than 30 are often considered sufficient for the CLT to hold.” I’ve seen a lot of confusion with regards to this. For example, I’ve heard people say that if the sample size is 30 then the data is normally distributed. Absolutely not! It means that the sampling distribution will converge to a normal distribution. This means that sample sizes less than 30 will not produce “normal” sampling distributions, and we can verify this using simulations.\n\n\n\n\n\n\nTip\n\n\n\nThis relevant for calculating “statistical significance,” which we’ll cover on week 5.\n\n\nThe following code picks up follows Steve’s simulation that we did in class:\n\n\nCode\nsvy_size &lt;- 15 ## this is different!\nest_prop &lt;- 1/3\nnum_sims &lt;- 10e3\n\nsims &lt;- replicate(num_sims, mean(rbinom(svy_size, 1, prob = est_prop)))\nd &lt;- tibble(prop = sims) |&gt; \n  rowid_to_column(\"sim_num\")\n\nd |&gt; summarize(mean = mean(prop), se = sd(prop))\n\n\n# A tibble: 1 × 2\n   mean    se\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.333 0.121\n\n\nThe “mean” value is still the same, but the standard error is bigger because our sample size is 15 instead of 2247. So far, so good.\nNow lets calculate the standard error:\n\n\nCode\nstd_error &lt;- sqrt((1/3) * (2/3) / svy_size)\nstd_error\n\n\n[1] 0.1217161\n\n\nHey, this is pretty much the same as the standard error we calculated in the simulation!\n\n\nCode\nsd(d$prop)\n\n\n[1] 0.1213351\n\n\nFinally, I know that in any normal distribution approximately 68% of the values of should be within one standard deviation away from the mean.\n\n\nCode\nmean(d$prop &gt; 1/3 - sd(d$prop) & d$prop &lt; 1/3 + sd(d$prop))\n\n\n[1] 0.5882\n\n\nOh no! The coverage is all wrong. This means that the sampling distribution we created does not have the properties of a normal distribution. This must be what people mean when they say that “sample sizes equal to or greater than 30 are often considered sufficient for the CLT to hold.”\n\n\n\n\n\n\nTip\n\n\n\nBtw, that previous line of code is a little bit clunky. The dplyr package has a useful function called between() which may be easier to understand.\n\n\nCode\nlower &lt;- 1/3 - sd(d$prop)\nupper &lt;- 1/3 + sd(d$prop)\nmean(between(d$prop, lower, upper))\n\n\n[1] 0.5882"
  },
  {
    "objectID": "solutions-04.html#extra",
    "href": "solutions-04.html#extra",
    "title": "8  Solutions 4",
    "section": "8.6 Extra",
    "text": "8.6 Extra\nLet’s plot how does the sample size affect the number of outcomes that fall within one standard deviations away from the mean.\nThis is how I built my simulation.\nStep. 1 Do it once!\nIs the “true” value inside the estimated 95% confidence interval?\n\n\nCode\ntrue_value &lt;- 1/3\nsample_size &lt;- 300\n\nx &lt;- rbinom(sample_size, 1, prob = true_value)\nestimate &lt;- mean(x)\nestimate\n\n\n[1] 0.3433333\n\n\nCode\nstd_error_estimate &lt;- sqrt(estimate * (1 - estimate) / sample_size)\nstd_error_estimate\n\n\n[1] 0.02741384\n\n\nCode\n## Is the \"true\" value inside the estimated 95% confidence interval??\nlower &lt;- estimate - 2 * std_error_estimate\nupper &lt;- estimate + 2 * std_error_estimate\n\n# true_value &gt; lower & true_value &lt; upper\nbetween(true_value, lower, upper)\n\n\n[1] TRUE\n\n\nStep 2. Do it 10,000 times!\nWhat percentage of the time is the “true” value inside the 95% confidence interval?\n\n\nCode\ntrue_value &lt;- 1/3\nsample_size &lt;- 300 ## play around with this!\nnum_sims &lt;- 10e3\n\nsims &lt;- replicate(num_sims, {\n  \n  x &lt;- rbinom(sample_size, 1, prob = true_value)\n  estimate &lt;- mean(x)\n  std_error_estimate &lt;- sqrt(estimate * (1 - estimate) / sample_size)\n  ## Is the \"true\" value inside the estimated 95% confidence interval??\n  lower &lt;- estimate - 2 * std_error_estimate\n  upper &lt;- estimate + 2 * std_error_estimate\n  between(true_value, lower, upper)\n  \n})\n\nmean(sims)\n\n\n[1] 0.9495\n\n\nStep 3. Do it 10,000 times for different sample sizes.\n\n\nCode\nsimulation &lt;- function(N, P, S) {\n  \n  out &lt;- replicate(S, expr = {\n  \n    x &lt;- rbinom(N, 1, prob = P)\n    estimate &lt;- mean(x)\n    std_error_estimate &lt;- sqrt(estimate * (1 - estimate) / N)\n    ## Is the \"true\" value inside the estimated 95% confidence interval??\n    lower &lt;- estimate - 2 * std_error_estimate\n    upper &lt;- estimate + 2 * std_error_estimate\n    dplyr::between(P, lower, upper)\n    \n  })\n  \n  return(mean(out))\n  \n}\n\nss &lt;- seq(5, 150, by = 1)\nout &lt;- map_dbl(ss, \\(x) simulation(N = x, P = 1/3, S = 5e3), .progress = TRUE)\n\ntibble(sample = ss, coverage = out) |&gt; \n  ggplot(aes(sample, coverage)) + \n  geom_rect(xmin = 0, ymin = 0, xmax = 30, ymax = Inf, alpha = 1/5) +\n  geom_hline(yintercept = 0.95) + \n  geom_line()"
  }
]